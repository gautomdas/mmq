{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596b33cf-382f-4121-ac88-92799c65e284",
   "metadata": {},
   "source": [
    "# Demo of Blip2 Quantization, Inference, and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de58b3-4ee9-4267-80c5-1a41582fe3cb",
   "metadata": {},
   "source": [
    "## 1. Load Model and Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c695ac4-7895-4a53-b4da-579984a1899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blip_quantizer import BlipQuantizer, QuantConfig, ModelPart, LayerGroup, LayerType\n",
    "from quant_functions import uniform_quantization\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer\n",
    "from datasets import VQAv2Eval\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import print_model_structure\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13309c9-5353-41f8-8bec-8351f4c82504",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqav2_dataset = VQAv2Eval(\n",
    "    image_root=\"./data/vqav2/val2014\",\n",
    "    ann_root=\"./data/vqav2/annotations\",\n",
    "    q_root=\"./data/vqav2/questions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25cac40-c06e-470e-a24f-8fcfe0a9b065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a4293a02a848f0b83411f190033419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, vis_processors, _ = load_model_and_preprocess(\"blip2_t5\", \"pretrain_flant5xl\", device=\"cuda\", is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00346f3b-378a-4008-b985-ba5619a350f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc4badbd54646e4a9bda54a30a28094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "configs = [\n",
    "    QuantConfig(ModelPart.VIT, LayerGroup.FIRST, LayerType.BOTH, \n",
    "                uniform_quantization, num_bits=8),\n",
    "    QuantConfig(ModelPart.VIT, LayerGroup.MIDDLE, LayerType.MLP, \n",
    "                uniform_quantization, num_bits=8),\n",
    "    QuantConfig(ModelPart.QFORMER, LayerGroup.MIDDLE, LayerType.MLP, \n",
    "                uniform_quantization, num_bits=4),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "#quantizer.apply_quantization(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398f0cfd-8957-4798-b2b6-4dca7f8551df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Finished, Saving Results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", padding_side=\"left\")\n",
    "\n",
    "processor_kwargs={\"padding\": \"longest\", \"max_length\": 32, \"truncation\": True}\n",
    "generate_kwargs={\"num_beams\": 5, \"max_new_tokens\": 10, \"min_length\": 1, \"length_penalty\": 0, \"do_sample\": False}\n",
    "\n",
    "inferencer = InferencePipeline(model, device, processor)\n",
    "print(\"Starting inference...\")\n",
    "results = inferencer.run_inference(\n",
    "    vqav2_dataset, \n",
    "    task=\"visual_question_answering\", \n",
    "    max_samples=10, \n",
    "    processor_kwargs=processor_kwargs, \n",
    "    generate_kwargs=generate_kwargs\n",
    ")\n",
    "print(\"Inference Finished, Saving Results...\")\n",
    "inferencer.save_results(results, \"./results/vqav2_quantized_inference.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feab94b-f109-48a2-8efb-793181262cc3",
   "metadata": {},
   "source": [
    "## 3. Score Results from .json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba17c86-c583-425b-8152-7aca8dcbe02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding current path to python system paths\n",
      "loading VQA annotations and questions into memory...\n",
      "0:00:06.386413\n",
      "creating index...\n"
     ]
    }
   ],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "scorer = ScoringPipeline()\n",
    "loaded_results = scorer.load_results(\"./results/vqav2_quantized_inference.json\")\n",
    "\n",
    "loaded_results[\"annotations\"] = \"./data/vqav2/annotations/v2_mscoco_val2014_annotations.json\"\n",
    "loaded_results[\"questions\"] = \"./data/vqav2/questions/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "\n",
    "scores = scorer.compute_scores(loaded_results, task=\"visual_question_answering\")\n",
    "\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774281d4-2d75-40ca-86b3-4a1338b214ff",
   "metadata": {},
   "source": [
    "## Sample Results\n",
    "\n",
    "This is not a necessary step but just helps qualitatively understand how the results relate to the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244503d-3acb-45db-8e2c-9aa5e5187ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "f = open(\"./results/vqav2_quantized_inference.json\")\n",
    "\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "def show_results(idx, results, vqav2_dataset):\n",
    "    pair = vqav2_dataset.qa_pairs[idx]\n",
    "    image = Image.open(vqav2_dataset.image_root+'/'+pair[\"image\"]).convert(\"RGB\")\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(pair['question'])\n",
    "    print(f\"true answers: {pair['answer']}\")\n",
    "\n",
    "    pred_ans = []\n",
    "    for qa in results[\"answers\"]:\n",
    "        if qa[\"question_id\"] == pair[\"question_id\"]:\n",
    "            pred_ans.append(qa[\"answer\"].strip())\n",
    "                            \n",
    "    print(f\"pred answers: {pred_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce264aa-7dc2-4d10-a889-ded5079a6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(1, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2051df-dde4-4f25-bdd8-c780e17cf864",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(3, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57dacb-bd85-4b0f-8b92-edca066ecf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(5, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4701b24-c336-4a88-aa1e-8c28ddc0b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(7, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb32a53-2f94-42a9-b31c-cb352f9cc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(9, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b2e3f-6b2d-4722-ab05-f615797055c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
