{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/nexus-scratch/vla/micromamba/envs/MMQ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoProcessor, Blip2ForImageTextRetrieval\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "# from utils import print_model_structure\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.82it/s]\n",
      "/fs/nexus-scratch/vla/micromamba/envs/MMQ/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# TODO: determine appropriate size for this calibration set\n",
    "# AutoAWQ defaults to a size of 512\n",
    "\n",
    "coco_dataset = COCODataset(ann_file='/nfshomes/vla/project_dirs/low-bit-vision/datasets/cocow/annotations/captions_val2017.json',\n",
    "                           img_dir='/nfshomes/vla/project_dirs/low-bit-vision/datasets/cocow/images/val2017')\n",
    "\n",
    "# calibration_set = [coco_dataset[0], coco_dataset[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class for AWQ quantizer\n",
    "class BaseAWQQuantizer():\n",
    "    \n",
    "    def __init__(self, model, device, inputs_processor, dataset, **kwargs):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.inputs_processor = inputs_processor\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.group_size = 128\n",
    "\n",
    "        # TODO: change to something appropriate\n",
    "        self.n_samples = 2\n",
    "        \n",
    "        self.run_model = None\n",
    "\n",
    "    @torch.no_grad\n",
    "    def quantize(self):\n",
    "        layer_groups = self._get_model_layer_groups()\n",
    "        calibration_set = self._get_calibration_set()\n",
    "        first_inputs, layer_args, layer_kwargs = self._gather_first_inputs(layer_groups, calibration_set)\n",
    "        \n",
    "\n",
    "        for layer_group, modules in layer_groups.items():\n",
    "\n",
    "            inps = first_inputs[layer_group]\n",
    "            for i in tqdm(range(len(modules)), desc= f\"Quantizing {layer_group}\"):\n",
    "                \n",
    "                inps = modules[i](inps, *layer_args[layer_group], **layer_kwargs[layer_group])\n",
    "                inps = inps[0]\n",
    "                \n",
    "\n",
    "        return layer_groups, first_inputs, layer_args, layer_kwargs\n",
    "    \n",
    "    def _compute_scales(self, layer, inp):\n",
    "\n",
    "        for mod_name, xs in inp.items():\n",
    "            x_flat = torch.cat([x.cpu().abs().view(-1, x.shape[-1]) for x in xs], dim = 0)\n",
    "\n",
    "            # average of absolute value of all channels in linear module\n",
    "            x_mean = x_flat.mean(0)\n",
    "            print(x_mean)\n",
    "\n",
    "            \n",
    "\n",
    "    def _gather_first_inputs(self, layer_groups, calibration_set):\n",
    "\n",
    "        first_inputs = {}\n",
    "        layer_args = {}\n",
    "        layer_kwargs = {}\n",
    "\n",
    "        # get input and kwargs to layer 0\n",
    "        # use this Catcher hack cause forward hooks cannot capture kwargs\n",
    "        class Catcher(nn.Module):\n",
    "            def __init__(self, module, layer_group):\n",
    "                super().__init__()\n",
    "                self.module = module\n",
    "                self.layer_group = layer_group\n",
    "\n",
    "            def forward(self, *args, **kwargs):\n",
    "                # assume first input to forward is hidden states\n",
    "                if len(args) > 0:\n",
    "                    hidden_states = args[0]\n",
    "                    # del args\n",
    "                else:\n",
    "                    first_key = list(kwargs.keys())[0]\n",
    "                    hidden_states = kwargs.pop(first_key)\n",
    "\n",
    "                first_inputs[self.layer_group] = hidden_states\n",
    "                layer_args[self.layer_group] = args[1:]\n",
    "                layer_kwargs[self.layer_group] = kwargs\n",
    "\n",
    "                # print(f'args: {type(args)}')\n",
    "                # print(args)\n",
    "                # print(f'kwargs: {type(kwargs)}')\n",
    "                # print(kwargs)\n",
    "\n",
    "                return self.module.forward(*args, **kwargs)\n",
    "\n",
    "        # raise ValueError  # early exit to break later inference\n",
    "       \n",
    "        for layer_group, modules in layer_groups.items():\n",
    "            # replace first module in group of layers with a Catcher\n",
    "            modules[0] = Catcher(modules[0], layer_group)\n",
    "\n",
    "\n",
    "        self.run_model(calibration_set)\n",
    "        \n",
    "        for layer_group, modules in layer_groups.items():\n",
    "            # restore proper module at beginning of layer group\n",
    "            modules[0] = modules[0].module\n",
    "        \n",
    "        return first_inputs, layer_args, layer_kwargs\n",
    "       \n",
    "\n",
    "\n",
    "    def _gather_layer_input(self, layers, calibration_set):\n",
    "\n",
    "        def input_hook(module, input, output, layer_index, module_name, inputs):\n",
    "            x = input[0]\n",
    "            x = x.detach().cpu()\n",
    "\n",
    "            out = output[0]\n",
    "            out = out.detach().cpu()\n",
    "\n",
    "            inputs[layer_index][module_name].append(x)\n",
    "        \n",
    "\n",
    "        # list of dicts holding inputs for each layer\n",
    "        inputs = [defaultdict(list)] * len(layers)\n",
    "        # list of hooks so we can remove them after\n",
    "        hooks = []\n",
    "        \n",
    "        for i,layer in enumerate(layers):\n",
    "            named_linears = self._get_named_linears(layer)\n",
    "            for name, mod in named_linears.items():\n",
    "                hooks.append(\n",
    "                    mod.register_forward_hook(partial(input_hook,\n",
    "                                                      layer_index = i, \n",
    "                                                      module_name=name, \n",
    "                                                      inputs = inputs))\n",
    "                )\n",
    "\n",
    "        \n",
    "        # TODO: setup proper dataloader for this\n",
    "        for batch in calibration_set:\n",
    "            X = self._prepare_input(batch)\n",
    "            self.run_model(**X)\n",
    "\n",
    "        # remove hooks from model\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        return inputs\n",
    "        \n",
    "\n",
    "    # returns all nn.linear within module (a layer)\n",
    "    def _get_named_linears(self, module):\n",
    "        return {name: mod for name, mod in module.named_modules() if isinstance(mod, nn.Linear)}\n",
    "\n",
    "    # return layers of model to consider for quantization (modify with config file)\n",
    "    def _get_model_layer_groups(self):\n",
    "        raise NotImplementedError('_get_model_layers')\n",
    "    \n",
    "    def _get_calibration_set(self):\n",
    "        raise NotImplementedError('_get_calibration_set')\n",
    "\n",
    "    def _prepare_input(self):\n",
    "        raise NotImplementedError('_prepare_input')\n",
    "    \n",
    "\n",
    "class Blip2ForConditionalGenerationAWQQuantizer(BaseAWQQuantizer):\n",
    "\n",
    "    def __init__(self, model, inputs_processor, dataset):\n",
    "        assert isinstance(model, Blip2ForConditionalGeneration)\n",
    "\n",
    "        super().__init__(model, device, inputs_processor, dataset)\n",
    "        self.run_model = model.generate\n",
    "        \n",
    "    def _get_model_layer_groups(self):\n",
    "        # NOTE: returning all layers for now\n",
    "        return {'vit_layers': self.model.vision_model.encoder.layers,\n",
    "                'qformer_layers': self.model.qformer.encoder.layer,\n",
    "                'llm_layers': self.model.language_model.model.decoder.layers\n",
    "              }\n",
    "\n",
    "    def _get_calibration_set(self):\n",
    "        # NOTE: small set for testing\n",
    "\n",
    "        samples = []\n",
    "        n = 0\n",
    "        for data in self.dataset:\n",
    "            \n",
    "            sample = self._prepare_input(data[0])\n",
    "            samples.append(sample)\n",
    "            \n",
    "            n += 1\n",
    "            if n == self.n_samples:\n",
    "                break\n",
    "        \n",
    "        samples = torch.cat(samples, dim = 0)\n",
    "        return samples\n",
    "\n",
    "    def _prepare_input(self, inp):\n",
    "        X = self.inputs_processor(images=inp, return_tensors=\"pt\").to(device)\n",
    "        return X['pixel_values']\n",
    "\n",
    "\n",
    "class Blip2ForImageTextRetrievalAWQQuantizer(BaseAWQQuantizer):\n",
    "\n",
    "    def __init__(self, model, device, inputs_processor, dataset):\n",
    "        assert isinstance(model, Blip2ForImageTextRetrieval)\n",
    "        super().__init__(model, device, inputs_processor, dataset)\n",
    "        self.run_model = model.forward\n",
    "        \n",
    "    def _get_model_layer_groups(self):\n",
    "        # NOTE: returning all layers for now\n",
    "        return [*[layer for layer in self.model.vision_model.encoder.layers],\n",
    "                *[layer for layer in self.model.qformer.encoder.layer]]\n",
    "\n",
    "    def _get_calibration_set(self):\n",
    "        return [self.dataset[0], self.dataset[1]]\n",
    "\n",
    "    def _prepare_input(self, batch):\n",
    "        X = self.processor(images=batch[0], text=batch[1][0], return_tensors=\"pt\").to(device, torch.float16)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g\", torch_dtype=torch.float16)\n",
    "# processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-itm-vit-g\")\n",
    "# model.to(device)\n",
    "\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2ForConditionalGeneration(\n",
       "  (vision_model): Blip2VisionModel(\n",
       "    (embeddings): Blip2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): Blip2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x Blip2EncoderLayer(\n",
       "          (self_attn): Blip2Attention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Blip2MLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): Blip2QFormerModel(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (encoder): Blip2QFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n",
       "  (language_model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Blip2ForConditionalGenerationAWQQuantizer(model, processor, coco_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing vit_layers: 100%|██████████| 39/39 [00:00<00:00, 2491.67it/s]\n",
      "Quantizing qformer_layers: 100%|██████████| 12/12 [00:00<00:00, 1307.93it/s]\n",
      "Quantizing llm_layers: 100%|██████████| 32/32 [00:00<00:00, 328.86it/s]\n"
     ]
    }
   ],
   "source": [
    "layers, first_inputs, layer_args, layer_kwargs = b.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7319,  0.2039, -0.1177,  ...,  0.3483, -0.1315, -0.3481],\n",
       "         [-0.2441,  0.8111, -0.0983,  ...,  0.0234, -0.1451, -0.7400],\n",
       "         [-0.0418,  1.7882, -0.3203,  ..., -0.0219, -0.0488, -0.1417],\n",
       "         ...,\n",
       "         [-0.9605, -0.2865,  0.5448,  ..., -0.0762,  0.4271,  1.2226],\n",
       "         [-0.6000, -0.1437,  0.1221,  ..., -0.1582,  0.1567,  1.4119],\n",
       "         [-0.2725, -0.3828,  0.3872,  ...,  0.2089,  0.1482,  0.7765]],\n",
       "\n",
       "        [[ 0.7319,  0.2039, -0.1177,  ...,  0.3483, -0.1315, -0.3481],\n",
       "         [ 0.4066,  0.8089, -0.1578,  ...,  0.0691,  0.0124, -0.3917],\n",
       "         [ 0.2490,  1.6557, -0.3386,  ..., -0.1113,  0.0690, -0.0610],\n",
       "         ...,\n",
       "         [-1.0813, -0.3526,  0.3304,  ..., -0.0324,  0.3346,  0.8685],\n",
       "         [-0.5170, -0.1903, -0.0778,  ..., -0.1721,  0.1628,  1.2161],\n",
       "         [-0.0057, -0.5496,  0.3215,  ...,  0.1134, -0.0510,  0.9223]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_inputs['vit_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_args['vit_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_attentions': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_kwargs['vit_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = layers['vit_layers'][0](first_inputs['vit_layers'], *layer_args['vit_layers'], **layer_kwargs['vit_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3378, -0.1946, -0.0252,  ...,  0.6891, -0.1665, -0.2123],\n",
       "         [-0.5796, -0.0850,  0.8516,  ...,  0.2505,  0.2570, -2.0609],\n",
       "         [-0.1985,  1.0866,  0.2261,  ..., -0.4743, -0.8173, -0.4517],\n",
       "         ...,\n",
       "         [-1.9534, -0.5337,  0.5927,  ...,  0.0530,  0.0967,  1.0787],\n",
       "         [-1.4451, -0.4682,  0.1352,  ...,  0.0619,  0.0195,  1.4128],\n",
       "         [-0.4682, -1.1753,  0.9844,  ...,  0.6326,  1.3233,  0.8188]],\n",
       "\n",
       "        [[ 0.0635, -0.3202,  0.1072,  ...,  0.6998, -0.0949, -0.1115],\n",
       "         [-0.6654,  0.2025,  0.1473,  ...,  0.6025,  0.0049,  0.0392],\n",
       "         [-0.9092,  1.0444, -0.1438,  ...,  0.4590,  0.1668,  0.3372],\n",
       "         ...,\n",
       "         [-1.1537, -1.1512,  1.0716,  ...,  0.6038, -0.2326,  0.4638],\n",
       "         [-0.8844, -0.3487,  0.2901,  ...,  0.2318, -0.4852,  1.4794],\n",
       "         [-0.9237, -0.8065,  0.7050,  ...,  0.6329,  0.0026,  1.1944]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0026, -0.0644, -0.1888,  ...,  1.0750,  0.1186,  0.5769],\n",
       "          [-1.0819,  0.1315,  0.9809,  ...,  0.7254,  0.2461, -1.5226],\n",
       "          [-0.2447,  0.9629,  0.4360,  ..., -0.0313, -1.1122, -0.2950],\n",
       "          ...,\n",
       "          [-2.0357,  0.1081,  0.3105,  ...,  0.5053,  0.0971,  1.0850],\n",
       "          [-1.2883,  0.0584, -0.2668,  ...,  0.5149,  0.1054,  1.6861],\n",
       "          [-0.4440, -1.2503,  0.4363,  ...,  1.1043,  0.8403,  0.6618]],\n",
       " \n",
       "         [[-0.1361,  0.0898, -0.0243,  ...,  0.9252,  0.0271,  0.6004],\n",
       "          [-0.6104,  0.3142, -0.1762,  ...,  0.7918, -0.2915,  0.5835],\n",
       "          [-0.9518,  0.8436, -0.6670,  ...,  0.6580,  0.1580,  0.7574],\n",
       "          ...,\n",
       "          [-0.8174, -0.8920,  1.0149,  ...,  0.7384,  0.2734,  0.8379],\n",
       "          [-1.1361, -0.0739, -0.0643,  ...,  0.7002,  0.1879,  2.2353],\n",
       "          [-0.8765, -0.5891,  0.4028,  ...,  1.1088,  0.3058,  1.6114]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers['vit_layers'][1](out[0], *layer_args['vit_layers'],  **layer_kwargs['vit_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7876, -0.3205, -0.0842,  ..., -0.6614, -0.0151, -0.5240],\n",
       "          [-0.0952, -0.0247,  0.3759,  ..., -0.2078,  0.4916, -0.4537],\n",
       "          [-0.5563,  0.5105, -0.6659,  ..., -0.2041,  0.5277,  0.8380],\n",
       "          ...,\n",
       "          [-0.1493,  1.2919,  1.5551,  ...,  0.2978, -1.4789,  0.2294],\n",
       "          [ 0.2532,  0.0649, -0.7901,  ..., -0.4740, -1.6942, -0.6370],\n",
       "          [ 0.3278, -0.4323,  0.2681,  ..., -0.4160,  0.3958, -0.1349]],\n",
       " \n",
       "         [[-0.7876, -0.3205, -0.0842,  ..., -0.6614, -0.0151, -0.5240],\n",
       "          [-0.0952, -0.0247,  0.3759,  ..., -0.2078,  0.4916, -0.4537],\n",
       "          [-0.5563,  0.5105, -0.6659,  ..., -0.2041,  0.5277,  0.8380],\n",
       "          ...,\n",
       "          [-0.1493,  1.2919,  1.5551,  ...,  0.2978, -1.4789,  0.2294],\n",
       "          [ 0.2532,  0.0649, -0.7901,  ..., -0.4740, -1.6942, -0.6370],\n",
       "          [ 0.3278, -0.4323,  0.2681,  ..., -0.4160,  0.3958, -0.1349]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0.]]],\n",
       " \n",
       " \n",
       "         [[[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0.]]]], device='cuda:0'),\n",
       " None,\n",
       " tensor([[[-0.7125,  0.3542,  0.7332,  ..., -0.5284, -0.0729,  0.0879],\n",
       "          [-0.9326, -0.9130, -0.3849,  ..., -0.3952, -0.8801,  0.9865],\n",
       "          [-0.2113,  0.1189,  1.1975,  ...,  0.2811, -0.8168,  0.2782],\n",
       "          ...,\n",
       "          [ 0.2122, -0.7599, -0.0777,  ...,  0.5073,  0.3702, -0.0031],\n",
       "          [ 0.6824, -0.3208,  0.6164,  ..., -0.0497,  1.5667,  0.4806],\n",
       "          [ 0.7810, -0.7398,  1.0975,  ...,  0.1164, -1.0482, -0.7280]],\n",
       " \n",
       "         [[ 0.0457,  0.5419,  0.7550,  ..., -0.0269, -0.6244,  0.9082],\n",
       "          [-0.8845, -0.2183,  0.2585,  ...,  0.9610, -0.3391,  1.6280],\n",
       "          [-0.1610, -0.2631, -1.1667,  ...,  1.0118, -0.2163,  2.4884],\n",
       "          ...,\n",
       "          [-1.2052, -0.1580,  0.1309,  ...,  0.7388, -1.3360,  1.4888],\n",
       "          [-0.2517,  0.3308,  0.3758,  ...,  0.4206, -0.6858, -0.0537],\n",
       "          [-0.3877,  0.5926, -0.1315,  ...,  0.1013,  0.0569,  2.2346]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0.]]],\n",
       " \n",
       " \n",
       "         [[[-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "            -0., -0., -0., -0.]]]], device='cuda:0'),\n",
       " None,\n",
       " False,\n",
       " 32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_inputs['qformer_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5611, -0.4313,  0.7348,  ..., -0.5339,  0.2271, -0.6419],\n",
       "          [-0.0081,  2.1908,  0.5378,  ...,  0.0602,  0.0924, -0.4073],\n",
       "          [-0.3419,  0.2889, -0.3378,  ..., -0.3128,  0.8518, -0.0093],\n",
       "          ...,\n",
       "          [-0.4123,  1.7034, -0.6390,  ...,  0.3584, -0.3071, -0.3509],\n",
       "          [-0.3916,  0.2184,  0.2309,  ..., -0.4097,  0.2242, -0.1063],\n",
       "          [ 0.1907,  1.5663,  0.5063,  ..., -0.2080,  0.2138, -0.3192]],\n",
       " \n",
       "         [[-0.6695, -0.3799,  0.7295,  ..., -0.4814,  0.0236, -0.6724],\n",
       "          [-0.1965, -0.3873, -0.5339,  ..., -0.6227,  0.0911,  0.0212],\n",
       "          [-0.8092,  0.3860, -0.2228,  ..., -0.5321, -0.2148, -0.6384],\n",
       "          ...,\n",
       "          [-0.1741,  0.9254, -0.0700,  ...,  0.0874, -1.2012, -0.1439],\n",
       "          [-0.3797,  0.2424, -0.4833,  ..., -0.7007, -0.8832, -0.7238],\n",
       "          [ 0.4638, -0.7695, -0.3554,  ..., -0.5841, -0.0452, -0.0482]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward0>),\n",
       " (tensor([[[[-1.7426,  0.2527,  1.7284,  ..., -0.5379,  2.2770, -0.5293],\n",
       "            [-2.1936,  1.0225,  2.0931,  ..., -1.4654,  1.6426, -1.1523],\n",
       "            [-4.0090, -0.4669,  2.5241,  ..., -1.1154,  2.2246, -0.2516],\n",
       "            ...,\n",
       "            [-3.3144,  1.3060,  2.3824,  ..., -1.1824,  1.4067, -0.1366],\n",
       "            [-3.0173,  0.0188,  1.7052,  ..., -0.9845,  1.6327, -0.2418],\n",
       "            [-2.0346,  0.6591,  1.4872,  ..., -0.7851,  2.3408, -1.1299]],\n",
       "  \n",
       "           [[-2.7478,  3.7550, -4.0788,  ..., -2.1270,  3.9505,  1.0892],\n",
       "            [-3.8315,  3.5415, -3.9039,  ..., -1.7877,  4.3625,  0.8698],\n",
       "            [-3.5459,  3.5241, -3.4317,  ..., -2.0070,  4.4055,  0.9214],\n",
       "            ...,\n",
       "            [-3.7869,  3.2775, -3.9355,  ..., -2.4593,  4.5060,  1.3617],\n",
       "            [-3.0420,  3.9243, -3.6340,  ..., -2.2467,  4.4966,  0.8398],\n",
       "            [-2.9885,  3.6175, -3.6710,  ..., -1.9921,  4.6328,  1.2275]],\n",
       "  \n",
       "           [[ 1.3924,  4.9049, -1.7552,  ..., -0.4193, -2.1027,  0.1648],\n",
       "            [ 1.8330,  5.2244, -0.9645,  ..., -0.3222, -3.3263,  0.4558],\n",
       "            [ 0.1955,  3.3746, -0.5799,  ...,  1.1173, -1.6927,  0.1691],\n",
       "            ...,\n",
       "            [ 2.6095,  5.3664, -1.0879,  ...,  0.1719, -3.6555,  0.6661],\n",
       "            [ 0.6007,  4.5137, -1.0096,  ...,  0.7298, -1.4749,  0.9084],\n",
       "            [ 2.3790,  4.4126, -1.0833,  ..., -0.2864, -4.0144,  0.3287]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 4.0254,  5.3143,  4.4063,  ...,  5.3301,  3.2246, -5.0406],\n",
       "            [ 3.6702,  5.0018,  4.6773,  ...,  5.5495,  3.2284, -4.9949],\n",
       "            [ 3.8103,  4.5260,  5.0497,  ...,  5.8504,  2.6462, -4.4099],\n",
       "            ...,\n",
       "            [ 4.0367,  5.0046,  4.3558,  ...,  5.2681,  3.7526, -3.9353],\n",
       "            [ 4.0777,  5.2801,  4.8919,  ...,  5.9271,  3.2300, -4.3761],\n",
       "            [ 3.5975,  4.9448,  4.3677,  ...,  5.7789,  3.0004, -4.0889]],\n",
       "  \n",
       "           [[-0.4958,  0.9595, -1.2258,  ...,  0.1549,  1.0148,  0.5841],\n",
       "            [ 0.0551,  0.9384, -1.6991,  ..., -0.2527,  0.6757,  1.3260],\n",
       "            [-0.4934,  1.4370, -1.2770,  ..., -0.9537,  0.7751,  1.0220],\n",
       "            ...,\n",
       "            [-0.4013,  0.5917, -1.6080,  ...,  0.0926,  0.2437,  0.2156],\n",
       "            [-0.0936,  1.0620, -0.6952,  ...,  0.2630,  1.2032,  1.0095],\n",
       "            [ 0.3620,  0.1742, -1.6091,  ..., -0.6771, -0.4892,  0.5879]],\n",
       "  \n",
       "           [[ 0.4105, -0.1538,  0.8694,  ..., -1.1033, -0.3848, -1.1660],\n",
       "            [-0.1155, -0.4170, -0.0627,  ..., -0.8235,  0.0848, -1.6725],\n",
       "            [ 0.3683, -0.4841, -0.0685,  ..., -0.2626,  0.4242, -1.4651],\n",
       "            ...,\n",
       "            [ 0.4227,  0.3204,  0.1761,  ..., -0.5606, -0.3172, -1.5169],\n",
       "            [ 1.0363, -0.0198, -0.6745,  ..., -0.4545,  0.9025, -1.9206],\n",
       "            [ 0.0199, -0.4970,  0.6197,  ..., -1.1668,  0.2750, -1.0439]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.7426,  0.2527,  1.7284,  ..., -0.5379,  2.2770, -0.5293],\n",
       "            [-2.1936,  1.0225,  2.0931,  ..., -1.4654,  1.6426, -1.1523],\n",
       "            [-4.0090, -0.4669,  2.5241,  ..., -1.1154,  2.2246, -0.2516],\n",
       "            ...,\n",
       "            [-3.3144,  1.3060,  2.3824,  ..., -1.1824,  1.4067, -0.1366],\n",
       "            [-3.0173,  0.0188,  1.7052,  ..., -0.9845,  1.6327, -0.2418],\n",
       "            [-2.0346,  0.6591,  1.4872,  ..., -0.7851,  2.3408, -1.1299]],\n",
       "  \n",
       "           [[-2.7478,  3.7550, -4.0788,  ..., -2.1270,  3.9505,  1.0892],\n",
       "            [-3.8315,  3.5415, -3.9039,  ..., -1.7877,  4.3625,  0.8698],\n",
       "            [-3.5459,  3.5241, -3.4317,  ..., -2.0070,  4.4055,  0.9214],\n",
       "            ...,\n",
       "            [-3.7869,  3.2775, -3.9355,  ..., -2.4593,  4.5060,  1.3617],\n",
       "            [-3.0420,  3.9243, -3.6340,  ..., -2.2467,  4.4966,  0.8398],\n",
       "            [-2.9885,  3.6175, -3.6710,  ..., -1.9921,  4.6328,  1.2275]],\n",
       "  \n",
       "           [[ 1.3924,  4.9049, -1.7552,  ..., -0.4193, -2.1027,  0.1648],\n",
       "            [ 1.8330,  5.2244, -0.9645,  ..., -0.3222, -3.3263,  0.4558],\n",
       "            [ 0.1955,  3.3746, -0.5799,  ...,  1.1173, -1.6927,  0.1691],\n",
       "            ...,\n",
       "            [ 2.6095,  5.3664, -1.0879,  ...,  0.1719, -3.6555,  0.6661],\n",
       "            [ 0.6007,  4.5137, -1.0096,  ...,  0.7298, -1.4749,  0.9084],\n",
       "            [ 2.3790,  4.4126, -1.0833,  ..., -0.2864, -4.0144,  0.3287]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 4.0254,  5.3143,  4.4063,  ...,  5.3301,  3.2246, -5.0406],\n",
       "            [ 3.6702,  5.0018,  4.6773,  ...,  5.5495,  3.2284, -4.9949],\n",
       "            [ 3.8103,  4.5260,  5.0497,  ...,  5.8504,  2.6462, -4.4099],\n",
       "            ...,\n",
       "            [ 4.0367,  5.0046,  4.3558,  ...,  5.2681,  3.7526, -3.9353],\n",
       "            [ 4.0777,  5.2801,  4.8919,  ...,  5.9271,  3.2300, -4.3761],\n",
       "            [ 3.5975,  4.9448,  4.3677,  ...,  5.7789,  3.0004, -4.0889]],\n",
       "  \n",
       "           [[-0.4958,  0.9595, -1.2258,  ...,  0.1549,  1.0148,  0.5841],\n",
       "            [ 0.0551,  0.9384, -1.6991,  ..., -0.2527,  0.6757,  1.3260],\n",
       "            [-0.4934,  1.4370, -1.2770,  ..., -0.9537,  0.7751,  1.0220],\n",
       "            ...,\n",
       "            [-0.4013,  0.5917, -1.6080,  ...,  0.0926,  0.2437,  0.2156],\n",
       "            [-0.0936,  1.0620, -0.6952,  ...,  0.2630,  1.2032,  1.0095],\n",
       "            [ 0.3620,  0.1742, -1.6091,  ..., -0.6771, -0.4892,  0.5879]],\n",
       "  \n",
       "           [[ 0.4105, -0.1538,  0.8694,  ..., -1.1033, -0.3848, -1.1660],\n",
       "            [-0.1155, -0.4170, -0.0627,  ..., -0.8235,  0.0848, -1.6725],\n",
       "            [ 0.3683, -0.4841, -0.0685,  ..., -0.2626,  0.4242, -1.4651],\n",
       "            ...,\n",
       "            [ 0.4227,  0.3204,  0.1761,  ..., -0.5606, -0.3172, -1.5169],\n",
       "            [ 1.0363, -0.0198, -0.6745,  ..., -0.4545,  0.9025, -1.9206],\n",
       "            [ 0.0199, -0.4970,  0.6197,  ..., -1.1668,  0.2750, -1.0439]]]],\n",
       "         device='cuda:0', grad_fn=<PermuteBackward0>),\n",
       "  tensor([[[[-1.6132e-01, -1.9777e-01,  6.2108e-02,  ...,  4.3885e-01,\n",
       "             -1.7953e-01, -6.6988e-03],\n",
       "            [-4.2562e-01, -4.5420e-02,  2.8826e-01,  ...,  1.9967e-01,\n",
       "              1.6694e-01,  7.7524e-01],\n",
       "            [-8.5928e-01, -1.2535e-01,  4.3205e-01,  ...,  1.3676e+00,\n",
       "              4.2652e-02,  5.7778e-01],\n",
       "            ...,\n",
       "            [-1.7935e-01, -1.2407e-02, -4.1336e-01,  ..., -4.3032e-01,\n",
       "             -5.0661e-01,  3.6595e-01],\n",
       "            [ 4.7784e-01, -2.3151e-01,  4.7365e-01,  ...,  1.0538e-01,\n",
       "             -6.8835e-02, -3.9348e-01],\n",
       "            [-1.0276e-02,  9.6232e-02, -9.5378e-03,  ...,  7.5653e-02,\n",
       "              1.5921e-01,  1.2661e-01]],\n",
       "  \n",
       "           [[-2.1616e-01,  1.8678e-01,  2.1142e-01,  ..., -4.8037e-01,\n",
       "              1.5050e-01,  3.6128e-01],\n",
       "            [ 2.8241e-02,  1.4519e-01, -1.3216e-01,  ...,  1.2693e-01,\n",
       "             -2.4401e-01,  2.6225e-01],\n",
       "            [-5.7787e-01, -5.3528e-01,  1.8614e-01,  ...,  5.9871e-01,\n",
       "             -2.0420e-01, -8.3836e-02],\n",
       "            ...,\n",
       "            [-2.5724e-02,  5.5927e-01, -2.0056e-01,  ..., -1.6714e-01,\n",
       "             -3.6315e-02,  4.7540e-01],\n",
       "            [ 3.0298e-01,  2.1632e-01,  6.3147e-01,  ...,  5.2655e-01,\n",
       "              8.5994e-01, -3.4367e-02],\n",
       "            [-4.4981e-01,  3.6036e-01,  9.1990e-01,  ..., -9.0668e-01,\n",
       "             -3.6985e-01,  5.5024e-01]],\n",
       "  \n",
       "           [[ 6.4948e-02,  4.7726e-01,  3.0883e-01,  ..., -2.9750e-01,\n",
       "             -6.2788e-02,  9.0698e-02],\n",
       "            [-9.7473e-01, -6.4983e-01,  7.5419e-02,  ...,  1.0404e-01,\n",
       "             -2.0122e-01, -2.9811e-01],\n",
       "            [-4.1846e-01, -5.8537e-01,  1.6265e-01,  ...,  3.4258e-02,\n",
       "             -2.0670e-01,  8.4758e-01],\n",
       "            ...,\n",
       "            [-1.0787e-01, -1.8664e-01, -4.0160e-01,  ...,  3.1751e-01,\n",
       "             -1.3550e-01,  2.5242e-02],\n",
       "            [ 2.0876e-01,  3.8203e-01,  6.0718e-01,  ...,  2.0911e-01,\n",
       "             -1.1891e+00,  3.9629e-01],\n",
       "            [-2.0169e-01,  1.5351e-01,  3.3841e-01,  ..., -4.7765e-01,\n",
       "              5.1804e-01, -6.5722e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-3.0526e-01, -3.4362e-01,  2.8872e-01,  ...,  1.4053e-01,\n",
       "             -3.3810e-01,  2.7446e-01],\n",
       "            [ 2.1490e-01, -1.2535e-02, -6.1873e-02,  ..., -4.7495e-01,\n",
       "              1.2965e-01,  2.4768e-01],\n",
       "            [-3.5820e-01,  2.8465e-01,  2.7055e-01,  ..., -4.6353e-02,\n",
       "              7.2415e-01, -4.1337e-01],\n",
       "            ...,\n",
       "            [ 6.2307e-01, -2.0690e-01, -2.2744e-01,  ...,  4.6605e-02,\n",
       "             -6.8086e-01,  3.8976e-01],\n",
       "            [-3.4686e-01, -5.2393e-02, -6.1102e-02,  ..., -2.8853e-01,\n",
       "              3.4042e-01, -4.3701e-01],\n",
       "            [-6.4899e-01, -3.6930e-01, -2.0457e-02,  ...,  4.4448e-01,\n",
       "              3.4927e-01,  2.2927e-01]],\n",
       "  \n",
       "           [[ 2.3730e-03, -1.7326e-01, -1.0600e-01,  ..., -3.4855e-02,\n",
       "             -7.3439e-04, -2.1353e-01],\n",
       "            [ 1.1948e-01,  5.8429e-02, -1.7470e-02,  ...,  7.6608e-02,\n",
       "              3.6228e-02,  2.6081e-01],\n",
       "            [ 2.5657e-01, -1.9929e-01, -9.6220e-01,  ..., -3.8119e-02,\n",
       "              9.8450e-01, -6.6390e-01],\n",
       "            ...,\n",
       "            [ 1.6944e-01, -2.6799e-02, -2.9567e-01,  ..., -3.6752e-01,\n",
       "              2.2093e-02, -3.6824e-01],\n",
       "            [ 7.8678e-01,  1.0653e-01,  9.8594e-02,  ..., -2.3505e-01,\n",
       "              3.6011e-02, -1.1146e-01],\n",
       "            [-2.7503e-01,  3.7849e-01,  2.0251e-01,  ...,  1.9756e-01,\n",
       "              1.0761e-01, -5.3689e-01]],\n",
       "  \n",
       "           [[-5.8893e-01, -1.2319e-01,  2.3031e-01,  ..., -4.8535e-01,\n",
       "             -6.4391e-02, -1.7639e-01],\n",
       "            [-2.3821e-01, -2.2866e-01,  1.3286e-01,  ...,  3.5992e-01,\n",
       "             -5.4884e-01, -1.8687e-01],\n",
       "            [-7.4974e-01, -4.8048e-01,  1.0557e-02,  ...,  4.6205e-01,\n",
       "             -4.9324e-01,  3.8888e-01],\n",
       "            ...,\n",
       "            [-1.2450e+00,  2.9424e-01,  1.2656e-01,  ..., -1.3655e+00,\n",
       "             -4.2124e-01,  1.3647e-01],\n",
       "            [ 6.5546e-01, -5.2208e-01,  7.7213e-01,  ..., -1.1790e+00,\n",
       "              1.5464e-02,  3.0198e-01],\n",
       "            [-2.7760e-01, -2.4039e-01, -4.3594e-01,  ...,  4.8473e-01,\n",
       "             -1.9405e-01,  2.6780e-01]]],\n",
       "  \n",
       "  \n",
       "          [[[-1.6132e-01, -1.9777e-01,  6.2108e-02,  ...,  4.3885e-01,\n",
       "             -1.7953e-01, -6.6988e-03],\n",
       "            [-4.2562e-01, -4.5420e-02,  2.8826e-01,  ...,  1.9967e-01,\n",
       "              1.6694e-01,  7.7524e-01],\n",
       "            [-8.5928e-01, -1.2535e-01,  4.3205e-01,  ...,  1.3676e+00,\n",
       "              4.2652e-02,  5.7778e-01],\n",
       "            ...,\n",
       "            [-1.7935e-01, -1.2407e-02, -4.1336e-01,  ..., -4.3032e-01,\n",
       "             -5.0661e-01,  3.6595e-01],\n",
       "            [ 4.7784e-01, -2.3151e-01,  4.7365e-01,  ...,  1.0538e-01,\n",
       "             -6.8835e-02, -3.9348e-01],\n",
       "            [-1.0276e-02,  9.6232e-02, -9.5378e-03,  ...,  7.5653e-02,\n",
       "              1.5921e-01,  1.2661e-01]],\n",
       "  \n",
       "           [[-2.1616e-01,  1.8678e-01,  2.1142e-01,  ..., -4.8037e-01,\n",
       "              1.5050e-01,  3.6128e-01],\n",
       "            [ 2.8241e-02,  1.4519e-01, -1.3216e-01,  ...,  1.2693e-01,\n",
       "             -2.4401e-01,  2.6225e-01],\n",
       "            [-5.7787e-01, -5.3528e-01,  1.8614e-01,  ...,  5.9871e-01,\n",
       "             -2.0420e-01, -8.3836e-02],\n",
       "            ...,\n",
       "            [-2.5724e-02,  5.5927e-01, -2.0056e-01,  ..., -1.6714e-01,\n",
       "             -3.6315e-02,  4.7540e-01],\n",
       "            [ 3.0298e-01,  2.1632e-01,  6.3147e-01,  ...,  5.2655e-01,\n",
       "              8.5994e-01, -3.4367e-02],\n",
       "            [-4.4981e-01,  3.6036e-01,  9.1990e-01,  ..., -9.0668e-01,\n",
       "             -3.6985e-01,  5.5024e-01]],\n",
       "  \n",
       "           [[ 6.4948e-02,  4.7726e-01,  3.0883e-01,  ..., -2.9750e-01,\n",
       "             -6.2788e-02,  9.0698e-02],\n",
       "            [-9.7473e-01, -6.4983e-01,  7.5419e-02,  ...,  1.0404e-01,\n",
       "             -2.0122e-01, -2.9811e-01],\n",
       "            [-4.1846e-01, -5.8537e-01,  1.6265e-01,  ...,  3.4258e-02,\n",
       "             -2.0670e-01,  8.4758e-01],\n",
       "            ...,\n",
       "            [-1.0787e-01, -1.8664e-01, -4.0160e-01,  ...,  3.1751e-01,\n",
       "             -1.3550e-01,  2.5242e-02],\n",
       "            [ 2.0876e-01,  3.8203e-01,  6.0718e-01,  ...,  2.0911e-01,\n",
       "             -1.1891e+00,  3.9629e-01],\n",
       "            [-2.0169e-01,  1.5351e-01,  3.3841e-01,  ..., -4.7765e-01,\n",
       "              5.1804e-01, -6.5722e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-3.0526e-01, -3.4362e-01,  2.8872e-01,  ...,  1.4053e-01,\n",
       "             -3.3810e-01,  2.7446e-01],\n",
       "            [ 2.1490e-01, -1.2535e-02, -6.1873e-02,  ..., -4.7495e-01,\n",
       "              1.2965e-01,  2.4768e-01],\n",
       "            [-3.5820e-01,  2.8465e-01,  2.7055e-01,  ..., -4.6353e-02,\n",
       "              7.2415e-01, -4.1337e-01],\n",
       "            ...,\n",
       "            [ 6.2307e-01, -2.0690e-01, -2.2744e-01,  ...,  4.6605e-02,\n",
       "             -6.8086e-01,  3.8976e-01],\n",
       "            [-3.4686e-01, -5.2393e-02, -6.1102e-02,  ..., -2.8853e-01,\n",
       "              3.4042e-01, -4.3701e-01],\n",
       "            [-6.4899e-01, -3.6930e-01, -2.0457e-02,  ...,  4.4448e-01,\n",
       "              3.4927e-01,  2.2927e-01]],\n",
       "  \n",
       "           [[ 2.3730e-03, -1.7326e-01, -1.0600e-01,  ..., -3.4855e-02,\n",
       "             -7.3439e-04, -2.1353e-01],\n",
       "            [ 1.1948e-01,  5.8429e-02, -1.7470e-02,  ...,  7.6608e-02,\n",
       "              3.6228e-02,  2.6081e-01],\n",
       "            [ 2.5657e-01, -1.9929e-01, -9.6220e-01,  ..., -3.8119e-02,\n",
       "              9.8450e-01, -6.6390e-01],\n",
       "            ...,\n",
       "            [ 1.6944e-01, -2.6799e-02, -2.9567e-01,  ..., -3.6752e-01,\n",
       "              2.2093e-02, -3.6824e-01],\n",
       "            [ 7.8678e-01,  1.0653e-01,  9.8594e-02,  ..., -2.3505e-01,\n",
       "              3.6011e-02, -1.1146e-01],\n",
       "            [-2.7503e-01,  3.7849e-01,  2.0251e-01,  ...,  1.9756e-01,\n",
       "              1.0761e-01, -5.3689e-01]],\n",
       "  \n",
       "           [[-5.8893e-01, -1.2319e-01,  2.3031e-01,  ..., -4.8535e-01,\n",
       "             -6.4391e-02, -1.7639e-01],\n",
       "            [-2.3821e-01, -2.2866e-01,  1.3286e-01,  ...,  3.5992e-01,\n",
       "             -5.4884e-01, -1.8687e-01],\n",
       "            [-7.4974e-01, -4.8048e-01,  1.0557e-02,  ...,  4.6205e-01,\n",
       "             -4.9324e-01,  3.8888e-01],\n",
       "            ...,\n",
       "            [-1.2450e+00,  2.9424e-01,  1.2656e-01,  ..., -1.3655e+00,\n",
       "             -4.2124e-01,  1.3647e-01],\n",
       "            [ 6.5546e-01, -5.2208e-01,  7.7213e-01,  ..., -1.1790e+00,\n",
       "              1.5464e-02,  3.0198e-01],\n",
       "            [-2.7760e-01, -2.4039e-01, -4.3594e-01,  ...,  4.8473e-01,\n",
       "             -1.9405e-01,  2.6780e-01]]]], device='cuda:0',\n",
       "         grad_fn=<PermuteBackward0>)))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers['qformer_layers'][0](*first_inputs['qformer_layers'], **layer_kwargs['qformer_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_kwargs['qformer_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2560])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_inputs['llm_layers'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]],\n",
       "        device='cuda:0'),\n",
       " 'layer_head_mask': None,\n",
       " 'past_key_value': (tensor([[[[ 3.4119e+00, -1.4275e+00,  1.6319e+00,  ...,  5.2653e-01,\n",
       "              2.4766e+00, -1.3366e+00],\n",
       "            [ 1.7859e+00, -2.7154e-01,  8.6161e-01,  ..., -8.4367e-02,\n",
       "              1.5484e+00,  4.6518e-01],\n",
       "            [ 1.8167e+00, -5.6734e-01,  1.4872e+00,  ..., -7.3273e-01,\n",
       "              6.8322e-01, -1.6523e-01],\n",
       "            ...,\n",
       "            [ 2.3114e+00, -3.1942e-01, -6.0153e-01,  ..., -1.4472e+00,\n",
       "              1.0140e+00, -6.6369e-01],\n",
       "            [ 6.2211e-01,  4.0976e-01, -1.6052e+00,  ..., -1.4000e-01,\n",
       "              2.1058e+00,  1.0663e-01],\n",
       "            [ 7.4871e-01,  4.7670e-01, -1.7015e+00,  ..., -9.2547e-01,\n",
       "              6.0385e-01, -8.4265e-01]],\n",
       "  \n",
       "           [[ 2.0427e-01, -8.5242e-01, -5.9123e-02,  ..., -5.6687e-03,\n",
       "             -2.8445e-01, -1.1846e+00],\n",
       "            [-8.1269e-01, -1.0259e+00,  3.2671e-01,  ..., -2.8126e-01,\n",
       "             -1.0370e-01, -7.9321e-01],\n",
       "            [-2.8310e-01, -7.4087e-01,  8.5872e-01,  ..., -3.6546e-01,\n",
       "              4.2935e-01, -9.3342e-01],\n",
       "            ...,\n",
       "            [ 5.3545e-01, -2.3374e-01, -2.2597e-01,  ...,  1.8407e-01,\n",
       "             -9.8845e-02, -4.5007e-01],\n",
       "            [-6.4834e-01,  1.9003e-01,  3.9929e-01,  ..., -4.2594e-01,\n",
       "              4.0967e-01, -1.2712e+00],\n",
       "            [-4.6268e-01, -4.0754e-01, -2.9112e-01,  ...,  4.3913e-01,\n",
       "             -8.8759e-01, -6.9873e-01]],\n",
       "  \n",
       "           [[ 5.0181e-02, -4.1784e-02,  6.9774e-01,  ...,  1.7297e-01,\n",
       "              1.5866e-01, -1.2300e-01],\n",
       "            [ 5.4867e-03, -1.5819e-01,  6.0861e-02,  ..., -4.2177e-01,\n",
       "              6.7404e-01, -1.4309e+00],\n",
       "            [-2.2433e-01, -6.2800e-01,  8.0317e-01,  ...,  6.0717e-01,\n",
       "              6.2085e-01, -1.4795e+00],\n",
       "            ...,\n",
       "            [-1.2212e-01, -1.0642e+00,  2.9587e-01,  ...,  7.2693e-01,\n",
       "              6.9153e-01,  3.9739e-01],\n",
       "            [ 3.4705e-01,  9.2443e-01,  8.7636e-01,  ...,  4.6441e-02,\n",
       "              3.8274e-01,  5.6799e-01],\n",
       "            [ 8.9853e-01,  1.5421e-01, -2.1758e-01,  ...,  1.9111e-01,\n",
       "             -4.3737e-01,  1.7084e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-1.1458e-01,  2.1604e-01, -3.1669e-01,  ...,  2.8972e-01,\n",
       "             -2.3264e-01,  1.1245e-02],\n",
       "            [ 2.4547e-01,  4.4201e-01,  6.9218e-02,  ..., -3.3527e-01,\n",
       "              3.4900e-01,  2.9654e-01],\n",
       "            [-2.2622e-01,  6.6884e-01, -1.6840e-02,  ..., -4.3466e-01,\n",
       "             -5.1018e-02,  3.2379e-02],\n",
       "            ...,\n",
       "            [-3.2269e-01,  5.8766e-01,  3.4991e-01,  ..., -1.8881e-01,\n",
       "              8.5003e-01,  1.2432e-01],\n",
       "            [ 7.5743e-01, -4.3923e-02, -2.7492e-01,  ...,  9.5778e-03,\n",
       "              4.5905e-02, -3.9826e-02],\n",
       "            [ 7.8822e-01,  8.4676e-01, -1.0307e-01,  ...,  3.8864e-01,\n",
       "              2.4572e-01,  3.0369e-01]],\n",
       "  \n",
       "           [[-4.5411e-01, -2.7564e-01, -8.3391e-02,  ...,  2.0212e-01,\n",
       "             -1.3232e-01, -2.3003e-01],\n",
       "            [-7.4457e-01, -4.4484e-02, -7.3791e-02,  ..., -5.8712e-01,\n",
       "              5.5727e-01, -7.7964e-01],\n",
       "            [-9.5967e-02,  1.9513e-01, -7.3116e-01,  ...,  2.0002e-01,\n",
       "              5.7398e-01,  2.0607e-01],\n",
       "            ...,\n",
       "            [-1.1153e+00, -1.1982e-01,  1.0204e+00,  ...,  5.4690e-01,\n",
       "              8.9361e-02,  2.7254e-01],\n",
       "            [-1.6236e-01, -6.6547e-01, -4.9992e-01,  ...,  6.8218e-01,\n",
       "              2.3517e-01,  3.1663e-02],\n",
       "            [-1.7180e-01, -2.2451e-01, -7.2630e-01,  ...,  3.2025e-01,\n",
       "             -2.5666e-01,  3.3631e-01]],\n",
       "  \n",
       "           [[-5.1739e-01, -2.3838e+00, -1.4913e+00,  ..., -2.1300e+00,\n",
       "              4.8042e-01, -9.2978e-01],\n",
       "            [-1.0081e+00, -1.3937e-01, -5.0364e-02,  ..., -1.6413e+00,\n",
       "              1.0211e-02,  6.0102e-01],\n",
       "            [-2.7290e-01, -3.9072e-01, -2.1012e-01,  ..., -2.2066e+00,\n",
       "             -1.2495e+00,  1.0377e+00],\n",
       "            ...,\n",
       "            [-2.3125e+00, -1.3331e+00,  3.5848e-01,  ..., -9.9238e-01,\n",
       "              2.0977e-01, -6.0358e-01],\n",
       "            [-5.1204e-01, -7.5416e-01, -1.2597e-01,  ..., -1.7993e+00,\n",
       "             -1.6060e+00, -1.2167e+00],\n",
       "            [-1.9304e+00, -2.3983e-01, -1.0915e+00,  ..., -1.2810e+00,\n",
       "             -6.8363e-01,  9.2228e-01]]],\n",
       "  \n",
       "  \n",
       "          [[[ 3.9896e+00, -1.3145e+00,  1.7020e+00,  ..., -2.5079e-01,\n",
       "              2.9296e+00, -1.1627e+00],\n",
       "            [ 1.3871e-01, -2.7068e-01,  8.1414e-01,  ...,  6.7821e-01,\n",
       "              2.1169e+00,  2.3235e-02],\n",
       "            [ 1.9715e+00, -5.2636e-01,  7.5252e-01,  ..., -7.4234e-01,\n",
       "              1.1449e+00,  2.5917e-01],\n",
       "            ...,\n",
       "            [ 7.0822e-01,  3.6416e-01, -3.7893e-01,  ..., -9.2279e-01,\n",
       "              1.4353e+00,  4.0519e-01],\n",
       "            [-3.7768e-02, -6.7008e-02, -1.5691e+00,  ..., -8.4768e-01,\n",
       "              1.9182e+00, -5.3681e-02],\n",
       "            [ 7.4871e-01,  4.7670e-01, -1.7015e+00,  ..., -9.2547e-01,\n",
       "              6.0385e-01, -8.4265e-01]],\n",
       "  \n",
       "           [[-4.1624e-01, -1.1329e-02,  1.2420e-01,  ...,  2.1470e-01,\n",
       "             -6.5463e-01, -1.4658e+00],\n",
       "            [-1.0526e+00, -1.4735e+00,  3.8987e-01,  ..., -3.9238e-01,\n",
       "             -4.0710e-01,  7.1560e-02],\n",
       "            [-4.4427e-01, -8.6162e-01,  6.7839e-01,  ..., -4.4447e-01,\n",
       "             -5.0267e-01, -1.2838e+00],\n",
       "            ...,\n",
       "            [ 3.6008e-01, -6.3112e-01,  5.5723e-01,  ...,  8.4266e-01,\n",
       "              2.0369e-01, -2.6053e-01],\n",
       "            [ 4.0245e-01,  4.0493e-02,  5.3339e-01,  ..., -2.7586e-01,\n",
       "              3.4185e-01, -6.0619e-01],\n",
       "            [-4.6268e-01, -4.0754e-01, -2.9112e-01,  ...,  4.3913e-01,\n",
       "             -8.8759e-01, -6.9873e-01]],\n",
       "  \n",
       "           [[-5.8816e-02,  4.0884e-01,  6.4248e-01,  ...,  3.2498e-01,\n",
       "              4.8382e-01, -7.6135e-01],\n",
       "            [ 3.3361e-01,  4.0855e-01,  6.2271e-01,  ..., -4.0405e-01,\n",
       "              8.1581e-02, -8.8792e-01],\n",
       "            [ 2.0017e-01, -1.6873e-01,  7.6931e-01,  ...,  1.3718e-01,\n",
       "              3.0772e-01, -1.0122e+00],\n",
       "            ...,\n",
       "            [-4.1047e-01,  2.8161e-01,  9.1698e-01,  ...,  3.7576e-01,\n",
       "              1.5782e+00,  3.3161e-01],\n",
       "            [ 8.9623e-01,  1.1323e+00,  7.1017e-01,  ..., -5.1485e-01,\n",
       "             -1.7331e-01,  1.0502e-02],\n",
       "            [ 8.9853e-01,  1.5421e-01, -2.1758e-01,  ...,  1.9111e-01,\n",
       "             -4.3737e-01,  1.7084e-01]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-2.7664e-01, -3.1798e-01, -4.3811e-01,  ...,  1.0027e-01,\n",
       "             -3.6830e-01,  2.1835e-01],\n",
       "            [ 2.4971e-01, -4.2420e-01, -5.9570e-01,  ...,  1.7604e-01,\n",
       "              2.3712e-01, -2.9864e-01],\n",
       "            [ 2.6005e-01, -8.0269e-03, -4.6603e-02,  ..., -2.6840e-01,\n",
       "              7.6143e-01,  2.7879e-01],\n",
       "            ...,\n",
       "            [ 5.6953e-02,  4.8980e-01,  5.4131e-01,  ..., -1.2086e-01,\n",
       "             -7.7309e-02,  2.5522e-01],\n",
       "            [ 3.6610e-01, -1.2588e-01, -4.2330e-01,  ..., -4.0428e-01,\n",
       "             -3.2483e-01,  1.2614e-01],\n",
       "            [ 7.8822e-01,  8.4676e-01, -1.0307e-01,  ...,  3.8864e-01,\n",
       "              2.4572e-01,  3.0369e-01]],\n",
       "  \n",
       "           [[-2.8367e-01, -1.5149e-01, -3.9994e-01,  ...,  2.0013e-01,\n",
       "              1.7496e-01,  1.3002e-02],\n",
       "            [-8.7976e-01, -6.7711e-02, -2.2137e-01,  ..., -1.6537e-01,\n",
       "              4.0843e-01, -2.6501e-01],\n",
       "            [-7.3199e-02,  5.8200e-01, -6.4580e-02,  ...,  3.1673e-01,\n",
       "              3.8625e-01,  5.5463e-01],\n",
       "            ...,\n",
       "            [-5.6693e-01,  5.9553e-03, -1.6492e-01,  ..., -6.7874e-02,\n",
       "             -4.6495e-01, -1.7404e-01],\n",
       "            [-1.0455e+00, -1.2142e-03, -3.2630e-01,  ...,  7.7015e-01,\n",
       "             -5.4449e-01, -3.2113e-01],\n",
       "            [-1.7180e-01, -2.2451e-01, -7.2630e-01,  ...,  3.2025e-01,\n",
       "             -2.5666e-01,  3.3631e-01]],\n",
       "  \n",
       "           [[-8.4018e-01, -2.1619e+00, -1.2331e+00,  ..., -9.9150e-01,\n",
       "             -2.7241e-01, -6.7306e-01],\n",
       "            [ 6.1206e-01, -1.4945e+00, -6.3976e-01,  ..., -2.3377e+00,\n",
       "             -7.8093e-01, -2.9379e-01],\n",
       "            [-9.4687e-01,  2.3472e-01, -6.9389e-01,  ..., -2.0366e+00,\n",
       "             -7.3638e-01, -5.4397e-02],\n",
       "            ...,\n",
       "            [-1.4308e+00,  2.0030e-01, -9.3339e-01,  ..., -1.5167e+00,\n",
       "              2.6676e-01, -9.5633e-01],\n",
       "            [-1.5133e-02, -5.6230e-01,  2.5631e-01,  ..., -2.0858e+00,\n",
       "             -7.2397e-01, -6.9738e-01],\n",
       "            [-1.9304e+00, -2.3983e-01, -1.0915e+00,  ..., -1.2810e+00,\n",
       "             -6.8363e-01,  9.2228e-01]]]], device='cuda:0'),\n",
       "  tensor([[[[-0.3814,  0.6795, -0.0654,  ...,  0.3769,  0.0460,  0.4259],\n",
       "            [-0.6923, -0.4789, -0.2991,  ...,  0.0966,  0.2435, -0.0172],\n",
       "            [-0.5321, -0.2648, -0.2829,  ...,  0.1733,  0.3567,  0.2681],\n",
       "            ...,\n",
       "            [ 0.2740,  0.1855,  0.1566,  ..., -0.3827, -0.4734, -0.6157],\n",
       "            [-0.3453, -0.2378,  0.3438,  ...,  0.0064, -0.1064,  0.1797],\n",
       "            [ 0.1496, -0.1346,  0.7159,  ..., -0.2194, -0.1649,  0.2712]],\n",
       "  \n",
       "           [[ 0.3284, -0.0093, -0.1313,  ...,  0.2180, -0.5675, -0.6120],\n",
       "            [ 0.2065, -0.3120,  0.2538,  ...,  0.5280,  0.3646,  0.2336],\n",
       "            [ 0.0343, -0.1570,  0.0198,  ..., -0.2061, -0.2165,  0.3225],\n",
       "            ...,\n",
       "            [ 0.1771, -0.1785,  0.4322,  ..., -0.3618, -0.2300,  0.3213],\n",
       "            [-0.2307,  0.1447,  0.0112,  ...,  0.3951,  0.0397,  0.1310],\n",
       "            [ 0.1127, -0.1306, -0.0276,  ...,  0.0885, -0.1937,  0.1828]],\n",
       "  \n",
       "           [[ 0.1944, -0.0647,  0.0196,  ...,  0.0956, -0.0893,  0.0660],\n",
       "            [-0.3531, -0.0828,  0.1091,  ..., -0.2573,  0.4620,  0.0963],\n",
       "            [-0.0430, -0.1245,  0.2237,  ..., -0.2556,  0.1607, -0.0630],\n",
       "            ...,\n",
       "            [ 0.0152, -0.3331, -0.1702,  ...,  0.0100,  0.1657, -0.0886],\n",
       "            [ 0.1011,  0.1310,  0.0680,  ...,  0.1973, -0.2176,  0.1120],\n",
       "            [ 0.1334,  0.0351, -0.0199,  ..., -0.0963, -0.0192, -0.2130]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0833,  0.2179,  0.2652,  ...,  0.2375,  0.3516,  0.2981],\n",
       "            [ 0.1914, -0.1690,  0.0971,  ...,  0.0954,  0.0878,  0.3025],\n",
       "            [-0.2950, -0.0474,  0.1559,  ..., -0.2550,  0.0340, -0.1934],\n",
       "            ...,\n",
       "            [-0.0729,  0.1744,  0.3997,  ..., -0.0482,  0.0873,  0.3243],\n",
       "            [ 0.0274,  0.3352, -0.0151,  ..., -0.0021, -0.2496,  0.1275],\n",
       "            [-0.1618,  0.1950,  0.2229,  ...,  0.2720,  0.2434, -0.0967]],\n",
       "  \n",
       "           [[ 0.1771,  0.1595,  0.2348,  ..., -0.3956, -0.3315,  0.1951],\n",
       "            [-0.3257, -0.0023,  0.1353,  ..., -0.2704,  0.2216,  0.3871],\n",
       "            [-0.1776, -0.1107,  0.3138,  ..., -0.1946,  0.3819,  0.3749],\n",
       "            ...,\n",
       "            [ 0.2832,  0.2642,  0.2622,  ..., -0.5144,  0.0010, -0.2310],\n",
       "            [ 0.0043, -0.1270, -0.1432,  ..., -0.0051,  0.2948, -0.0647],\n",
       "            [ 0.0741,  0.3326, -0.0382,  ...,  0.1664, -0.1396,  0.0575]],\n",
       "  \n",
       "           [[-0.6475,  0.0477,  0.3084,  ...,  0.0051, -0.0333, -0.6528],\n",
       "            [-0.0402,  0.0109,  0.1616,  ..., -0.0806, -0.3659, -0.2291],\n",
       "            [ 0.2130,  0.5460,  0.1521,  ..., -0.1454, -0.0053, -0.5258],\n",
       "            ...,\n",
       "            [-0.4552,  0.7987,  0.3524,  ...,  0.1708,  0.7358,  0.1870],\n",
       "            [-0.5109, -0.0558, -0.3413,  ...,  0.0183,  0.1459, -0.1703],\n",
       "            [-0.3228, -0.7602, -0.0480,  ...,  0.0692,  0.0468, -0.1515]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.5360,  0.6180, -0.0309,  ...,  0.6376,  0.4239, -0.0682],\n",
       "            [-0.6019,  0.1859,  0.1521,  ..., -0.0970, -0.2608,  0.2002],\n",
       "            [-0.0865, -0.0072, -0.2515,  ..., -0.4279,  0.1869,  0.0493],\n",
       "            ...,\n",
       "            [ 0.2282,  0.0935,  0.2216,  ..., -0.0809, -0.6652, -0.4752],\n",
       "            [-0.1619,  0.0678, -0.1071,  ..., -0.1788, -0.0725, -0.2161],\n",
       "            [ 0.1496, -0.1346,  0.7159,  ..., -0.2194, -0.1649,  0.2712]],\n",
       "  \n",
       "           [[ 0.1451,  0.0117,  0.4955,  ...,  0.0685, -0.3135, -0.4043],\n",
       "            [ 0.6015,  0.1990,  0.0666,  ...,  0.5443,  0.5377,  0.1837],\n",
       "            [ 0.3055,  0.0040, -0.1623,  ..., -0.5589, -0.4056,  0.4731],\n",
       "            ...,\n",
       "            [-0.0164, -0.1177,  0.1211,  ..., -0.1472,  0.0044,  0.1872],\n",
       "            [-0.2253, -0.0619, -0.2018,  ...,  0.2248, -0.0748,  0.1011],\n",
       "            [ 0.1127, -0.1306, -0.0276,  ...,  0.0885, -0.1937,  0.1828]],\n",
       "  \n",
       "           [[ 0.1863, -0.0428, -0.0333,  ...,  0.0087, -0.0552,  0.2047],\n",
       "            [-0.1047, -0.0270,  0.1995,  ..., -0.3557,  0.2850, -0.0093],\n",
       "            [ 0.2345, -0.1085,  0.0391,  ..., -0.1970,  0.1932, -0.0571],\n",
       "            ...,\n",
       "            [ 0.0575, -0.3401, -0.1766,  ...,  0.3417, -0.0693,  0.0152],\n",
       "            [ 0.1088,  0.1139, -0.0382,  ...,  0.0657, -0.0196,  0.2871],\n",
       "            [ 0.1334,  0.0351, -0.0199,  ..., -0.0963, -0.0192, -0.2130]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.1739,  0.0411,  0.2637,  ...,  0.1808,  0.1697,  0.0782],\n",
       "            [-0.1258, -0.1691, -0.3540,  ..., -0.1128, -0.1786, -0.1526],\n",
       "            [-0.0690, -0.0626, -0.0330,  ..., -0.0210, -0.1472, -0.3348],\n",
       "            ...,\n",
       "            [-0.0677, -0.0274,  0.0022,  ...,  0.1557,  0.2501,  0.2439],\n",
       "            [-0.1213, -0.0095, -0.0990,  ...,  0.2003, -0.4044,  0.0155],\n",
       "            [-0.1618,  0.1950,  0.2229,  ...,  0.2720,  0.2434, -0.0967]],\n",
       "  \n",
       "           [[ 0.0818,  0.3839, -0.0233,  ..., -0.1660, -0.2479,  0.2176],\n",
       "            [-0.4637, -0.2909,  0.3302,  ..., -0.2005, -0.0080,  0.3280],\n",
       "            [-0.3203,  0.1661,  0.0870,  ..., -0.2283,  0.0659,  0.5906],\n",
       "            ...,\n",
       "            [ 0.1148, -0.1955, -0.2303,  ..., -0.0589, -0.1291, -0.0807],\n",
       "            [ 0.0822, -0.2093, -0.0641,  ...,  0.1067,  0.0944, -0.1248],\n",
       "            [ 0.0741,  0.3326, -0.0382,  ...,  0.1664, -0.1396,  0.0575]],\n",
       "  \n",
       "           [[-0.3394,  0.2128,  0.1492,  ...,  0.0056,  0.0836, -0.6340],\n",
       "            [-0.0513, -0.1794, -0.1271,  ..., -0.1926, -0.1792, -0.1216],\n",
       "            [ 0.1318,  0.2929, -0.4020,  ...,  0.1705,  0.2310, -0.4705],\n",
       "            ...,\n",
       "            [ 0.0597,  0.1825,  0.0396,  ..., -0.0556,  0.2577,  0.2384],\n",
       "            [ 0.1124,  0.6324, -0.2973,  ..., -0.2968,  0.2636,  0.2895],\n",
       "            [-0.3228, -0.7602, -0.0480,  ...,  0.0692,  0.0468, -0.1515]]]],\n",
       "         device='cuda:0')),\n",
       " 'output_attentions': False,\n",
       " 'use_cache': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_kwargs['llm_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = model.vision_model.encoder.layers\n",
    "                # *[layer for layer in self.model.qformer.encoder.layer],\n",
    "                # *[layer for layer in self.model.language_model.model.decoder.layers]]\n",
    "\n",
    "modules[0] = modules[0].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calib_set = b._get_calibration_set()\n",
    "calib_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inps = []\n",
    "layer_kwargs = {}\n",
    "# get input and kwargs to layer 0\n",
    "# with_kwargs is only supported in PyTorch 2.0\n",
    "# use this Catcher hack for now\n",
    "class Catcher(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # assume first input to forward is hidden states\n",
    "        if len(args) > 0:\n",
    "            hidden_states = args[0]\n",
    "            del args\n",
    "        else:\n",
    "            first_key = list(kwargs.keys())[0]\n",
    "            hidden_states = kwargs.pop(first_key)\n",
    "\n",
    "        inps.append(hidden_states)\n",
    "        layer_kwargs.update(kwargs)\n",
    "        raise ValueError  # early exit to break later inference\n",
    "\n",
    "\n",
    "modules[0] = Catcher(modules[0])\n",
    "\n",
    "try:\n",
    "    model.generate(calib_set.to(next(model.parameters()).device))\n",
    "except ValueError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Catcher(\n",
       "    (module): Catcher(\n",
       "      (module): Catcher(\n",
       "        (module): Blip2EncoderLayer(\n",
       "          (self_attn): Blip2Attention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Blip2MLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1-38): 38 x Blip2EncoderLayer(\n",
       "    (self_attn): Blip2Attention(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "      (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "    )\n",
       "    (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Blip2MLP(\n",
       "      (activation_fn): GELUActivation()\n",
       "      (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "      (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "    )\n",
       "    (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.7319,  0.2039, -0.1177,  ...,  0.3483, -0.1315, -0.3481],\n",
       "          [-0.2441,  0.8111, -0.0983,  ...,  0.0234, -0.1451, -0.7400],\n",
       "          [-0.0418,  1.7882, -0.3203,  ..., -0.0219, -0.0488, -0.1417],\n",
       "          ...,\n",
       "          [-0.9605, -0.2865,  0.5448,  ..., -0.0762,  0.4271,  1.2226],\n",
       "          [-0.6000, -0.1437,  0.1221,  ..., -0.1582,  0.1567,  1.4119],\n",
       "          [-0.2725, -0.3828,  0.3872,  ...,  0.2089,  0.1482,  0.7765]],\n",
       " \n",
       "         [[ 0.7319,  0.2039, -0.1177,  ...,  0.3483, -0.1315, -0.3481],\n",
       "          [ 0.4066,  0.8089, -0.1578,  ...,  0.0691,  0.0124, -0.3917],\n",
       "          [ 0.2490,  1.6557, -0.3386,  ..., -0.1113,  0.0690, -0.0610],\n",
       "          ...,\n",
       "          [-1.0813, -0.3526,  0.3304,  ..., -0.0324,  0.3346,  0.8685],\n",
       "          [-0.5170, -0.1903, -0.0778,  ..., -0.1721,  0.1628,  1.2161],\n",
       "          [-0.0057, -0.5496,  0.3215,  ...,  0.1134, -0.0510,  0.9223]]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = Blip2ForImageTextRetrievalAWQQuantizer(model, processor, coco_dataset)\n",
    "# inputs = b.quantize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: exlude certain linear layers, reading from quant config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve for optimal (per input channel) scaling factor\n",
    "# TODO: grid search for \\alpha which balances protection of salient / non-salient weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
