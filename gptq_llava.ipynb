{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e561d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/nexus-scratch/vla/micromamba/envs/MMQ_LLAVA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "from dataset import VQAv2Eval\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2362434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab5e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# ====================================================\n",
    "# Quantization Classes and Functions\n",
    "# ====================================================\n",
    "\n",
    "def quantize(x, scale, zero, maxq):\n",
    "    if maxq < 0:\n",
    "        return (x > scale / 2).float() * scale + (x < zero / 2).float() * zero\n",
    "    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n",
    "    return scale * (q - zero)\n",
    "\n",
    "\n",
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, shape=1):\n",
    "        super(Quantizer, self).__init__()\n",
    "        self.register_buffer(\"maxq\", torch.tensor(0))\n",
    "        self.register_buffer(\"scale\", torch.zeros(shape))\n",
    "        self.register_buffer(\"zero\", torch.zeros(shape))\n",
    "\n",
    "    def configure(\n",
    "        self,\n",
    "        bits,\n",
    "        perchannel=False,\n",
    "        sym=True,\n",
    "        mse=False,\n",
    "        norm=2.4,\n",
    "        grid=100,\n",
    "        maxshrink=0.8,\n",
    "        trits=False,\n",
    "    ):\n",
    "        device = self.maxq.device\n",
    "        self.maxq = torch.tensor(2**bits - 1, device=device)\n",
    "        self.perchannel = perchannel\n",
    "        self.sym = sym\n",
    "        self.mse = mse\n",
    "        self.norm = norm\n",
    "        self.grid = grid\n",
    "        self.maxshrink = maxshrink\n",
    "        if trits:\n",
    "            self.maxq = torch.tensor(-1, device=device)\n",
    "\n",
    "    def find_params(self, x, weight=False):\n",
    "        dev = x.device\n",
    "        self.maxq = self.maxq.to(dev)\n",
    "\n",
    "        shape = x.shape\n",
    "        if self.perchannel:\n",
    "            if weight:\n",
    "                x = x.flatten(1)\n",
    "            else:\n",
    "                if len(shape) == 4:\n",
    "                    x = x.permute([1, 0, 2, 3])\n",
    "                    x = x.flatten(1)\n",
    "                if len(shape) == 3:\n",
    "                    x = x.reshape((-1, shape[-1])).t()\n",
    "                if len(shape) == 2:\n",
    "                    x = x.t()\n",
    "        else:\n",
    "            x = x.flatten().unsqueeze(0)\n",
    "\n",
    "        tmp = torch.zeros(x.shape[0], device=dev)\n",
    "        xmin = torch.minimum(x.min(1)[0], tmp)\n",
    "        xmax = torch.maximum(x.max(1)[0], tmp)\n",
    "\n",
    "        if self.sym:\n",
    "            xmax = torch.maximum(torch.abs(xmin), xmax)\n",
    "            tmp = xmin < 0\n",
    "            if torch.any(tmp):\n",
    "                xmin[tmp] = -xmax[tmp]\n",
    "        tmp = (xmin == 0) & (xmax == 0)\n",
    "        xmin[tmp] = -1\n",
    "        xmax[tmp] = +1\n",
    "\n",
    "        if self.maxq < 0:\n",
    "            self.scale = xmax\n",
    "            self.zero = xmin\n",
    "        else:\n",
    "            self.scale = (xmax - xmin) / self.maxq\n",
    "            if self.sym:\n",
    "                self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n",
    "            else:\n",
    "                self.zero = torch.round(-xmin / self.scale)\n",
    "\n",
    "        if self.mse:\n",
    "            best = torch.full([x.shape[0]], float(\"inf\"), device=dev)\n",
    "            for i in range(int(self.maxshrink * self.grid)):\n",
    "                p = 1 - i / self.grid\n",
    "                xmin1 = p * xmin\n",
    "                xmax1 = p * xmax\n",
    "                scale1 = (xmax1 - xmin1) / self.maxq\n",
    "                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n",
    "                q = quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n",
    "                q -= x\n",
    "                q.abs_()\n",
    "                q.pow_(self.norm)\n",
    "                err = torch.sum(q, 1)\n",
    "                tmp = err < best\n",
    "                if torch.any(tmp):\n",
    "                    best[tmp] = err[tmp]\n",
    "                    self.scale[tmp] = scale1[tmp]\n",
    "                    self.zero[tmp] = zero1[tmp]\n",
    "        if not self.perchannel:\n",
    "            if weight:\n",
    "                tmp = shape[0]\n",
    "            else:\n",
    "                tmp = shape[1] if len(shape) != 3 else shape[2]\n",
    "            self.scale = self.scale.repeat(tmp)\n",
    "            self.zero = self.zero.repeat(tmp)\n",
    "\n",
    "        if weight:\n",
    "            shape = [-1] + [1] * (len(shape) - 1)\n",
    "            self.scale = self.scale.reshape(shape)\n",
    "            self.zero = self.zero.reshape(shape)\n",
    "            return\n",
    "        if len(shape) == 4:\n",
    "            self.scale = self.scale.reshape((1, -1, 1, 1))\n",
    "            self.zero = self.zero.reshape((1, -1, 1, 1))\n",
    "        if len(shape) == 3:\n",
    "            self.scale = self.scale.reshape((1, 1, -1))\n",
    "            self.zero = self.zero.reshape((1, 1, -1))\n",
    "        if len(shape) == 2:\n",
    "            self.scale = self.scale.unsqueeze(0)\n",
    "            self.zero = self.zero.unsqueeze(0)\n",
    "\n",
    "        # Ensure buffers are on the same device as input x\n",
    "        self.scale = self.scale.to(dev)\n",
    "        self.zero = self.zero.to(dev)\n",
    "\n",
    "    def quantize(self, x):\n",
    "        if self.ready():\n",
    "            # Ensure buffers are on the same device as x\n",
    "            self.scale = self.scale.to(x.device)\n",
    "            self.zero = self.zero.to(x.device)\n",
    "            self.maxq = self.maxq.to(x.device)\n",
    "            return quantize(x, self.scale, self.zero, self.maxq)\n",
    "        return x\n",
    "\n",
    "    def enabled(self):\n",
    "        return self.maxq > 0\n",
    "\n",
    "    def ready(self):\n",
    "        return torch.all(self.scale != 0)\n",
    "\n",
    "\n",
    "class GPTQ:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "        self.quantizer = Quantizer()\n",
    "        self.quantizer.to(self.dev)\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(\n",
    "            self.layer, transformers.Conv1D\n",
    "        ):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            unfold = nn.Unfold(\n",
    "                self.layer.kernel_size,\n",
    "                dilation=self.layer.dilation,\n",
    "                padding=self.layer.padding,\n",
    "                stride=self.layer.stride,\n",
    "            )\n",
    "            inp = unfold(inp)\n",
    "            inp = inp.permute([1, 0, 2])\n",
    "            inp = inp.flatten(1)\n",
    "\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterquant(\n",
    "        self,\n",
    "        blocksize=128,\n",
    "        percdamp=0.01,\n",
    "        groupsize=-1,\n",
    "        actorder=False,\n",
    "        static_groups=False,\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        tick = time.time()\n",
    "\n",
    "        if not self.quantizer.ready():\n",
    "            self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        if static_groups:\n",
    "            import copy\n",
    "\n",
    "            groups = []\n",
    "            for i in range(0, self.columns, groupsize):\n",
    "                quantizer = copy.deepcopy(self.quantizer)\n",
    "                quantizer.find_params(W[:, i : (i + groupsize)], weight=True)\n",
    "                groups.append(quantizer)\n",
    "\n",
    "        if actorder:\n",
    "            perm = torch.argsort(torch.diag(H), descending=True)\n",
    "            W = W[:, perm]\n",
    "            H = H[perm][:, perm]\n",
    "            invperm = torch.argsort(perm)\n",
    "\n",
    "        Losses = torch.zeros_like(W)\n",
    "        Q = torch.zeros_like(W)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                if groupsize != -1:\n",
    "                    if not static_groups:\n",
    "                        if (i1 + i) % groupsize == 0:\n",
    "                            self.quantizer.find_params(\n",
    "                                W[:, (i1 + i) : (i1 + i + groupsize)], weight=True\n",
    "                            )\n",
    "                    else:\n",
    "                        idx = i1 + i\n",
    "                        if actorder:\n",
    "                            idx = perm[idx]\n",
    "                        self.quantizer = groups[idx // groupsize]\n",
    "\n",
    "                q = quantize(\n",
    "                    w.unsqueeze(1),\n",
    "                    self.quantizer.scale,\n",
    "                    self.quantizer.zero,\n",
    "                    self.quantizer.maxq,\n",
    "                ).flatten()\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d**2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            Q[:, i1:i2] = Q1\n",
    "            Losses[:, i1:i2] = Losses1 / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = Q[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                print(torch.sum(Losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Time for quantization: %.2f seconds\" % (time.time() - tick))\n",
    "        print(\"Total quantization error:\", torch.sum(Losses).item())\n",
    "\n",
    "        if actorder:\n",
    "            Q = Q[:, invperm]\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            Q = Q.t()\n",
    "        self.layer.weight.data = Q.reshape(self.layer.weight.shape).to(\n",
    "            self.layer.weight.data.dtype\n",
    "        )\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        self.Losses = None\n",
    "        self.Trace = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def find_linear_layers_in_model(model):\n",
    "    layers = {}\n",
    "\n",
    "    def recurse(module, prefix=\"\"):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            layers[prefix.rstrip(\".\")] = module\n",
    "        for name, child in module.named_children():\n",
    "            recurse(child, prefix + name + \".\")\n",
    "\n",
    "    recurse(model)\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fb58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaQuantizer:\n",
    "    def __init__(self, model, processor, device, chunk_size=32, task = 'vqav2'):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.chunk_size = chunk_size\n",
    "        self.task = task\n",
    "\n",
    "        # Component-specific configuration parameters\n",
    "        self.config = {\n",
    "            \"vision\": {\n",
    "                \"bits\": 4,\n",
    "                \"percent_dampening\": 0.01,\n",
    "                \"group_size\": -1,\n",
    "                \"use_symmetric\": True,\n",
    "                \"use_act_order\": False,\n",
    "                \"use_static_groups\": False,\n",
    "            },\n",
    "            \"language\": {\n",
    "                \"bits\": 4,\n",
    "                \"percent_dampening\": 0.01,\n",
    "                \"group_size\": -1,\n",
    "                \"use_symmetric\": True,\n",
    "                \"use_act_order\": False,\n",
    "                \"use_static_groups\": False,\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "    def _prepare_quantizers(self, layers, component_type):\n",
    "        \"\"\"Initialize GPTQ quantizers for given layers with component-specific settings\"\"\"\n",
    "        config = self.config[component_type]\n",
    "        quantizers = {}\n",
    "        for name, layer in layers.items():\n",
    "            quantizers[name] = GPTQ(layer)\n",
    "            quantizers[name].quantizer.configure(\n",
    "                bits=config[\"bits\"],\n",
    "                perchannel=True,\n",
    "                sym=config[\"use_symmetric\"],\n",
    "                mse=False,\n",
    "            )\n",
    "        return quantizers\n",
    "    \n",
    "    def _process_chunk(\n",
    "        self, layers, start_idx, end_idx, forward_func, desc, component_type\n",
    "    ):\n",
    "        \"\"\"Process a chunk of layers with component-specific quantization settings\"\"\"\n",
    "        current_layers = dict(list(layers.items())[start_idx:end_idx])\n",
    "        print(\n",
    "            f\"\\nProcessing {desc} layers {start_idx} to {end_idx-1} with {self.config[component_type]['bits']}-bit precision\"\n",
    "        )\n",
    "\n",
    "        # Initialize quantizers for current chunk\n",
    "        quantizers = self._prepare_quantizers(current_layers, component_type)\n",
    "        hooks = []\n",
    "\n",
    "        def get_hook(name):\n",
    "            def hook(module, inp, out):\n",
    "                if name in quantizers:\n",
    "                    quantizers[name].add_batch(inp[0].detach(), out.detach())\n",
    "\n",
    "            return hook\n",
    "\n",
    "        for name, layer in current_layers.items():\n",
    "            hooks.append(layer.register_forward_hook(get_hook(name)))\n",
    "\n",
    "        forward_func()\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        config = self.config[component_type]\n",
    "        for name, layer in current_layers.items():\n",
    "            print(f\"Quantizing layer {name}...\")\n",
    "            quantizer = quantizers[name]\n",
    "            quantizer.fasterquant(\n",
    "                blocksize=32,\n",
    "                percdamp=config[\"percent_dampening\"],\n",
    "                groupsize=config[\"group_size\"],\n",
    "                actorder=config[\"use_act_order\"],\n",
    "                static_groups=config[\"use_static_groups\"],\n",
    "            )\n",
    "\n",
    "            layer.weight.data = quantizer.quantizer.quantize(layer.weight.data).to(\n",
    "                layer.weight.data.dtype\n",
    "            )\n",
    "            quantizer.free()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    def quantize_vision_model(self, calibration_set):\n",
    "        \"\"\"Quantize vision model with 8-bit precision\"\"\"\n",
    "        print(\n",
    "            f\"Quantizing Vision Model with {self.config['vision']['bits']}-bit precision...\"\n",
    "        )\n",
    "\n",
    "        self.model.vision_tower.vision_model.to(self.device)\n",
    "\n",
    "        layers = find_linear_layers_in_model(self.model.vision_tower.vision_model)\n",
    "        total_layers = len(layers)\n",
    "\n",
    "        print(f'total_layers: {total_layers}')\n",
    "        print(layers)\n",
    "\n",
    "        def forward_pass():\n",
    "            \n",
    "            vision_feature_layer = self.model.config.vision_feature_layer\n",
    "            vision_feature_select_strategy = self.model.config.vision_feature_select_strategy\n",
    "            image_sizes = None\n",
    "            \n",
    "            # TODO: adjust for GQA if needed\n",
    "            if self.task == 'vqav2':\n",
    "                \n",
    "                for img, question in tqdm(calibration_set, desc='Processing vision model batch'):\n",
    "\n",
    "                    # short answer prompting according to: https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n",
    "                    prompt = 'USER: <image>\\n' + question + '\\nAnswer the question using a single word or phrase. ASSISTANT:'\n",
    "\n",
    "                    inputs = self.processor(images = [img],\n",
    "                                            text= [prompt],\n",
    "                                            return_tensors='pt',\n",
    "                                            padding=True).to(self.device)\n",
    "                    \n",
    "                    # runs forward pass through vision_tower\n",
    "                    self.model.get_image_features(\n",
    "                        pixel_values = inputs['pixel_values'],\n",
    "                        vision_feature_layer=vision_feature_layer,\n",
    "                        vision_feature_select_strategy=vision_feature_select_strategy,\n",
    "                        image_sizes=image_sizes\n",
    "                    )\n",
    "\n",
    "\n",
    "        for start_idx in range(0, total_layers, self.chunk_size):\n",
    "            end_idx = min(start_idx + self.chunk_size, total_layers)\n",
    "            self._process_chunk(\n",
    "                layers, start_idx, end_idx, forward_pass, \"vision model\", \"vision\"\n",
    "            )\n",
    "\n",
    "        self.model.vision_tower.vision_model.cpu()\n",
    "        print(\"Vision Model quantization complete.\\n\")\n",
    "\n",
    "\n",
    "    def quantize_language_model(self, calibration_set):\n",
    "        \"\"\"Quantize language model with 4-bit precision\"\"\"\n",
    "        print(\n",
    "            f\"Quantizing Language Model with {self.config['language']['bits']}-bit precision...\"\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        layers = find_linear_layers_in_model(self.model.language_model.model)\n",
    "        # layers[\"language_projection\"] = self.model.language_projection\n",
    "        total_layers = len(layers)\n",
    "\n",
    "        def forward_pass():\n",
    "            # TODO: adjust for GQA if needed\n",
    "            if self.task == 'vqav2':\n",
    "                \n",
    "                for img, question in tqdm(calibration_set, desc='Processing language model batch'):\n",
    "\n",
    "                    # short answer prompting according to: https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n",
    "                    prompt = 'USER: <image>\\n' + question + '\\nAnswer the question using a single word or phrase. ASSISTANT:'\n",
    "\n",
    "                    inputs = self.processor(images = [img],\n",
    "                                            text= [prompt],\n",
    "                                            return_tensors='pt',\n",
    "                                            padding=True).to(self.device)\n",
    "                    \n",
    "                    self.model.generate(**inputs)\n",
    "   \n",
    "\n",
    "        for start_idx in range(0, total_layers, self.chunk_size):\n",
    "            end_idx = min(start_idx + self.chunk_size, total_layers)\n",
    "            self._process_chunk(\n",
    "                layers, start_idx, end_idx, forward_pass, \"language model\", \"language\"\n",
    "            )\n",
    "\n",
    "        self.model.cpu()\n",
    "        print(\"Language Model quantization complete.\\n\")\n",
    "\n",
    "    def quantize(self, calibration_set):\n",
    "        \"\"\"Quantize all LLAVA components\"\"\"\n",
    "        print(\"Starting LLAVA model quantization...\")\n",
    "        self.quantize_vision_model(calibration_set)\n",
    "        self.quantize_language_model(calibration_set)\n",
    "        print(\"LLAVA model quantization complete.\")\n",
    "\n",
    "\n",
    "    # TODO:\n",
    "    def prepare_for_inference():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d8aef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.93s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "print(device)\n",
    "\n",
    "# Load the model\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", pad_token = '<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67f7977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32064, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.language_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16da1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQAv2 dataset paths\n",
    "ann_root = '/fs/cfar-projects/low-bit-vision/datasets/vqav2/annotations'\n",
    "q_root = '/fs/cfar-projects/low-bit-vision/datasets/vqav2/questions'\n",
    "image_root = '/fs/cfar-projects/low-bit-vision/datasets/vqav2/val2014'\n",
    "\n",
    "\n",
    "dataset = VQAv2Eval(image_root=image_root,\n",
    "                    ann_root=ann_root,\n",
    "                    q_root=q_root,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6505e84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 214354)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_indices = range(len(dataset))\n",
    "total_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c273e0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CALIBRATION_SIZE = 128\n",
    "calibration_indices = random.sample(total_indices, CALIBRATION_SIZE)\n",
    "\n",
    "calibration_set = [(dataset[i]['image'], dataset.qa_pairs[i]['question']) for i in calibration_indices]\n",
    "len(calibration_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3cf219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLAVA model quantization...\n",
      "Quantizing Vision Model with 4-bit precision...\n",
      "total_layers: 144\n",
      "{'encoder.layers.0.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.0.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.0.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.0.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.0.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.0.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.1.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.1.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.1.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.1.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.1.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.1.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.2.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.2.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.2.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.2.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.2.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.2.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.3.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.3.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.3.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.3.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.3.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.3.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.4.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.4.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.4.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.4.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.4.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.4.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.5.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.5.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.5.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.5.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.5.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.5.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.6.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.6.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.6.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.6.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.6.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.6.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.7.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.7.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.7.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.7.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.7.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.7.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.8.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.8.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.8.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.8.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.8.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.8.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.9.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.9.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.9.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.9.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.9.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.9.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.10.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.10.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.10.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.10.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.10.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.10.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.11.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.11.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.11.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.11.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.11.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.11.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.12.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.12.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.12.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.12.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.12.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.12.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.13.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.13.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.13.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.13.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.13.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.13.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.14.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.14.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.14.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.14.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.14.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.14.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.15.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.15.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.15.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.15.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.15.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.15.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.16.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.16.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.16.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.16.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.16.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.16.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.17.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.17.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.17.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.17.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.17.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.17.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.18.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.18.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.18.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.18.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.18.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.18.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.19.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.19.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.19.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.19.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.19.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.19.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.20.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.20.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.20.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.20.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.20.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.20.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.21.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.21.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.21.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.21.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.21.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.21.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.22.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.22.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.22.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.22.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.22.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.22.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True), 'encoder.layers.23.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.23.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.23.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.23.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True), 'encoder.layers.23.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True), 'encoder.layers.23.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True)}\n",
      "\n",
      "Processing vision model layers 0 to 31 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vision model batch: 100%|██████████| 128/128 [00:03<00:00, 36.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer encoder.layers.0.self_attn.k_proj...\n",
      "Time for quantization: 0.40 seconds\n",
      "Total quantization error: 324.318115234375\n",
      "Quantizing layer encoder.layers.0.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 203.19284057617188\n",
      "Quantizing layer encoder.layers.0.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 473.4849548339844\n",
      "Quantizing layer encoder.layers.0.self_attn.out_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 1.8828723430633545\n",
      "Quantizing layer encoder.layers.0.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 7512.52783203125\n",
      "Quantizing layer encoder.layers.0.mlp.fc2...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 41.70160675048828\n",
      "Quantizing layer encoder.layers.1.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 403.9676208496094\n",
      "Quantizing layer encoder.layers.1.self_attn.v_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 273.5756530761719\n",
      "Quantizing layer encoder.layers.1.self_attn.q_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 443.541015625\n",
      "Quantizing layer encoder.layers.1.self_attn.out_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 1.2011284828186035\n",
      "Quantizing layer encoder.layers.1.mlp.fc1...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 4007.837890625\n",
      "Quantizing layer encoder.layers.1.mlp.fc2...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 39.82926940917969\n",
      "Quantizing layer encoder.layers.2.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 785.1456298828125\n",
      "Quantizing layer encoder.layers.2.self_attn.v_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 465.1142578125\n",
      "Quantizing layer encoder.layers.2.self_attn.q_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 760.13916015625\n",
      "Quantizing layer encoder.layers.2.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2.163909912109375\n",
      "Quantizing layer encoder.layers.2.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 7004.3359375\n",
      "Quantizing layer encoder.layers.2.mlp.fc2...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 56.084259033203125\n",
      "Quantizing layer encoder.layers.3.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2172.64794921875\n",
      "Quantizing layer encoder.layers.3.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 882.75732421875\n",
      "Quantizing layer encoder.layers.3.self_attn.q_proj...\n",
      "Time for quantization: 0.33 seconds\n",
      "Total quantization error: 1937.080078125\n",
      "Quantizing layer encoder.layers.3.self_attn.out_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 25.27263641357422\n",
      "Quantizing layer encoder.layers.3.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 5130.83203125\n",
      "Quantizing layer encoder.layers.3.mlp.fc2...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 59.686561584472656\n",
      "Quantizing layer encoder.layers.4.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 1404.9134521484375\n",
      "Quantizing layer encoder.layers.4.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 564.059326171875\n",
      "Quantizing layer encoder.layers.4.self_attn.q_proj...\n",
      "Time for quantization: 0.37 seconds\n",
      "Total quantization error: 1147.145751953125\n",
      "Quantizing layer encoder.layers.4.self_attn.out_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 8.486044883728027\n",
      "Quantizing layer encoder.layers.4.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 5041.52099609375\n",
      "Quantizing layer encoder.layers.4.mlp.fc2...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 71.69609069824219\n",
      "Quantizing layer encoder.layers.5.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 1563.251220703125\n",
      "Quantizing layer encoder.layers.5.self_attn.v_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 661.0287475585938\n",
      "\n",
      "Processing vision model layers 32 to 63 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vision model batch: 100%|██████████| 128/128 [00:03<00:00, 40.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer encoder.layers.5.self_attn.q_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 1266.112548828125\n",
      "Quantizing layer encoder.layers.5.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 11.363433837890625\n",
      "Quantizing layer encoder.layers.5.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 5879.78173828125\n",
      "Quantizing layer encoder.layers.5.mlp.fc2...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 84.09638977050781\n",
      "Quantizing layer encoder.layers.6.self_attn.k_proj...\n",
      "Time for quantization: 0.31 seconds\n",
      "Total quantization error: 3823.885498046875\n",
      "Quantizing layer encoder.layers.6.self_attn.v_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 1262.759033203125\n",
      "Quantizing layer encoder.layers.6.self_attn.q_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 2733.95849609375\n",
      "Quantizing layer encoder.layers.6.self_attn.out_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 57.97265625\n",
      "Quantizing layer encoder.layers.6.mlp.fc1...\n",
      "Time for quantization: 0.31 seconds\n",
      "Total quantization error: 5196.517578125\n",
      "Quantizing layer encoder.layers.6.mlp.fc2...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 90.50564575195312\n",
      "Quantizing layer encoder.layers.7.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 2641.937744140625\n",
      "Quantizing layer encoder.layers.7.self_attn.v_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 1279.159423828125\n",
      "Quantizing layer encoder.layers.7.self_attn.q_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 2081.37060546875\n",
      "Quantizing layer encoder.layers.7.self_attn.out_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 29.16206932067871\n",
      "Quantizing layer encoder.layers.7.mlp.fc1...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 6542.8466796875\n",
      "Quantizing layer encoder.layers.7.mlp.fc2...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 118.97940063476562\n",
      "Quantizing layer encoder.layers.8.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 2942.11181640625\n",
      "Quantizing layer encoder.layers.8.self_attn.v_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 1492.8773193359375\n",
      "Quantizing layer encoder.layers.8.self_attn.q_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 2398.79638671875\n",
      "Quantizing layer encoder.layers.8.self_attn.out_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 40.72288131713867\n",
      "Quantizing layer encoder.layers.8.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 7648.2255859375\n",
      "Quantizing layer encoder.layers.8.mlp.fc2...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 154.70843505859375\n",
      "Quantizing layer encoder.layers.9.self_attn.k_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 2979.09130859375\n",
      "Quantizing layer encoder.layers.9.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 1532.30419921875\n",
      "Quantizing layer encoder.layers.9.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2366.09521484375\n",
      "Quantizing layer encoder.layers.9.self_attn.out_proj...\n",
      "Time for quantization: 0.31 seconds\n",
      "Total quantization error: 41.476226806640625\n",
      "Quantizing layer encoder.layers.9.mlp.fc1...\n",
      "Time for quantization: 0.32 seconds\n",
      "Total quantization error: 7875.6708984375\n",
      "Quantizing layer encoder.layers.9.mlp.fc2...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 153.2353515625\n",
      "Quantizing layer encoder.layers.10.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2911.346923828125\n",
      "Quantizing layer encoder.layers.10.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 1539.650390625\n",
      "Quantizing layer encoder.layers.10.self_attn.q_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 2467.37548828125\n",
      "Quantizing layer encoder.layers.10.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 41.60853958129883\n",
      "\n",
      "Processing vision model layers 64 to 95 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vision model batch: 100%|██████████| 128/128 [00:03<00:00, 39.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer encoder.layers.10.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 9273.6240234375\n",
      "Quantizing layer encoder.layers.10.mlp.fc2...\n",
      "Time for quantization: 1.10 seconds\n",
      "Total quantization error: 144.94256591796875\n",
      "Quantizing layer encoder.layers.11.self_attn.k_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 3375.654296875\n",
      "Quantizing layer encoder.layers.11.self_attn.v_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 1678.41748046875\n",
      "Quantizing layer encoder.layers.11.self_attn.q_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 2912.82958984375\n",
      "Quantizing layer encoder.layers.11.self_attn.out_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 35.04420852661133\n",
      "Quantizing layer encoder.layers.11.mlp.fc1...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 9718.44921875\n",
      "Quantizing layer encoder.layers.11.mlp.fc2...\n",
      "Time for quantization: 1.10 seconds\n",
      "Total quantization error: 153.88845825195312\n",
      "Quantizing layer encoder.layers.12.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3092.06640625\n",
      "Quantizing layer encoder.layers.12.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 1545.407958984375\n",
      "Quantizing layer encoder.layers.12.self_attn.q_proj...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 2771.04541015625\n",
      "Quantizing layer encoder.layers.12.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 27.4853515625\n",
      "Quantizing layer encoder.layers.12.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 9710.736328125\n",
      "Quantizing layer encoder.layers.12.mlp.fc2...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 225.07681274414062\n",
      "Quantizing layer encoder.layers.13.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3314.078369140625\n",
      "Quantizing layer encoder.layers.13.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 1753.4466552734375\n",
      "Quantizing layer encoder.layers.13.self_attn.q_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 2991.690185546875\n",
      "Quantizing layer encoder.layers.13.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 34.473182678222656\n",
      "Quantizing layer encoder.layers.13.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 9076.259765625\n",
      "Quantizing layer encoder.layers.13.mlp.fc2...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 124.0893783569336\n",
      "Quantizing layer encoder.layers.14.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 2950.796875\n",
      "Quantizing layer encoder.layers.14.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 1548.001953125\n",
      "Quantizing layer encoder.layers.14.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2706.087646484375\n",
      "Quantizing layer encoder.layers.14.self_attn.out_proj...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 30.812793731689453\n",
      "Quantizing layer encoder.layers.14.mlp.fc1...\n",
      "Time for quantization: 0.31 seconds\n",
      "Total quantization error: 10352.169921875\n",
      "Quantizing layer encoder.layers.14.mlp.fc2...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 123.21295166015625\n",
      "Quantizing layer encoder.layers.15.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2792.576171875\n",
      "Quantizing layer encoder.layers.15.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 1608.333251953125\n",
      "Quantizing layer encoder.layers.15.self_attn.q_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 2649.042724609375\n",
      "Quantizing layer encoder.layers.15.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 23.668777465820312\n",
      "Quantizing layer encoder.layers.15.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 12966.06640625\n",
      "Quantizing layer encoder.layers.15.mlp.fc2...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 128.6141815185547\n",
      "\n",
      "Processing vision model layers 96 to 127 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vision model batch: 100%|██████████| 128/128 [00:03<00:00, 41.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer encoder.layers.16.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 3392.469482421875\n",
      "Quantizing layer encoder.layers.16.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2051.998779296875\n",
      "Quantizing layer encoder.layers.16.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3167.6328125\n",
      "Quantizing layer encoder.layers.16.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 29.2899227142334\n",
      "Quantizing layer encoder.layers.16.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 17877.18359375\n",
      "Quantizing layer encoder.layers.16.mlp.fc2...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 147.67857360839844\n",
      "Quantizing layer encoder.layers.17.self_attn.k_proj...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 3366.25927734375\n",
      "Quantizing layer encoder.layers.17.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2060.041015625\n",
      "Quantizing layer encoder.layers.17.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3198.875\n",
      "Quantizing layer encoder.layers.17.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 28.46754264831543\n",
      "Quantizing layer encoder.layers.17.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 25426.62890625\n",
      "Quantizing layer encoder.layers.17.mlp.fc2...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 167.7866973876953\n",
      "Quantizing layer encoder.layers.18.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3740.113525390625\n",
      "Quantizing layer encoder.layers.18.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2480.950439453125\n",
      "Quantizing layer encoder.layers.18.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3765.423828125\n",
      "Quantizing layer encoder.layers.18.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 33.463905334472656\n",
      "Quantizing layer encoder.layers.18.mlp.fc1...\n",
      "Time for quantization: 0.30 seconds\n",
      "Total quantization error: 31236.486328125\n",
      "Quantizing layer encoder.layers.18.mlp.fc2...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 216.33746337890625\n",
      "Quantizing layer encoder.layers.19.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 4026.91162109375\n",
      "Quantizing layer encoder.layers.19.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2618.5693359375\n",
      "Quantizing layer encoder.layers.19.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3795.072998046875\n",
      "Quantizing layer encoder.layers.19.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 40.903045654296875\n",
      "Quantizing layer encoder.layers.19.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 34720.4453125\n",
      "Quantizing layer encoder.layers.19.mlp.fc2...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 273.92120361328125\n",
      "Quantizing layer encoder.layers.20.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3803.240234375\n",
      "Quantizing layer encoder.layers.20.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2860.08642578125\n",
      "Quantizing layer encoder.layers.20.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3762.940673828125\n",
      "Quantizing layer encoder.layers.20.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 54.255592346191406\n",
      "Quantizing layer encoder.layers.20.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 46455.09765625\n",
      "Quantizing layer encoder.layers.20.mlp.fc2...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 468.7069396972656\n",
      "Quantizing layer encoder.layers.21.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2846.377685546875\n",
      "Quantizing layer encoder.layers.21.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2494.29833984375\n",
      "\n",
      "Processing vision model layers 128 to 143 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vision model batch: 100%|██████████| 128/128 [00:02<00:00, 45.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer encoder.layers.21.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3506.24658203125\n",
      "Quantizing layer encoder.layers.21.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 62.4183349609375\n",
      "Quantizing layer encoder.layers.21.mlp.fc1...\n",
      "Time for quantization: 0.27 seconds\n",
      "Total quantization error: 40719.02734375\n",
      "Quantizing layer encoder.layers.21.mlp.fc2...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 630.75927734375\n",
      "Quantizing layer encoder.layers.22.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2750.724609375\n",
      "Quantizing layer encoder.layers.22.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2910.30810546875\n",
      "Quantizing layer encoder.layers.22.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 28895.27734375\n",
      "Quantizing layer encoder.layers.22.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 85.51264190673828\n",
      "Quantizing layer encoder.layers.22.mlp.fc1...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 24796.71484375\n",
      "Quantizing layer encoder.layers.22.mlp.fc2...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 384.3135986328125\n",
      "Quantizing layer encoder.layers.23.self_attn.k_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2192.246826171875\n",
      "Quantizing layer encoder.layers.23.self_attn.v_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 3246.730712890625\n",
      "Quantizing layer encoder.layers.23.self_attn.q_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 2059.803955078125\n",
      "Quantizing layer encoder.layers.23.self_attn.out_proj...\n",
      "Time for quantization: 0.28 seconds\n",
      "Total quantization error: 46.16343688964844\n",
      "Quantizing layer encoder.layers.23.mlp.fc1...\n",
      "Time for quantization: 0.29 seconds\n",
      "Total quantization error: 10667.8330078125\n",
      "Quantizing layer encoder.layers.23.mlp.fc2...\n",
      "Time for quantization: 1.09 seconds\n",
      "Total quantization error: 148.5315704345703\n",
      "Vision Model quantization complete.\n",
      "\n",
      "Quantizing Language Model with 4-bit precision...\n",
      "\n",
      "Processing language model layers 0 to 31 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:43<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.0.self_attn.q_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 39.27503967285156\n",
      "Quantizing layer layers.0.self_attn.k_proj...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 37.31718444824219\n",
      "Quantizing layer layers.0.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 2.3259124755859375\n",
      "Quantizing layer layers.0.self_attn.o_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 0.0518023781478405\n",
      "Quantizing layer layers.0.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 12.721443176269531\n",
      "Quantizing layer layers.0.mlp.up_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 11.534239768981934\n",
      "Quantizing layer layers.0.mlp.down_proj...\n",
      "Time for quantization: 3.39 seconds\n",
      "Total quantization error: 0.08381568640470505\n",
      "Quantizing layer layers.1.self_attn.q_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 83.57455444335938\n",
      "Quantizing layer layers.1.self_attn.k_proj...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 90.35342407226562\n",
      "Quantizing layer layers.1.self_attn.v_proj...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 6.499544143676758\n",
      "Quantizing layer layers.1.self_attn.o_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 0.22300592064857483\n",
      "Quantizing layer layers.1.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 47.31085968017578\n",
      "Quantizing layer layers.1.mlp.up_proj...\n",
      "Time for quantization: 1.33 seconds\n",
      "Total quantization error: 40.11989212036133\n",
      "Quantizing layer layers.1.mlp.down_proj...\n",
      "Time for quantization: 3.52 seconds\n",
      "Total quantization error: 1682.39306640625\n",
      "Quantizing layer layers.2.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 279.03814697265625\n",
      "Quantizing layer layers.2.self_attn.k_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 201.93568420410156\n",
      "Quantizing layer layers.2.self_attn.v_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 34.045936584472656\n",
      "Quantizing layer layers.2.self_attn.o_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 1.287614345550537\n",
      "Quantizing layer layers.2.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 90.2472152709961\n",
      "Quantizing layer layers.2.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 76.46188354492188\n",
      "Quantizing layer layers.2.mlp.down_proj...\n",
      "Time for quantization: 3.48 seconds\n",
      "Total quantization error: 2.280508518218994\n",
      "Quantizing layer layers.3.self_attn.q_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 731.429443359375\n",
      "Quantizing layer layers.3.self_attn.k_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 399.96929931640625\n",
      "Quantizing layer layers.3.self_attn.v_proj...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 83.40946197509766\n",
      "Quantizing layer layers.3.self_attn.o_proj...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 0.9800522923469543\n",
      "Quantizing layer layers.3.mlp.gate_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 154.70266723632812\n",
      "Quantizing layer layers.3.mlp.up_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 128.890380859375\n",
      "Quantizing layer layers.3.mlp.down_proj...\n",
      "Time for quantization: 3.39 seconds\n",
      "Total quantization error: 5.989475727081299\n",
      "Quantizing layer layers.4.self_attn.q_proj...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 682.8641357421875\n",
      "Quantizing layer layers.4.self_attn.k_proj...\n",
      "Time for quantization: 1.10 seconds\n",
      "Total quantization error: 398.59490966796875\n",
      "Quantizing layer layers.4.self_attn.v_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 85.02912139892578\n",
      "Quantizing layer layers.4.self_attn.o_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 1.6879351139068604\n",
      "\n",
      "Processing language model layers 32 to 63 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:45<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.4.mlp.gate_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 219.23712158203125\n",
      "Quantizing layer layers.4.mlp.up_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 173.56700134277344\n",
      "Quantizing layer layers.4.mlp.down_proj...\n",
      "Time for quantization: 3.38 seconds\n",
      "Total quantization error: 11.117080688476562\n",
      "Quantizing layer layers.5.self_attn.q_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 566.7357177734375\n",
      "Quantizing layer layers.5.self_attn.k_proj...\n",
      "Time for quantization: 1.10 seconds\n",
      "Total quantization error: 467.8176574707031\n",
      "Quantizing layer layers.5.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 106.7626724243164\n",
      "Quantizing layer layers.5.self_attn.o_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 3.8878931999206543\n",
      "Quantizing layer layers.5.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 270.57965087890625\n",
      "Quantizing layer layers.5.mlp.up_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 214.0594940185547\n",
      "Quantizing layer layers.5.mlp.down_proj...\n",
      "Time for quantization: 3.39 seconds\n",
      "Total quantization error: 14.104846000671387\n",
      "Quantizing layer layers.6.self_attn.q_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 862.3492431640625\n",
      "Quantizing layer layers.6.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 624.26123046875\n",
      "Quantizing layer layers.6.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 163.00320434570312\n",
      "Quantizing layer layers.6.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 6.497465133666992\n",
      "Quantizing layer layers.6.mlp.gate_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 352.20855712890625\n",
      "Quantizing layer layers.6.mlp.up_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 267.46441650390625\n",
      "Quantizing layer layers.6.mlp.down_proj...\n",
      "Time for quantization: 3.39 seconds\n",
      "Total quantization error: 19.312236785888672\n",
      "Quantizing layer layers.7.self_attn.q_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 873.6777954101562\n",
      "Quantizing layer layers.7.self_attn.k_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 695.3619384765625\n",
      "Quantizing layer layers.7.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 202.1181640625\n",
      "Quantizing layer layers.7.self_attn.o_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 10.29556941986084\n",
      "Quantizing layer layers.7.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 442.25701904296875\n",
      "Quantizing layer layers.7.mlp.up_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 338.59197998046875\n",
      "Quantizing layer layers.7.mlp.down_proj...\n",
      "Time for quantization: 3.46 seconds\n",
      "Total quantization error: 28.065631866455078\n",
      "Quantizing layer layers.8.self_attn.q_proj...\n",
      "Time for quantization: 1.24 seconds\n",
      "Total quantization error: 864.0894775390625\n",
      "Quantizing layer layers.8.self_attn.k_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 762.4486083984375\n",
      "Quantizing layer layers.8.self_attn.v_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 217.03494262695312\n",
      "Quantizing layer layers.8.self_attn.o_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 17.825206756591797\n",
      "Quantizing layer layers.8.mlp.gate_proj...\n",
      "Time for quantization: 1.24 seconds\n",
      "Total quantization error: 484.8800048828125\n",
      "Quantizing layer layers.8.mlp.up_proj...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 394.82135009765625\n",
      "Quantizing layer layers.8.mlp.down_proj...\n",
      "Time for quantization: 3.44 seconds\n",
      "Total quantization error: 34.39647674560547\n",
      "Quantizing layer layers.9.self_attn.q_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 918.4611206054688\n",
      "\n",
      "Processing language model layers 64 to 95 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:43<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.9.self_attn.k_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 882.43017578125\n",
      "Quantizing layer layers.9.self_attn.v_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 256.9330139160156\n",
      "Quantizing layer layers.9.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 27.053913116455078\n",
      "Quantizing layer layers.9.mlp.gate_proj...\n",
      "Time for quantization: 1.24 seconds\n",
      "Total quantization error: 547.028076171875\n",
      "Quantizing layer layers.9.mlp.up_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 459.4306945800781\n",
      "Quantizing layer layers.9.mlp.down_proj...\n",
      "Time for quantization: 3.49 seconds\n",
      "Total quantization error: 39.58306121826172\n",
      "Quantizing layer layers.10.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 935.745361328125\n",
      "Quantizing layer layers.10.self_attn.k_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 967.4424438476562\n",
      "Quantizing layer layers.10.self_attn.v_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 293.16943359375\n",
      "Quantizing layer layers.10.self_attn.o_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 35.35161209106445\n",
      "Quantizing layer layers.10.mlp.gate_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 586.7703247070312\n",
      "Quantizing layer layers.10.mlp.up_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 502.5540466308594\n",
      "Quantizing layer layers.10.mlp.down_proj...\n",
      "Time for quantization: 3.41 seconds\n",
      "Total quantization error: 48.902976989746094\n",
      "Quantizing layer layers.11.self_attn.q_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1113.711181640625\n",
      "Quantizing layer layers.11.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 1026.23388671875\n",
      "Quantizing layer layers.11.self_attn.v_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 411.8172607421875\n",
      "Quantizing layer layers.11.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 44.452842712402344\n",
      "Quantizing layer layers.11.mlp.gate_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 661.5831298828125\n",
      "Quantizing layer layers.11.mlp.up_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 582.66455078125\n",
      "Quantizing layer layers.11.mlp.down_proj...\n",
      "Time for quantization: 3.39 seconds\n",
      "Total quantization error: 52.211570739746094\n",
      "Quantizing layer layers.12.self_attn.q_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 1213.277587890625\n",
      "Quantizing layer layers.12.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 1217.703125\n",
      "Quantizing layer layers.12.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 428.9674072265625\n",
      "Quantizing layer layers.12.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 32.58979797363281\n",
      "Quantizing layer layers.12.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 709.1033935546875\n",
      "Quantizing layer layers.12.mlp.up_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 638.603271484375\n",
      "Quantizing layer layers.12.mlp.down_proj...\n",
      "Time for quantization: 3.44 seconds\n",
      "Total quantization error: 57.698455810546875\n",
      "Quantizing layer layers.13.self_attn.q_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 1366.390869140625\n",
      "Quantizing layer layers.13.self_attn.k_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1230.60009765625\n",
      "Quantizing layer layers.13.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 457.20953369140625\n",
      "Quantizing layer layers.13.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 28.724618911743164\n",
      "Quantizing layer layers.13.mlp.gate_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 745.819580078125\n",
      "\n",
      "Processing language model layers 96 to 127 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:45<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.13.mlp.up_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 718.2681274414062\n",
      "Quantizing layer layers.13.mlp.down_proj...\n",
      "Time for quantization: 3.49 seconds\n",
      "Total quantization error: 72.29673767089844\n",
      "Quantizing layer layers.14.self_attn.q_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1312.2158203125\n",
      "Quantizing layer layers.14.self_attn.k_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1357.573974609375\n",
      "Quantizing layer layers.14.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 473.7171630859375\n",
      "Quantizing layer layers.14.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 39.82175827026367\n",
      "Quantizing layer layers.14.mlp.gate_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 880.6115112304688\n",
      "Quantizing layer layers.14.mlp.up_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 816.5322875976562\n",
      "Quantizing layer layers.14.mlp.down_proj...\n",
      "Time for quantization: 3.42 seconds\n",
      "Total quantization error: 79.90638732910156\n",
      "Quantizing layer layers.15.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 1529.061767578125\n",
      "Quantizing layer layers.15.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 1403.230224609375\n",
      "Quantizing layer layers.15.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 527.9586181640625\n",
      "Quantizing layer layers.15.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 32.71067428588867\n",
      "Quantizing layer layers.15.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 992.5611572265625\n",
      "Quantizing layer layers.15.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 933.0162353515625\n",
      "Quantizing layer layers.15.mlp.down_proj...\n",
      "Time for quantization: 3.50 seconds\n",
      "Total quantization error: 109.33663940429688\n",
      "Quantizing layer layers.16.self_attn.q_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1751.212158203125\n",
      "Quantizing layer layers.16.self_attn.k_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1558.8626708984375\n",
      "Quantizing layer layers.16.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 613.2882080078125\n",
      "Quantizing layer layers.16.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 24.008485794067383\n",
      "Quantizing layer layers.16.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 1151.134033203125\n",
      "Quantizing layer layers.16.mlp.up_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 1059.626220703125\n",
      "Quantizing layer layers.16.mlp.down_proj...\n",
      "Time for quantization: 3.47 seconds\n",
      "Total quantization error: 150.60971069335938\n",
      "Quantizing layer layers.17.self_attn.q_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1803.002685546875\n",
      "Quantizing layer layers.17.self_attn.k_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 1585.280029296875\n",
      "Quantizing layer layers.17.self_attn.v_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 655.2760620117188\n",
      "Quantizing layer layers.17.self_attn.o_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 29.21463394165039\n",
      "Quantizing layer layers.17.mlp.gate_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 1339.1494140625\n",
      "Quantizing layer layers.17.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 1201.738037109375\n",
      "Quantizing layer layers.17.mlp.down_proj...\n",
      "Time for quantization: 3.42 seconds\n",
      "Total quantization error: 156.0867156982422\n",
      "Quantizing layer layers.18.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 2162.104248046875\n",
      "Quantizing layer layers.18.self_attn.k_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 1766.4254150390625\n",
      "\n",
      "Processing language model layers 128 to 159 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:43<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.18.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 852.6350708007812\n",
      "Quantizing layer layers.18.self_attn.o_proj...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 21.600988388061523\n",
      "Quantizing layer layers.18.mlp.gate_proj...\n",
      "Time for quantization: 1.29 seconds\n",
      "Total quantization error: 1626.6728515625\n",
      "Quantizing layer layers.18.mlp.up_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 1427.949951171875\n",
      "Quantizing layer layers.18.mlp.down_proj...\n",
      "Time for quantization: 3.45 seconds\n",
      "Total quantization error: 203.8971405029297\n",
      "Quantizing layer layers.19.self_attn.q_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 2392.518798828125\n",
      "Quantizing layer layers.19.self_attn.k_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 1923.9208984375\n",
      "Quantizing layer layers.19.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 860.2222900390625\n",
      "Quantizing layer layers.19.self_attn.o_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 35.00975036621094\n",
      "Quantizing layer layers.19.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 1785.79150390625\n",
      "Quantizing layer layers.19.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 1562.02197265625\n",
      "Quantizing layer layers.19.mlp.down_proj...\n",
      "Time for quantization: 3.38 seconds\n",
      "Total quantization error: 226.14642333984375\n",
      "Quantizing layer layers.20.self_attn.q_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 2455.70458984375\n",
      "Quantizing layer layers.20.self_attn.k_proj...\n",
      "Time for quantization: 1.11 seconds\n",
      "Total quantization error: 2046.102294921875\n",
      "Quantizing layer layers.20.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 906.541259765625\n",
      "Quantizing layer layers.20.self_attn.o_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 28.63027000427246\n",
      "Quantizing layer layers.20.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 1923.3128662109375\n",
      "Quantizing layer layers.20.mlp.up_proj...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 1668.688720703125\n",
      "Quantizing layer layers.20.mlp.down_proj...\n",
      "Time for quantization: 3.47 seconds\n",
      "Total quantization error: 269.7759094238281\n",
      "Quantizing layer layers.21.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 3043.78515625\n",
      "Quantizing layer layers.21.self_attn.k_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 2286.7021484375\n",
      "Quantizing layer layers.21.self_attn.v_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 1092.29638671875\n",
      "Quantizing layer layers.21.self_attn.o_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 18.41374969482422\n",
      "Quantizing layer layers.21.mlp.gate_proj...\n",
      "Time for quantization: 1.21 seconds\n",
      "Total quantization error: 2140.42822265625\n",
      "Quantizing layer layers.21.mlp.up_proj...\n",
      "Time for quantization: 1.23 seconds\n",
      "Total quantization error: 1830.6436767578125\n",
      "Quantizing layer layers.21.mlp.down_proj...\n",
      "Time for quantization: 3.52 seconds\n",
      "Total quantization error: 279.8625183105469\n",
      "Quantizing layer layers.22.self_attn.q_proj...\n",
      "Time for quantization: 1.23 seconds\n",
      "Total quantization error: 3086.064208984375\n",
      "Quantizing layer layers.22.self_attn.k_proj...\n",
      "Time for quantization: 2.40 seconds\n",
      "Total quantization error: 2522.217529296875\n",
      "Quantizing layer layers.22.self_attn.v_proj...\n",
      "Time for quantization: 1.27 seconds\n",
      "Total quantization error: 1180.1776123046875\n",
      "Quantizing layer layers.22.self_attn.o_proj...\n",
      "Time for quantization: 1.23 seconds\n",
      "Total quantization error: 38.30768585205078\n",
      "Quantizing layer layers.22.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 2327.7548828125\n",
      "Quantizing layer layers.22.mlp.up_proj...\n",
      "Time for quantization: 1.26 seconds\n",
      "Total quantization error: 1962.0740966796875\n",
      "\n",
      "Processing language model layers 160 to 191 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:45<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.22.mlp.down_proj...\n",
      "Time for quantization: 3.67 seconds\n",
      "Total quantization error: 324.6732177734375\n",
      "Quantizing layer layers.23.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 3664.8134765625\n",
      "Quantizing layer layers.23.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 2839.9580078125\n",
      "Quantizing layer layers.23.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1512.9879150390625\n",
      "Quantizing layer layers.23.self_attn.o_proj...\n",
      "Time for quantization: 1.23 seconds\n",
      "Total quantization error: 33.73889923095703\n",
      "Quantizing layer layers.23.mlp.gate_proj...\n",
      "Time for quantization: 1.26 seconds\n",
      "Total quantization error: 2687.8544921875\n",
      "Quantizing layer layers.23.mlp.up_proj...\n",
      "Time for quantization: 2.40 seconds\n",
      "Total quantization error: 2303.2822265625\n",
      "Quantizing layer layers.23.mlp.down_proj...\n",
      "Time for quantization: 3.49 seconds\n",
      "Total quantization error: 388.1644592285156\n",
      "Quantizing layer layers.24.self_attn.q_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 3440.502685546875\n",
      "Quantizing layer layers.24.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 2901.029296875\n",
      "Quantizing layer layers.24.self_attn.v_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 1518.9473876953125\n",
      "Quantizing layer layers.24.self_attn.o_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 41.27019119262695\n",
      "Quantizing layer layers.24.mlp.gate_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 2876.116943359375\n",
      "Quantizing layer layers.24.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 2447.3330078125\n",
      "Quantizing layer layers.24.mlp.down_proj...\n",
      "Time for quantization: 3.43 seconds\n",
      "Total quantization error: 409.40252685546875\n",
      "Quantizing layer layers.25.self_attn.q_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 4065.453125\n",
      "Quantizing layer layers.25.self_attn.k_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 3216.205322265625\n",
      "Quantizing layer layers.25.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 1887.1705322265625\n",
      "Quantizing layer layers.25.self_attn.o_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 19.007732391357422\n",
      "Quantizing layer layers.25.mlp.gate_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 3196.3984375\n",
      "Quantizing layer layers.25.mlp.up_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 2721.084716796875\n",
      "Quantizing layer layers.25.mlp.down_proj...\n",
      "Time for quantization: 4.19 seconds\n",
      "Total quantization error: 471.04632568359375\n",
      "Quantizing layer layers.26.self_attn.q_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 4511.01708984375\n",
      "Quantizing layer layers.26.self_attn.k_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 3316.626953125\n",
      "Quantizing layer layers.26.self_attn.v_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 1917.3997802734375\n",
      "Quantizing layer layers.26.self_attn.o_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 99.317138671875\n",
      "Quantizing layer layers.26.mlp.gate_proj...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 3317.18505859375\n",
      "Quantizing layer layers.26.mlp.up_proj...\n",
      "Time for quantization: 1.21 seconds\n",
      "Total quantization error: 2853.381103515625\n",
      "Quantizing layer layers.26.mlp.down_proj...\n",
      "Time for quantization: 3.43 seconds\n",
      "Total quantization error: 451.3273620605469\n",
      "Quantizing layer layers.27.self_attn.q_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 5125.623046875\n",
      "Quantizing layer layers.27.self_attn.k_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 3306.683349609375\n",
      "Quantizing layer layers.27.self_attn.v_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 1959.866943359375\n",
      "\n",
      "Processing language model layers 192 to 223 with 4-bit precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing language model batch: 100%|██████████| 128/128 [00:45<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing layer layers.27.self_attn.o_proj...\n",
      "Time for quantization: 1.10 seconds\n",
      "Total quantization error: 56.34391403198242\n",
      "Quantizing layer layers.27.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 3818.139404296875\n",
      "Quantizing layer layers.27.mlp.up_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 3325.672607421875\n",
      "Quantizing layer layers.27.mlp.down_proj...\n",
      "Time for quantization: 3.42 seconds\n",
      "Total quantization error: 557.786376953125\n",
      "Quantizing layer layers.28.self_attn.q_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 6318.03759765625\n",
      "Quantizing layer layers.28.self_attn.k_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 3663.02001953125\n",
      "Quantizing layer layers.28.self_attn.v_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 2329.18505859375\n",
      "Quantizing layer layers.28.self_attn.o_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 77.68634796142578\n",
      "Quantizing layer layers.28.mlp.gate_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 3965.61328125\n",
      "Quantizing layer layers.28.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 3549.2021484375\n",
      "Quantizing layer layers.28.mlp.down_proj...\n",
      "Time for quantization: 3.49 seconds\n",
      "Total quantization error: 634.3869018554688\n",
      "Quantizing layer layers.29.self_attn.q_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 6037.0869140625\n",
      "Quantizing layer layers.29.self_attn.k_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 3521.132568359375\n",
      "Quantizing layer layers.29.self_attn.v_proj...\n",
      "Time for quantization: 1.13 seconds\n",
      "Total quantization error: 2173.603515625\n",
      "Quantizing layer layers.29.self_attn.o_proj...\n",
      "Time for quantization: 1.12 seconds\n",
      "Total quantization error: 90.9534912109375\n",
      "Quantizing layer layers.29.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 4131.59130859375\n",
      "Quantizing layer layers.29.mlp.up_proj...\n",
      "Time for quantization: 1.22 seconds\n",
      "Total quantization error: 3743.063720703125\n",
      "Quantizing layer layers.29.mlp.down_proj...\n",
      "Time for quantization: 3.58 seconds\n",
      "Total quantization error: 754.6605224609375\n",
      "Quantizing layer layers.30.self_attn.q_proj...\n",
      "Time for quantization: 1.19 seconds\n",
      "Total quantization error: 8177.33984375\n",
      "Quantizing layer layers.30.self_attn.k_proj...\n",
      "Time for quantization: 1.18 seconds\n",
      "Total quantization error: 3632.24658203125\n",
      "Quantizing layer layers.30.self_attn.v_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 2507.53857421875\n",
      "Quantizing layer layers.30.self_attn.o_proj...\n",
      "Time for quantization: 1.17 seconds\n",
      "Total quantization error: 122.67948913574219\n",
      "Quantizing layer layers.30.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 4422.41845703125\n",
      "Quantizing layer layers.30.mlp.up_proj...\n",
      "Time for quantization: 1.24 seconds\n",
      "Total quantization error: 3885.106201171875\n",
      "Quantizing layer layers.30.mlp.down_proj...\n",
      "Time for quantization: 3.75 seconds\n",
      "Total quantization error: 1232.452880859375\n",
      "Quantizing layer layers.31.self_attn.q_proj...\n",
      "Time for quantization: 1.16 seconds\n",
      "Total quantization error: 3597.97900390625\n",
      "Quantizing layer layers.31.self_attn.k_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 3077.56298828125\n",
      "Quantizing layer layers.31.self_attn.v_proj...\n",
      "Time for quantization: 1.15 seconds\n",
      "Total quantization error: 1535.436767578125\n",
      "Quantizing layer layers.31.self_attn.o_proj...\n",
      "Time for quantization: 1.14 seconds\n",
      "Total quantization error: 246.47686767578125\n",
      "Quantizing layer layers.31.mlp.gate_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 3943.7197265625\n",
      "Quantizing layer layers.31.mlp.up_proj...\n",
      "Time for quantization: 1.20 seconds\n",
      "Total quantization error: 3550.89306640625\n",
      "Quantizing layer layers.31.mlp.down_proj...\n",
      "Time for quantization: 3.70 seconds\n",
      "Total quantization error: 3080.99609375\n",
      "Language Model quantization complete.\n",
      "\n",
      "LLAVA model quantization complete.\n",
      "Elapsed time: 741.7763066291809\n"
     ]
    }
   ],
   "source": [
    "quantizer = LlavaQuantizer(model, processor, device)\n",
    "\n",
    "# quantizer.quantize_vision_model(calibration_set)\n",
    "# quantizer.quantize_language_model(calibration_set)\n",
    "\n",
    "start_time = time.time()\n",
    "quantizer.quantize(calibration_set)\n",
    "\n",
    "print(f'Elapsed time: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f17f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: <image>\n",
      "Are there any boxes in the room?\n",
      "Answer the question using a single word or phrase. ASSISTANT:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['USER:  \\nAre there any boxes in the room?\\nAnswer the question using a single word or phrase. ASSISTANT: No']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = dataset[42]['image']\n",
    "prompt = 'USER: <image>\\n' + dataset.qa_pairs[42]['question'] + '\\nAnswer the question using a single word or phrase. ASSISTANT:'\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "model = model.to('cuda')\n",
    "samples = processor(images = [img],\n",
    "                     text=[prompt],\n",
    "                     return_tensors='pt',\n",
    "                     padding=True).to(model.device)\n",
    "\n",
    "# Generate\n",
    "# generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generate_ids = model.generate(**samples)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
