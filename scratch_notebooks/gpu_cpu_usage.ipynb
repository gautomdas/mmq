{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "777bfc6a-c131-433c-a47f-8fe4c6dd6983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage:\n",
      "CPU Memory: 4.57 GB / 31.27 GB\n",
      "GPU Memory: 550.00 MB / 24576.00 MB\n",
      "\n",
      "Loading model into CPU memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796c05c8cae844cd88649fba75efd541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Memory Used for Model: 14223.23 MB\n",
      "\n",
      "Memory Usage after loading to CPU:\n",
      "CPU Memory: 18.47 GB / 31.27 GB\n",
      "GPU Memory: 548.00 MB / 24576.00 MB\n",
      "\n",
      "Loading model into GPU memory...\n",
      "GPU Memory Used for Model: 14870.00 MB\n",
      "\n",
      "Memory Usage after loading to GPU:\n",
      "CPU Memory: 4.76 GB / 31.27 GB\n",
      "GPU Memory: 15422.00 MB / 24576.00 MB\n",
      "\n",
      "Removing model from GPU memory...\n",
      "sleeping for 5 seconds\n",
      "Final Memory Usage:\n",
      "CPU Memory: 4.79 GB / 31.27 GB\n",
      "GPU Memory: 813.00 MB / 24576.00 MB\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "777bfc6a-c131-433c-a47f-8fe4c6dd6983",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
<<<<<<< HEAD
    "def print_memory_usage():\n",
    "    # CPU Memory\n",
    "    cpu_memory = psutil.virtual_memory()\n",
    "    print(f\"CPU Memory: {cpu_memory.used / (1024 ** 3):.2f} GB / {cpu_memory.total / (1024 ** 3):.2f} GB\")\n",
    "    \n",
=======
    "\n",
    "def print_memory_usage():\n",
    "    # CPU Memory\n",
    "    cpu_memory = psutil.virtual_memory()\n",
    "    print(f\"CPU Memory: {cpu_memory.used / (1024**3):.2f} GB / {cpu_memory.total / (1024**3):.2f} GB\")\n",
    "\n",
>>>>>>> cleaned_old
    "    # GPU Memory\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        gpu = gpus[0]  # Assuming we're using the first GPU\n",
    "        print(f\"GPU Memory: {gpu.memoryUsed:.2f} MB / {gpu.memoryTotal:.2f} MB\")\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "# 1. Print initial memory usage\n",
    "print(\"Initial Memory Usage:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 2. Load model into CPU memory\n",
    "print(\"Loading model into CPU memory...\")\n",
    "cpu_memory_before = psutil.virtual_memory().used\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "cpu_memory_after = psutil.virtual_memory().used\n",
    "\n",
<<<<<<< HEAD
    "print(f\"CPU Memory Used for Model: {(cpu_memory_after - cpu_memory_before) / (1024 ** 2):.2f} MB\")\n",
=======
    "print(f\"CPU Memory Used for Model: {(cpu_memory_after - cpu_memory_before) / (1024**2):.2f} MB\")\n",
>>>>>>> cleaned_old
    "print()\n",
    "\n",
    "# 3. Print memory usage after loading to CPU\n",
    "print(\"Memory Usage after loading to CPU:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 4. Load model into GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Loading model into GPU memory...\")\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_before = gpus[0].memoryUsed if gpus else 0\n",
<<<<<<< HEAD
    "    \n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()  # Ensure the operation is complete\n",
    "    \n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_after = gpus[0].memoryUsed if gpus else 0\n",
    "    \n",
=======
    "\n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()  # Ensure the operation is complete\n",
    "\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_after = gpus[0].memoryUsed if gpus else 0\n",
    "\n",
>>>>>>> cleaned_old
    "    print(f\"GPU Memory Used for Model: {gpu_memory_after - gpu_memory_before:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Skipping GPU memory loading.\")\n",
    "print()\n",
    "\n",
    "# 5. Print memory usage after loading to GPU\n",
    "print(\"Memory Usage after loading to GPU:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 6. Remove model from GPU or move back to CPU\n",
    "remove_from_gpu = True  # Set this to False to move back to CPU instead\n",
    "\n",
    "if remove_from_gpu:\n",
    "    print(\"Removing model from GPU memory...\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Moving model back to CPU and removing from GPU memory...\")\n",
    "    model = model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"sleeping for 5 seconds\")\n",
    "import time\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "time.sleep(5)\n",
    "# 7. Print final memory usage\n",
    "print(\"Final Memory Usage:\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "9426d007-0e2d-46c5-98ee-a9a317ea16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage:\n",
      "CPU Memory: 4.80 GB / 31.27 GB\n",
      "GPU Memory: 813.00 MB / 24576.00 MB\n",
      "\n",
      "Loading model into CPU memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27add083716743e7b26a2eb1e3e9c63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Memory Used for Model: 14162.66 MB\n",
      "\n",
      "Memory Usage after loading to CPU:\n",
      "CPU Memory: 18.63 GB / 31.27 GB\n",
      "GPU Memory: 795.00 MB / 24576.00 MB\n",
      "\n",
      "--\n",
      "Memory Usage after Cloning Model:\n",
      "CPU Memory: 18.88 GB / 31.27 GB\n",
      "GPU Memory: 797.00 MB / 24576.00 MB\n",
      "\n",
      "Loading model into GPU memory...\n",
      "GPU Memory Used for Model: 14614.00 MB\n",
      "\n",
      "Memory Usage after loading to GPU:\n",
      "CPU Memory: 9.67 GB / 31.27 GB\n",
      "GPU Memory: 15411.00 MB / 24576.00 MB\n",
      "\n",
      "Removing model from GPU memory...\n",
      "sleeping for 5 seconds\n",
      "Final Memory Usage:\n",
      "CPU Memory: 9.34 GB / 31.27 GB\n",
      "GPU Memory: 801.00 MB / 24576.00 MB\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "9426d007-0e2d-46c5-98ee-a9a317ea16fb",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import copy\n",
    "\n",
    "# 1. Print initial memory usage\n",
    "print(\"Initial Memory Usage:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 2. Load model into CPU memory\n",
    "print(\"Loading model into CPU memory...\")\n",
    "cpu_memory_before = psutil.virtual_memory().used\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "cpu_memory_after = psutil.virtual_memory().used\n",
    "\n",
<<<<<<< HEAD
    "print(f\"CPU Memory Used for Model: {(cpu_memory_after - cpu_memory_before) / (1024 ** 2):.2f} MB\")\n",
=======
    "print(f\"CPU Memory Used for Model: {(cpu_memory_after - cpu_memory_before) / (1024**2):.2f} MB\")\n",
>>>>>>> cleaned_old
    "print()\n",
    "\n",
    "# 3. Print memory usage after loading to CPU\n",
    "print(\"Memory Usage after loading to CPU:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "def apply_uniform_quantization(model, num_bits):\n",
    "    def quantize_uniform(tensor, num_bits):\n",
    "        qmin, qmax = 0, 2**num_bits - 1\n",
    "        scale = (tensor.max() - tensor.min()) / (qmax - qmin)\n",
    "        zero_point = qmin - torch.round(tensor.min() / scale)\n",
<<<<<<< HEAD
    "        \n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "        \n",
=======
    "\n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "\n",
>>>>>>> cleaned_old
    "        return dequantized\n",
    "\n",
    "    def quantize_layer(layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.data = quantize_uniform(layer.weight.data, num_bits)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data = quantize_uniform(layer.bias.data, num_bits)\n",
    "        return layer\n",
    "\n",
    "    return model.apply(quantize_layer)\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "# Apply uniform quantization to the copied model\n",
    "model = apply_uniform_quantization(model, num_bits=6)  # You can adjust num_bits as needed\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> cleaned_old
    "# 3. Print memory usage after loading to CPU\n",
    "print(\"--\")\n",
    "print(\"Memory Usage after Cloning Model:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "\n",
    "# 4. Load model into GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Loading model into GPU memory...\")\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_before = gpus[0].memoryUsed if gpus else 0\n",
<<<<<<< HEAD
    "    \n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()  # Ensure the operation is complete\n",
    "    \n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_after = gpus[0].memoryUsed if gpus else 0\n",
    "    \n",
=======
    "\n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()  # Ensure the operation is complete\n",
    "\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_after = gpus[0].memoryUsed if gpus else 0\n",
    "\n",
>>>>>>> cleaned_old
    "    print(f\"GPU Memory Used for Model: {gpu_memory_after - gpu_memory_before:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Skipping GPU memory loading.\")\n",
    "print()\n",
    "\n",
    "# 5. Print memory usage after loading to GPU\n",
    "print(\"Memory Usage after loading to GPU:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 6. Remove model from GPU or move back to CPU\n",
    "remove_from_gpu = True  # Set this to False to move back to CPU instead\n",
    "\n",
    "if remove_from_gpu:\n",
    "    print(\"Removing model from GPU memory...\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Moving model back to CPU and removing from GPU memory...\")\n",
    "    model = model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"sleeping for 5 seconds\")\n",
    "import time\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "time.sleep(5)\n",
    "# 7. Print final memory usage\n",
    "print(\"Final Memory Usage:\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba461c-c3ba-468e-be37-c71738d05a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# 1. Print initial memory usage\n",
    "print(\"Initial Memory Usage:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 2. Load model into CPU memory\n",
    "print(\"Loading model into CPU memory...\")\n",
    "cpu_memory_before = psutil.virtual_memory().used\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "cpu_memory_after = psutil.virtual_memory().used\n",
    "\n",
<<<<<<< HEAD
    "print(f\"CPU Memory Used for Model: {(cpu_memory_after - cpu_memory_before) / (1024 ** 2):.2f} MB\")\n",
=======
    "print(f\"CPU Memory Used for Model: {(cpu_memory_after - cpu_memory_before) / (1024**2):.2f} MB\")\n",
>>>>>>> cleaned_old
    "print()\n",
    "\n",
    "# 3. Print memory usage after loading to CPU\n",
    "print(\"Memory Usage after loading to CPU:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "def apply_uniform_quantization(model, num_bits):\n",
    "    def quantize_uniform(tensor, num_bits):\n",
    "        qmin, qmax = 0, 2**num_bits - 1\n",
    "        scale = (tensor.max() - tensor.min()) / (qmax - qmin)\n",
    "        zero_point = qmin - torch.round(tensor.min() / scale)\n",
<<<<<<< HEAD
    "        \n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "        \n",
=======
    "\n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "\n",
>>>>>>> cleaned_old
    "        return dequantized\n",
    "\n",
    "    def quantize_layer(layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.data = quantize_uniform(layer.weight.data, num_bits)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data = quantize_uniform(layer.bias.data, num_bits)\n",
    "        return layer\n",
    "\n",
    "    return model.apply(quantize_layer)\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "# Apply uniform quantization to the copied model\n",
    "model = apply_uniform_quantization(model, num_bits=6)  # You can adjust num_bits as needed\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "\n",
=======
>>>>>>> cleaned_old
    "# 3. Print memory usage after loading to CPU\n",
    "print(\"--\")\n",
    "print(\"Memory Usage after Cloning Model:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "\n",
    "# 4. Load model into GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Loading model into GPU memory...\")\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_before = gpus[0].memoryUsed if gpus else 0\n",
<<<<<<< HEAD
    "    \n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()  # Ensure the operation is complete\n",
    "    \n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_after = gpus[0].memoryUsed if gpus else 0\n",
    "    \n",
=======
    "\n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()  # Ensure the operation is complete\n",
    "\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory_after = gpus[0].memoryUsed if gpus else 0\n",
    "\n",
>>>>>>> cleaned_old
    "    print(f\"GPU Memory Used for Model: {gpu_memory_after - gpu_memory_before:.2f} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Skipping GPU memory loading.\")\n",
    "print()\n",
    "\n",
    "# 5. Print memory usage after loading to GPU\n",
    "print(\"Memory Usage after loading to GPU:\")\n",
    "print_memory_usage()\n",
    "print()\n",
    "\n",
    "# 6. Remove model from GPU or move back to CPU\n",
    "remove_from_gpu = True  # Set this to False to move back to CPU instead\n",
    "\n",
    "if remove_from_gpu:\n",
    "    print(\"Removing model from GPU memory...\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Moving model back to CPU and removing from GPU memory...\")\n",
    "    model = model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"sleeping for 5 seconds\")\n",
    "import time\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "time.sleep(5)\n",
    "# 7. Print final memory usage\n",
    "print(\"Final Memory Usage:\")\n",
    "print_memory_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.7"
=======
   "version": "3.12.4"
>>>>>>> cleaned_old
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
