{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596b33cf-382f-4121-ac88-92799c65e284",
   "metadata": {},
   "source": [
    "# Demo of Blip2 Quantization, Inference, and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de58b3-4ee9-4267-80c5-1a41582fe3cb",
   "metadata": {},
   "source": [
    "## 1. Load Model and Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c695ac4-7895-4a53-b4da-579984a1899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blip_quantizer import BlipQuantizer, QuantConfig, ModelPart, LayerGroup, LayerType\n",
    "from quant_functions import uniform_quantization\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer\n",
    "from datasets import VQAv2Eval\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import print_model_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13309c9-5353-41f8-8bec-8351f4c82504",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqav2_dataset = VQAv2Eval(\n",
    "    image_root=\"./data/vqav2/val2014\",\n",
    "    ann_root=\"./data/vqav2/annotations\",\n",
    "    q_root=\"./data/vqav2/questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00346f3b-378a-4008-b985-ba5619a350f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "configs = [\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.FIRST,\n",
    "        LayerType.BOTH,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=4,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "# quantizer.apply_quantization(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f0cfd-8957-4798-b2b6-4dca7f8551df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", padding_side=\"left\")\n",
    "\n",
    "processor_kwargs = {\"padding\": \"longest\", \"max_length\": 32, \"truncation\": True}\n",
    "generate_kwargs = {\n",
    "    \"num_beams\": 5,\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"min_length\": 1,\n",
    "    \"length_penalty\": 0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "inferencer = InferencePipeline(model, device, processor)\n",
    "print(\"Starting inference...\")\n",
    "results = inferencer.run_inference(\n",
    "    vqav2_dataset,\n",
    "    task=\"visual_question_answering\",\n",
    "    max_samples=10,\n",
    "    processor_kwargs=processor_kwargs,\n",
    "    generate_kwargs=generate_kwargs,\n",
    ")\n",
    "print(\"Inference Finished, Saving Results...\")\n",
    "inferencer.save_results(results, \"./results/vqav2_quantized_inference.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feab94b-f109-48a2-8efb-793181262cc3",
   "metadata": {},
   "source": [
    "## 3. Score Results from .json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba17c86-c583-425b-8152-7aca8dcbe02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "scorer = ScoringPipeline()\n",
    "loaded_results = scorer.load_results(\"./results/vqav2_quantized_inference.json\")\n",
    "\n",
    "loaded_results[\"annotations\"] = \"./data/vqav2/annotations/v2_mscoco_val2014_annotations.json\"\n",
    "loaded_results[\"questions\"] = \"./data/vqav2/questions/v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    "\n",
    "scores = scorer.compute_scores(loaded_results, task=\"visual_question_answering\")\n",
    "\n",
    "for metric, score in scores.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774281d4-2d75-40ca-86b3-4a1338b214ff",
   "metadata": {},
   "source": [
    "## Sample Results\n",
    "\n",
    "This is not a necessary step but just helps qualitatively understand how the results relate to the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244503d-3acb-45db-8e2c-9aa5e5187ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "f = open(\"./results/vqav2_quantized_inference.json\")\n",
    "\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "def show_results(idx, results, vqav2_dataset):\n",
    "    pair = vqav2_dataset.qa_pairs[idx]\n",
    "    image = Image.open(vqav2_dataset.image_root + \"/\" + pair[\"image\"]).convert(\"RGB\")\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print(pair[\"question\"])\n",
    "    print(f\"true answers: {pair['answer']}\")\n",
    "\n",
    "    pred_ans = []\n",
    "    for qa in results[\"answers\"]:\n",
    "        if qa[\"question_id\"] == pair[\"question_id\"]:\n",
    "            pred_ans.append(qa[\"answer\"].strip())\n",
    "\n",
    "    print(f\"pred answers: {pred_ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce264aa-7dc2-4d10-a889-ded5079a6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(1, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2051df-dde4-4f25-bdd8-c780e17cf864",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(3, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57dacb-bd85-4b0f-8b92-edca066ecf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(5, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4701b24-c336-4a88-aa1e-8c28ddc0b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(7, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb32a53-2f94-42a9-b31c-cb352f9cc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(9, data, vqav2_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b2e3f-6b2d-4722-ab05-f615797055c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
