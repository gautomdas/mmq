{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e9eed2-a8db-4c04-bf14-a0cbd53383c3",
   "metadata": {},
   "source": [
    "# Demo of Blip2 Quantization, Inference, and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b1b66-5d41-46e7-83a0-eb197d2eb84a",
   "metadata": {},
   "source": [
    "## 1. Load Model and Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373a9d1-1924-43bd-8ac2-763627d98cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor, AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# Paths to your local COCO dataset\n",
    "COCO_DIR = \"../data/coco\"  # Replace with your COCO dataset directory\n",
    "IMAGE_DIR = os.path.join(COCO_DIR, \"val2017\")  # Adjust if your image directory is different\n",
    "ANNOTATION_FILE = os.path.join(\n",
    "    COCO_DIR, \"annotations\", \"captions_val2017.json\"\n",
    ")  # Adjust if your annotation file is different\n",
    "\n",
    "# Load COCO annotations\n",
    "with open(ANNOTATION_FILE, \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "\n",
    "def get_random_samples(annotations, num_samples=128):\n",
    "    image_ids = list(set(ann[\"image_id\"] for ann in annotations[\"annotations\"]))\n",
    "    selected_ids = random.sample(image_ids, num_samples)\n",
    "\n",
    "    samples = []\n",
    "    for ann in annotations[\"annotations\"]:\n",
    "        if ann[\"image_id\"] in selected_ids:\n",
    "            image_info = next(img for img in annotations[\"images\"] if img[\"id\"] == ann[\"image_id\"])\n",
    "            samples.append(\n",
    "                {\n",
    "                    \"image_file\": os.path.join(IMAGE_DIR, image_info[\"file_name\"]),\n",
    "                    \"caption\": ann[\"caption\"],\n",
    "                }\n",
    "            )\n",
    "            if len(samples) == num_samples:\n",
    "                break\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Get random samples for calibration\n",
    "calibration_data = get_random_samples(annotations, num_samples=128)\n",
    "\n",
    "\n",
    "# Process calibration data\n",
    "def process_example(example):\n",
    "    image = Image.open(example[\"image_file\"]).convert(\"RGB\")\n",
    "    inputs = processor(images=image, text=example[\"caption\"], return_tensors=\"pt\", padding=True)\n",
    "    return {\n",
    "        \"input_ids\": inputs.input_ids,\n",
    "        \"attention_mask\": inputs.attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "examples = [process_example(example) for example in calibration_data]\n",
    "\n",
    "# Save the language model separately\n",
    "language_model = model.language_model\n",
    "language_model_path = \"./blip2_language_model\"\n",
    "language_model.save_pretrained(language_model_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(language_model_path)\n",
    "\n",
    "# Define quantization config\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    ")\n",
    "\n",
    "# Load and quantize the language model\n",
    "quantized_model = AutoGPTQForCausalLM.from_pretrained(language_model_path, quantize_config)\n",
    "quantized_model.quantize(examples)\n",
    "\n",
    "# print_model_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b8bd-2935-48f3-ba5a-d072c9b5db1f",
   "metadata": {},
   "source": [
    "## 2. Run Inference on Model and Generate a .json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf557c0-17e3-49b4-b89a-208ba781718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_pipeline import InferencePipeline\n",
    "from dataset import COCODataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")\n",
    "\n",
    "inferencer = InferencePipeline(quantized_model, device, processor)\n",
    "print(\"Starting inference...\")\n",
    "results = inferencer.run_inference(coco_dataset, task=\"image_captioning\", max_samples=20)\n",
    "print(\"Inference Finished, Saving Results...\")\n",
    "inferencer.save_results(results, \"./results/coco_quantized_inference.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6517463-d2a5-4fa4-8215-53846a880086",
   "metadata": {},
   "source": [
    "## 3. Score Results from .json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbebad8-8e91-4bc9-9de3-9854335d2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "scorer = ScoringPipeline()\n",
    "loaded_results = scorer.load_results(\"./results/coco_quantized_inference.json\")\n",
    "scores = scorer.compute_scores(loaded_results, task=\"image_captioning\")\n",
    "\n",
    "for metric, score in scores.items():\n",
    "    if not metric.endswith(\"_per_caption\"):\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e50f2-28bc-48ad-ac42-75d656b4245b",
   "metadata": {},
   "source": [
    "## Sample Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832e8d6-48be-4328-a9b2-06aa94ebddd9",
   "metadata": {},
   "source": [
    "This is not a necessary step but just helps qualitatively understand how the results relate to the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dde190-8399-40fe-bf65-db7484f60629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\n",
    "    \"./results/coco_quantized_inference.json\",\n",
    ")\n",
    "\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "for i in range(0, 5):\n",
    "    img_id, caption = data[\"predictions\"][i].values()\n",
    "    references = data[\"references\"][i]\n",
    "    print(f\"Image Id: {img_id}\\nPredicted Caption:{caption}\")\n",
    "    print(f\"Reference Captions: {' '.join(references)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000a26-5414-4efe-bf90-71a39bd22292",
   "metadata": {},
   "source": [
    "### Here's what the first predicted image caption from above looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fe9e1-50a1-4a1b-9cbd-3a7b6ef9d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb426f-4b69-4f5a-935f-6fcffd907032",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44456709-defa-4fc0-839c-502c8480c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.to(\"cpu\")\n",
    "del model, evaluator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51688d6d-245e-4953-b254-4beb1cc5ec35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
