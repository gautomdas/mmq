{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e9eed2-a8db-4c04-bf14-a0cbd53383c3",
   "metadata": {},
   "source": [
    "# Demo of Blip2 Quantization, Inference, and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b1b66-5d41-46e7-83a0-eb197d2eb84a",
   "metadata": {},
   "source": [
    "## 1. Load Model and Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373a9d1-1924-43bd-8ac2-763627d98cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blip_quantizer import BlipQuantizer, QuantConfig, ModelPart, LayerGroup, LayerType\n",
    "from quant_functions import uniform_quantization\n",
    "import torch\n",
    "from transformers import Blip2ForImageTextRetrieval\n",
    "from dataset import Flickr30kEvalDataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import print_model_structure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Blip2ForImageTextRetrieval.from_pretrained(\"Salesforce/blip2-itm-vit-g-coco\", torch_dtype=torch.float16)\n",
    "model = model.to(device)\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "configs = [\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.FIRST,\n",
    "        LayerType.BOTH,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=4,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "quantizer.apply_quantization(configs)\n",
    "\n",
    "# print_model_structure(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b8bd-2935-48f3-ba5a-d072c9b5db1f",
   "metadata": {},
   "source": [
    "## 2. Run Inference on Model and Generate a .json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf557c0-17e3-49b4-b89a-208ba781718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Flickr30kEvalDataset\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "img_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "flickr30k = Flickr30kEvalDataset(\n",
    "    \"./data/flickr30k/annotations/flickr30k_test.json\",\n",
    "    \"./data/flickr30k/images_flickr_1k_test\",\n",
    "    img_transform=img_transform,\n",
    ")\n",
    "\n",
    "inferencer = InferencePipeline(model, device)\n",
    "\n",
    "results = inferencer.run_inference(flickr30k, task=\"image_text_retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6517463-d2a5-4fa4-8215-53846a880086",
   "metadata": {},
   "source": [
    "## 3. Score Results from .json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbebad8-8e91-4bc9-9de3-9854335d2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "scorer = ScoringPipeline()\n",
    "retrieval_results = scorer.compute_scores(results, \"image_text_retrieval\")\n",
    "\n",
    "print(retrieval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e50f2-28bc-48ad-ac42-75d656b4245b",
   "metadata": {},
   "source": [
    "## Sample Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832e8d6-48be-4328-a9b2-06aa94ebddd9",
   "metadata": {},
   "source": [
    "This is not a necessary step but just helps qualitatively understand how the results relate to the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dde190-8399-40fe-bf65-db7484f60629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\n",
    "    \"./results/coco_quantized_inference.json\",\n",
    ")\n",
    "\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "for i in range(0, 5):\n",
    "    img_id, caption = data[\"predictions\"][i].values()\n",
    "    references = data[\"references\"][i]\n",
    "    print(f\"Image Id: {img_id}\\nPredicted Caption:{caption}\")\n",
    "    print(f\"Reference Captions: {' '.join(references)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03000a26-5414-4efe-bf90-71a39bd22292",
   "metadata": {},
   "source": [
    "### Here's what the first predicted image caption from above looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fe9e1-50a1-4a1b-9cbd-3a7b6ef9d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb426f-4b69-4f5a-935f-6fcffd907032",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44456709-defa-4fc0-839c-502c8480c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.to(\"cpu\")\n",
    "del model, evaluator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51688d6d-245e-4953-b254-4beb1cc5ec35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
