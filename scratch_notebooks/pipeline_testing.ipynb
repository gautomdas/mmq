{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373a9d1-1924-43bd-8ac2-763627d98cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from blip_quantizer import BlipQuantizer, QuantConfig, ModelPart, LayerGroup, LayerType\n",
    "from quant_functions import uniform_quantization\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import print_model_structure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "configs = [\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=4,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.LLM,\n",
    "        LayerGroup.MIDDLE,\n",
    "        LayerType.MLP,\n",
    "        uniform_quantization,\n",
    "        num_bits=4,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "quantizer.apply_quantization(configs)\n",
    "\n",
    "# print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a6472-a6a8-4eaa-a8c3-07fcb2ffe50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_pipeline import EvaluationPipeline\n",
    "\n",
    "# from evaluation_pipeline import EvaluationPipeline\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")\n",
    "\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = EvaluationPipeline(model, processor, device)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Starting evaluation...\")\n",
    "coco_results = evaluator.evaluate(coco_dataset, task=\"image_captioning\", max_samples=1000)\n",
    "\n",
    "# Save results\n",
    "evaluator.save_results(coco_results, \"./results/coco_quantized_evaluation.json\")\n",
    "\n",
    "# Print overall CIDEr score\n",
    "print(f\"COCO CIDEr score: {coco_results['overall_cider']}\")\n",
    "\n",
    "# Print a few example predictions\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(5):  # Print first 5 predictions\n",
    "    print(f\"Image ID: {coco_results['predictions'][i]['image_id']}\")\n",
    "    print(f\"Prediction: {coco_results['predictions'][i]['caption']}\")\n",
    "    print(f\"References: {coco_results['references'][i]}\")\n",
    "    print(f\"Individual CIDEr score: {coco_results['individual_cider'][i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf557c0-17e3-49b4-b89a-208ba781718f",
   "metadata": {},
   "source": [
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Set up the model, processor, and dataset as before\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "inferencer = InferencePipeline(model, processor, device)\n",
    "print(\"Starting inference...\")\n",
    "results = inferencer.run_inference(coco_dataset, task=\"image_captioning\", max_samples=20)\n",
    "inferencer.save_results(results, \"./results/coco_quantized_inference.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbebad8-8e91-4bc9-9de3-9854335d2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "# Compute scores\n",
    "scorer = ScoringPipeline()\n",
    "loaded_results = scorer.load_results(\"./results/coco_quantized_inference.json\")\n",
    "scores = scorer.compute_scores(loaded_results, task=\"image_captioning\")\n",
    "\n",
    "# Print scores\n",
    "for metric, score in scores.items():\n",
    "    if not metric.endswith(\"_per_caption\"):\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44456709-defa-4fc0-839c-502c8480c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.to(\"cpu\")\n",
    "del model, evaluator\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44118fa-7186-4f78-93bc-f215dae898fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
