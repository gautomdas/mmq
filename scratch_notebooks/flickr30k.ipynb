{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "be537dae-0272-4160-84a7-c4da53eaeb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28b7dc8d97044a99382840d493cd041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "id": "be537dae-0272-4160-84a7-c4da53eaeb7b",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "from transformers import Blip2Model, Blip2Processor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Flickr30k\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "def encode_images(dataloader):\n",
    "    encodings = []\n",
    "    for images, _ in tqdm(dataloader, desc=\"Encoding images\"):\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "            # Use the pooled output of the vision model\n",
    "            image_features = outputs.pooler_output\n",
    "        encodings.append(image_features.cpu().numpy())\n",
    "    return np.vstack(encodings)\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "def encode_text(captions):\n",
    "    encodings = []\n",
    "    for caption_group in tqdm(captions, desc=\"Encoding text\"):\n",
    "        caption = caption_group[0]  # Take the first caption for each image\n",
    "        inputs = processor(text=caption, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_text_features(**inputs)\n",
    "            # Use the last hidden state of the last token as the text feature\n",
    "            text_features = outputs.logits[:, -1, :]\n",
    "        encodings.append(text_features.cpu().numpy())\n",
    "    return np.vstack(encodings)\n",
<<<<<<< HEAD
    "    \n",
=======
    "\n",
    "\n",
>>>>>>> cleaned_old
    "def calculate_recall(similarities, k_values):\n",
    "    recalls = {}\n",
    "    for k in k_values:\n",
    "        top_k = np.argsort(-similarities, axis=1)[:, :k]\n",
    "        recall_at_k = np.mean([1 if i in row else 0 for i, row in enumerate(top_k)])\n",
<<<<<<< HEAD
    "        recalls[f'R@{k}'] = recall_at_k\n",
    "    return recalls\n",
    "\n",
    "def perform_retrieval(dataset, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: ([item[0] for item in x], [item[1] for item in x]))\n",
    "    \n",
=======
    "        recalls[f\"R@{k}\"] = recall_at_k\n",
    "    return recalls\n",
    "\n",
    "\n",
    "def perform_retrieval(dataset, batch_size=32):\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: ([item[0] for item in x], [item[1] for item in x]),\n",
    "    )\n",
    "\n",
>>>>>>> cleaned_old
    "    image_features = encode_images(dataloader)\n",
    "    text_features = encode_text([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "    # Normalize features\n",
    "    image_features /= np.linalg.norm(image_features, axis=1, keepdims=True)\n",
    "    text_features /= np.linalg.norm(text_features, axis=1, keepdims=True)\n",
    "\n",
    "    # Calculate similarities\n",
    "    similarities = np.dot(image_features, text_features.T)\n",
    "\n",
    "    # Calculate metrics\n",
    "    k_values = [1, 5, 10]\n",
    "    i2t_recalls = calculate_recall(similarities, k_values)\n",
    "    t2i_recalls = calculate_recall(similarities.T, k_values)\n",
    "\n",
<<<<<<< HEAD
    "    return i2t_recalls, t2i_recalls\n"
=======
    "    return i2t_recalls, t2i_recalls"
>>>>>>> cleaned_old
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4ed3b-0b7b-4f2e-97f0-4c04083ae299",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "# Load your Flickr30k dataset\n",
    "flickr_dataset = Flickr30k(csv_file='./data/flickr30k/results.csv', \n",
    "                           img_dir='./data/flickr30k/images/')\n",
=======
    "# Load your Flickr30k dataset\n",
    "flickr_dataset = Flickr30k(csv_file=\"./data/flickr30k/results.csv\", img_dir=\"./data/flickr30k/images/\")\n",
>>>>>>> cleaned_old
    "\n",
    "# Create a subset of the first 1000 images\n",
    "subset_indices = list(range(1000))\n",
    "subset_dataset = Subset(flickr_dataset, subset_indices)\n",
    "\n",
    "# Perform retrieval\n",
    "i2t_recalls, t2i_recalls = perform_retrieval(subset_dataset)\n",
    "\n",
    "print(\"Image-to-Text Retrieval:\")\n",
    "for k, v in i2t_recalls.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nText-to-Image Retrieval:\")\n",
    "for k, v in t2i_recalls.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "id": "7ad0b994-cb30-4dc5-88b7-b0a65d7ce4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb5723d37304bdc952587999d5cbbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeds shape: torch.Size([5, 257, 1408])\n",
      "Text embeds shape: torch.Size([5, 21, 2560])\n",
      "Image features shape: torch.Size([5, 256])\n",
      "Text features shape: torch.Size([5, 256])\n",
      "Similarity matrix shape: torch.Size([5, 5])\n",
      "For image 0, best matching text is 0\n",
      "Query image caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
      "Retrieved text: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "7ad0b994-cb30-4dc5-88b7-b0a65d7ce4dd",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Blip2Processor, Blip2Model\n",
    "from datasets import Flickr30k\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load a small subset of Flickr30k dataset\n",
<<<<<<< HEAD
    "flickr_dataset = Flickr30k(csv_file='./data/flickr30k/results.csv', \n",
    "                           img_dir='./data/flickr30k/images/')\n",
=======
    "flickr_dataset = Flickr30k(csv_file=\"./data/flickr30k/results.csv\", img_dir=\"./data/flickr30k/images/\")\n",
>>>>>>> cleaned_old
    "subset_size = 5  # Small subset for example\n",
    "subset_dataset = Subset(flickr_dataset, range(subset_size))\n",
    "\n",
    "# We'll define the projection layers inside the encode function after we know the correct dimensions\n",
    "\n",
<<<<<<< HEAD
    "def encode_image_and_text(images, captions):\n",
    "    # Process images\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Process text\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
=======
    "\n",
    "def encode_image_and_text(images, captions):\n",
    "    # Process images\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Process text\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
>>>>>>> cleaned_old
    "    with torch.no_grad():\n",
    "        # Get image features\n",
    "        image_outputs = model.get_image_features(**image_inputs)\n",
    "        image_embeds = image_outputs.last_hidden_state\n",
    "        print(f\"Image embeds shape: {image_embeds.shape}\")\n",
<<<<<<< HEAD
    "        \n",
=======
    "\n",
>>>>>>> cleaned_old
    "        # Get text features\n",
    "        text_outputs = model.get_text_features(**text_inputs, output_hidden_states=True)\n",
    "        text_embeds = text_outputs.hidden_states[-1]  # Use the last hidden state\n",
    "        print(f\"Text embeds shape: {text_embeds.shape}\")\n",
<<<<<<< HEAD
    "        \n",
    "        # Define projection layers with correct dimensions\n",
    "        vision_proj = torch.nn.Linear(image_embeds.shape[-1], 256).to(device)\n",
    "        text_proj = torch.nn.Linear(text_embeds.shape[-1], 256).to(device)\n",
    "        \n",
    "        image_feats = F.normalize(vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        text_feat = F.normalize(text_proj(text_embeds[:, 0, :]), dim=-1)\n",
    "        \n",
    "    return image_feats, text_feat\n",
    "\n",
=======
    "\n",
    "        # Define projection layers with correct dimensions\n",
    "        vision_proj = torch.nn.Linear(image_embeds.shape[-1], 256).to(device)\n",
    "        text_proj = torch.nn.Linear(text_embeds.shape[-1], 256).to(device)\n",
    "\n",
    "        image_feats = F.normalize(vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        text_feat = F.normalize(text_proj(text_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "    return image_feats, text_feat\n",
    "\n",
    "\n",
>>>>>>> cleaned_old
    "# Encode all images and texts in the subset\n",
    "images, captions = zip(*[(item[0], item[1][0]) for item in subset_dataset])\n",
    "image_feats, text_feats = encode_image_and_text(list(images), list(captions))\n",
    "\n",
    "print(f\"Image features shape: {image_feats.shape}\")\n",
    "print(f\"Text features shape: {text_feats.shape}\")\n",
    "\n",
<<<<<<< HEAD
    "def compute_similarity(image_feats, text_feats):\n",
    "    return torch.matmul(image_feats, text_feats.t())\n",
    "\n",
=======
    "\n",
    "def compute_similarity(image_feats, text_feats):\n",
    "    return torch.matmul(image_feats, text_feats.t())\n",
    "\n",
    "\n",
>>>>>>> cleaned_old
    "# Compute similarity matrix\n",
    "similarity_matrix = compute_similarity(image_feats, text_feats)\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "\n",
    "# Example: Retrieve text for the first image\n",
    "image_idx = 0\n",
    "scores = similarity_matrix[image_idx]\n",
    "best_text_idx = scores.argmax().item()\n",
    "\n",
    "print(f\"For image {image_idx}, best matching text is {best_text_idx}\")\n",
    "print(f\"Query image caption: {captions[image_idx]}\")\n",
    "print(f\"Retrieved text: {captions[best_text_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "9c9eee9c-caec-4336-bac9-b2b971b690cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139abe72cced460aa4a1889332685db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeds shape: torch.Size([20, 257, 1408])\n",
      "Text embeds shape: torch.Size([20, 25, 2560])\n",
      "Image features shape: torch.Size([20, 256])\n",
      "Text features shape: torch.Size([20, 256])\n",
      "Similarity matrix shape: torch.Size([20, 20])\n",
      "For image 0, best matching text is 0\n",
      "Query image caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
      "Retrieved text: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "9c9eee9c-caec-4336-bac9-b2b971b690cd",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Blip2Processor, Blip2Model\n",
    "from datasets import Flickr30k\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load a small subset of Flickr30k dataset\n",
<<<<<<< HEAD
    "flickr_dataset = Flickr30k(csv_file='./data/flickr30k/results.csv', \n",
    "                           img_dir='./data/flickr30k/images/')\n",
=======
    "flickr_dataset = Flickr30k(csv_file=\"./data/flickr30k/results.csv\", img_dir=\"./data/flickr30k/images/\")\n",
>>>>>>> cleaned_old
    "subset_size = 20  # Small subset for example\n",
    "subset_dataset = Subset(flickr_dataset, range(subset_size))\n",
    "\n",
    "# We'll define the projection layers inside the encode function after we know the correct dimensions\n",
    "\n",
<<<<<<< HEAD
    "def encode_image_and_text(images, captions):\n",
    "    # Process images\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Process text\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
=======
    "\n",
    "def encode_image_and_text(images, captions):\n",
    "    # Process images\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Process text\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
>>>>>>> cleaned_old
    "    with torch.no_grad():\n",
    "        # Get image features\n",
    "        image_outputs = model.get_image_features(**image_inputs)\n",
    "        image_embeds = image_outputs.last_hidden_state\n",
    "        print(f\"Image embeds shape: {image_embeds.shape}\")\n",
<<<<<<< HEAD
    "        \n",
=======
    "\n",
>>>>>>> cleaned_old
    "        # Get text features\n",
    "        text_outputs = model.get_text_features(**text_inputs, output_hidden_states=True)\n",
    "        text_embeds = text_outputs.hidden_states[-1]  # Use the last hidden state\n",
    "        print(f\"Text embeds shape: {text_embeds.shape}\")\n",
<<<<<<< HEAD
    "        \n",
    "        # Define projection layers with correct dimensions\n",
    "        vision_proj = torch.nn.Linear(image_embeds.shape[-1], 256).to(device)\n",
    "        text_proj = torch.nn.Linear(text_embeds.shape[-1], 256).to(device)\n",
    "        \n",
    "        image_feats = F.normalize(vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        text_feat = F.normalize(text_proj(text_embeds[:, 0, :]), dim=-1)\n",
    "        \n",
    "    return image_feats, text_feat\n",
    "\n",
=======
    "\n",
    "        # Define projection layers with correct dimensions\n",
    "        vision_proj = torch.nn.Linear(image_embeds.shape[-1], 256).to(device)\n",
    "        text_proj = torch.nn.Linear(text_embeds.shape[-1], 256).to(device)\n",
    "\n",
    "        image_feats = F.normalize(vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        text_feat = F.normalize(text_proj(text_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "    return image_feats, text_feat\n",
    "\n",
    "\n",
>>>>>>> cleaned_old
    "# Encode all images and texts in the subset\n",
    "images, captions = zip(*[(item[0], item[1][0]) for item in subset_dataset])\n",
    "image_feats, text_feats = encode_image_and_text(list(images), list(captions))\n",
    "\n",
    "print(f\"Image features shape: {image_feats.shape}\")\n",
    "print(f\"Text features shape: {text_feats.shape}\")\n",
    "\n",
<<<<<<< HEAD
    "def compute_similarity(image_feats, text_feats):\n",
    "    return torch.matmul(image_feats, text_feats.t())\n",
    "\n",
=======
    "\n",
    "def compute_similarity(image_feats, text_feats):\n",
    "    return torch.matmul(image_feats, text_feats.t())\n",
    "\n",
    "\n",
>>>>>>> cleaned_old
    "# Compute similarity matrix\n",
    "similarity_matrix = compute_similarity(image_feats, text_feats)\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "\n",
    "# Example: Retrieve text for the first image\n",
    "image_idx = 0\n",
    "scores = similarity_matrix[image_idx]\n",
    "best_text_idx = scores.argmax().item()\n",
    "\n",
    "print(f\"For image {image_idx}, best matching text is {best_text_idx}\")\n",
    "print(f\"Query image caption: {captions[image_idx]}\")\n",
    "print(f\"Retrieved text: {captions[best_text_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "id": "12b4af70-ae28-44b7-8592-f69e0d291c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For image 10, best matching text is 0\n",
      "Query image caption: Five ballet dancers caught mid jump in a dancing studio with sunlight coming through a window .\n",
      "Retrieved text: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "12b4af70-ae28-44b7-8592-f69e0d291c7e",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "image_idx = 10\n",
    "scores = similarity_matrix[image_idx]\n",
    "best_text_idx = scores.argmax().item()\n",
    "\n",
    "print(f\"For image {image_idx}, best matching text is {best_text_idx}\")\n",
    "print(f\"Query image caption: {captions[image_idx]}\")\n",
    "print(f\"Retrieved text: {captions[best_text_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "4bf3c7da-2584-4adb-8fa5-0eba73d2cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd23f39395c94a7a8634295e8becca59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeds shape: torch.Size([1, 257, 1408])\n",
      "Text embeds shape: torch.Size([1, 20, 2560])\n",
      "Image embedding shape: torch.Size([1, 256])\n",
      "Text embedding shape: torch.Size([1, 256])\n",
      "Cosine similarity: 0.0688\n",
      "\n",
      "Caption: Two young guys with shaggy hair look at their hands while hanging out in the yard .\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "4bf3c7da-2584-4adb-8fa5-0eba73d2cea8",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Blip2Processor, Blip2Model\n",
    "from datasets import Flickr30k\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load Flickr30k dataset\n",
<<<<<<< HEAD
    "flickr_dataset = Flickr30k(csv_file='./data/flickr30k/results.csv', \n",
    "                           img_dir='./data/flickr30k/images/')\n",
    "subset_size = 5  # Small subset for example\n",
    "subset_dataset = Subset(flickr_dataset, range(subset_size))\n",
    "\n",
    "def encode_image_and_text(image, caption):\n",
    "    # Process image\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Process text\n",
    "    text_inputs = processor(text=caption, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
=======
    "flickr_dataset = Flickr30k(csv_file=\"./data/flickr30k/results.csv\", img_dir=\"./data/flickr30k/images/\")\n",
    "subset_size = 5  # Small subset for example\n",
    "subset_dataset = Subset(flickr_dataset, range(subset_size))\n",
    "\n",
    "\n",
    "def encode_image_and_text(image, caption):\n",
    "    # Process image\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # Process text\n",
    "    text_inputs = processor(text=caption, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
>>>>>>> cleaned_old
    "    with torch.no_grad():\n",
    "        # Get image features\n",
    "        image_outputs = model.get_image_features(**image_inputs)\n",
    "        image_embeds = image_outputs.last_hidden_state\n",
    "        print(f\"Image embeds shape: {image_embeds.shape}\")\n",
<<<<<<< HEAD
    "        \n",
=======
    "\n",
>>>>>>> cleaned_old
    "        # Get text features\n",
    "        text_outputs = model.get_text_features(**text_inputs, output_hidden_states=True)\n",
    "        text_embeds = text_outputs.hidden_states[-1]  # Use the last hidden state\n",
    "        print(f\"Text embeds shape: {text_embeds.shape}\")\n",
<<<<<<< HEAD
    "        \n",
    "        # Define projection layers with correct dimensions\n",
    "        vision_proj = torch.nn.Linear(image_embeds.shape[-1], 256).to(device)\n",
    "        text_proj = torch.nn.Linear(text_embeds.shape[-1], 256).to(device)\n",
    "        \n",
    "        image_feats = F.normalize(vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        text_feat = F.normalize(text_proj(text_embeds[:, 0, :]), dim=-1)\n",
    "        \n",
    "    return image_feats, text_feat\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2)\n",
    "\n",
=======
    "\n",
    "        # Define projection layers with correct dimensions\n",
    "        vision_proj = torch.nn.Linear(image_embeds.shape[-1], 256).to(device)\n",
    "        text_proj = torch.nn.Linear(text_embeds.shape[-1], 256).to(device)\n",
    "\n",
    "        image_feats = F.normalize(vision_proj(image_embeds[:, 0, :]), dim=-1)\n",
    "        text_feat = F.normalize(text_proj(text_embeds[:, 0, :]), dim=-1)\n",
    "\n",
    "    return image_feats, text_feat\n",
    "\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "\n",
>>>>>>> cleaned_old
    "# Get embeddings for a single image-text pair\n",
    "image, captions = subset_dataset[0]\n",
    "text = captions[0]  # Use the first caption\n",
    "\n",
    "image_embedding, text_embedding = encode_image_and_text(image, text)\n",
    "\n",
    "similarity = compute_similarity(image_embedding, text_embedding)\n",
    "\n",
    "print(f\"Image embedding shape: {image_embedding.shape}\")\n",
    "print(f\"Text embedding shape: {text_embedding.shape}\")\n",
    "print(f\"Cosine similarity: {similarity.item():.4f}\")\n",
    "\n",
    "# Print the caption\n",
    "print(f\"\\nCaption: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b953a0-3086-47bd-917c-2f6e48c68ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.19"
=======
   "version": "3.12.4"
>>>>>>> cleaned_old
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
