{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4b89a-225d-47e5-a9be-050d8987de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "class InferencePipeline:\n",
    "    def __init__(self, model, processor, device):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "\n",
    "    def run_inference(self, dataset, task, max_samples=None):\n",
    "        if task == \"image_captioning\":\n",
    "            return self._run_image_captioning(dataset, max_samples)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task: {task}\")\n",
    "\n",
    "    def _run_image_captioning(self, dataset, max_samples):\n",
    "        results = []\n",
    "        references = []\n",
    "\n",
    "        for i in tqdm(range(min(len(dataset), max_samples or len(dataset)))):\n",
    "            image = dataset[i][0]\n",
    "            captions = dataset[i][1]\n",
    "            img_id = dataset.ids[i]\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(**inputs)\n",
    "\n",
    "            caption = self.processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            results.append({\"image_id\": img_id, \"caption\": caption})\n",
    "            references.append(captions)\n",
    "\n",
    "        return {\"predictions\": results, \"references\": references}\n",
    "\n",
    "    def save_results(self, results, filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cb72b-7922-4ba8-8e9c-2b7f2c29f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "\n",
    "\n",
    "class ScoringPipeline:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = PTBTokenizer()\n",
    "        self.caption_scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            (Meteor(), \"METEOR\"),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            (Cider(), \"CIDEr\"),\n",
    "            (Spice(), \"SPICE\"),\n",
    "        ]\n",
    "\n",
    "    def load_results(self, filename):\n",
    "        with open(filename, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def compute_scores(self, results, task):\n",
    "        if task == \"image_captioning\":\n",
    "            return self._compute_image_captioning_scores(results)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task: {task}\")\n",
    "\n",
    "    def _compute_image_captioning_scores(self, results):\n",
    "        gts = {i: [{\"caption\": c} for c in ref] for i, ref in enumerate(results[\"references\"])}\n",
    "        res = {i: [{\"caption\": p[\"caption\"]}] for i, p in enumerate(results[\"predictions\"])}\n",
    "\n",
    "        print(\"Tokenizing...\")\n",
    "        gts_tokenized = self.tokenizer.tokenize(gts)\n",
    "        res_tokenized = self.tokenizer.tokenize(res)\n",
    "\n",
    "        scores = {}\n",
    "        print(\"Computing scores...\")\n",
    "        for scorer, method in self.caption_scorers:\n",
    "            print(f\"Computing {method} score...\")\n",
    "            score, scores_per_caption = scorer.compute_score(gts_tokenized, res_tokenized)\n",
    "            if isinstance(method, list):\n",
    "                for sc, scs, m in zip(score, scores_per_caption, method):\n",
    "                    scores[m] = sc\n",
    "                    scores[f\"{m}_per_caption\"] = scs\n",
    "            else:\n",
    "                scores[method] = score\n",
    "                scores[f\"{method}_per_caption\"] = scores_per_caption\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5d018-0588-4dfb-a74b-67876fbda04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import print_model_structure\n",
    "\n",
    "# Set up the model, processor, and dataset as before\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bbfe1-82ca-4c10-8016-2544b6b3d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "inferencer = InferencePipeline(model, processor, device)\n",
    "print(\"Starting inference...\")\n",
    "results = inferencer.run_inference(coco_dataset, task=\"image_captioning\", max_samples=20)\n",
    "inferencer.save_results(results, \"./results/coco_quantized_inference.json\")\n",
    "\n",
    "# Compute scores\n",
    "scorer = ScoringPipeline()\n",
    "loaded_results = scorer.load_results(\"./results/coco_quantized_inference.json\")\n",
    "scores = scorer.compute_scores(loaded_results, task=\"image_captioning\")\n",
    "\n",
    "# Print scores\n",
    "for metric, score in scores.items():\n",
    "    if not metric.endswith(\"_per_caption\"):\n",
    "        print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3538b1d-0f75-4ad5-805b-e8e83eaaf862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
