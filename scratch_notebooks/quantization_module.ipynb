{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "1cde8c20-51b4-490d-8bf0-9e67f5277856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautom/anaconda3/envs/lavis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268547e914014eb6a5bea00fc16d41a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "1cde8c20-51b4-490d-8bf0-9e67f5277856",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Load COCO dataset\n",
<<<<<<< HEAD
    "coco_dataset = COCODataset(ann_file='./data/coco/annotations/captions_val2017.json',\n",
    "                           img_dir='./data/coco/val2017')"
=======
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")"
>>>>>>> cleaned_old
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": null,
>>>>>>> cleaned_old
   "id": "f20d5ffd-c6e7-4b63-9898-f7f5cd07575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Callable, Union\n",
    "from enum import Enum, auto\n",
    "from torch import nn, Tensor\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "class ModelPart(Enum):\n",
    "    VIT = auto()\n",
    "    QFORMER = auto()\n",
    "    LLM = auto()\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "class LayerGroup(Enum):\n",
    "    FIRST = auto()\n",
    "    MIDDLE = auto()\n",
    "    LAST = auto()\n",
    "    ALL = auto()\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "class LayerType(Enum):\n",
    "    MLP = auto()\n",
    "    ATTENTION = auto()\n",
    "    BOTH = auto()\n",
    "\n",
<<<<<<< HEAD
    "class QuantConfig:\n",
    "    def __init__(self, \n",
    "                 model_part: ModelPart,\n",
    "                 layer_group: LayerGroup,\n",
    "                 layer_type: LayerType,\n",
    "                 quant_function: Callable,\n",
    "                 num_bits: int):\n",
=======
    "\n",
    "class QuantConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_part: ModelPart,\n",
    "        layer_group: LayerGroup,\n",
    "        layer_type: LayerType,\n",
    "        quant_function: Callable,\n",
    "        num_bits: int,\n",
    "    ):\n",
>>>>>>> cleaned_old
    "        self.model_part = model_part\n",
    "        self.layer_group = layer_group\n",
    "        self.layer_type = layer_type\n",
    "        self.quant_function = quant_function\n",
    "        self.num_bits = num_bits\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "class BlipQuantizer:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.num_bits = 0\n",
    "\n",
    "    def apply_quantization(self, configs: List[QuantConfig]):\n",
    "        for config in configs:\n",
    "            self._quantize_part(config)\n",
    "\n",
    "    def _quantize_part(self, config: QuantConfig):\n",
    "        if config.model_part == ModelPart.VIT:\n",
    "            layers = self.model.vision_model.encoder.layers\n",
    "        elif config.model_part == ModelPart.QFORMER:\n",
    "            layers = self.model.qformer.encoder.layer\n",
    "        else:  # LLM\n",
    "            layers = self.model.language_model.model.decoder.layers\n",
    "\n",
    "        total_layers = len(layers)\n",
    "        start, end = self._get_layer_range(config.layer_group, total_layers)\n",
    "\n",
    "        self.num_bits = config.num_bits\n",
    "        print(f\"running {self.num_bits} quant\")\n",
    "        bit_quant_function = config.quant_function(config.num_bits)\n",
    "\n",
    "        for layer in layers[start:end]:\n",
    "            if config.layer_type in [LayerType.MLP, LayerType.BOTH]:\n",
    "                self._quantize_mlp(layer, bit_quant_function)\n",
    "            if config.layer_type in [LayerType.ATTENTION, LayerType.BOTH]:\n",
    "                self._quantize_attention(layer, bit_quant_function)\n",
    "\n",
    "    def _get_layer_range(self, group: LayerGroup, total_layers: int):\n",
    "        if group == LayerGroup.FIRST:\n",
    "            return 0, total_layers // 3\n",
    "        elif group == LayerGroup.LAST:\n",
    "            return 2 * total_layers // 3, total_layers\n",
    "        elif group == LayerGroup.MIDDLE:\n",
    "            return total_layers // 3, 2 * total_layers // 3\n",
    "        else:  # ALL\n",
    "            return 0, total_layers\n",
    "\n",
    "    def _quantize_mlp(self, layer: nn.Module, quant_function: Callable):\n",
<<<<<<< HEAD
    "        if hasattr(layer, 'mlp'):\n",
    "            self._quantize_linear(layer.mlp.fc1, quant_function)\n",
    "            self._quantize_linear(layer.mlp.fc2, quant_function)\n",
    "        elif hasattr(layer, 'fc1') and hasattr(layer, 'fc2'):\n",
=======
    "        if hasattr(layer, \"mlp\"):\n",
    "            self._quantize_linear(layer.mlp.fc1, quant_function)\n",
    "            self._quantize_linear(layer.mlp.fc2, quant_function)\n",
    "        elif hasattr(layer, \"fc1\") and hasattr(layer, \"fc2\"):\n",
>>>>>>> cleaned_old
    "            self._quantize_linear(layer.fc1, quant_function)\n",
    "            self._quantize_linear(layer.fc2, quant_function)\n",
    "\n",
    "    def _quantize_attention(self, layer: nn.Module, quant_function: Callable):\n",
<<<<<<< HEAD
    "        if hasattr(layer, 'self_attn'):\n",
    "            if hasattr(layer.self_attn, 'qkv'):\n",
    "                self._quantize_linear(layer.self_attn.qkv, quant_function)\n",
    "            if hasattr(layer.self_attn, 'projection'):\n",
    "                self._quantize_linear(layer.self_attn.projection, quant_function)\n",
    "        elif hasattr(layer, 'attention'):\n",
    "            if hasattr(layer.attention, 'attention'):\n",
    "                self._quantize_linear(layer.attention.attention.query, quant_function)\n",
    "                self._quantize_linear(layer.attention.attention.key, quant_function)\n",
    "                self._quantize_linear(layer.attention.attention.value, quant_function)\n",
    "            if hasattr(layer.attention, 'output'):\n",
    "                self._quantize_linear(layer.attention.output.dense, quant_function)\n",
    "        elif hasattr(layer, 'k_proj'):\n",
=======
    "        if hasattr(layer, \"self_attn\"):\n",
    "            if hasattr(layer.self_attn, \"qkv\"):\n",
    "                self._quantize_linear(layer.self_attn.qkv, quant_function)\n",
    "            if hasattr(layer.self_attn, \"projection\"):\n",
    "                self._quantize_linear(layer.self_attn.projection, quant_function)\n",
    "        elif hasattr(layer, \"attention\"):\n",
    "            if hasattr(layer.attention, \"attention\"):\n",
    "                self._quantize_linear(layer.attention.attention.query, quant_function)\n",
    "                self._quantize_linear(layer.attention.attention.key, quant_function)\n",
    "                self._quantize_linear(layer.attention.attention.value, quant_function)\n",
    "            if hasattr(layer.attention, \"output\"):\n",
    "                self._quantize_linear(layer.attention.output.dense, quant_function)\n",
    "        elif hasattr(layer, \"k_proj\"):\n",
>>>>>>> cleaned_old
    "            self._quantize_linear(layer.k_proj, quant_function)\n",
    "            self._quantize_linear(layer.v_proj, quant_function)\n",
    "            self._quantize_linear(layer.q_proj, quant_function)\n",
    "            self._quantize_linear(layer.out_proj, quant_function)\n",
    "\n",
    "    def _quantize_linear(self, module: nn.Module, quant_function: Callable):\n",
<<<<<<< HEAD
    "        if hasattr(module, 'weight') and isinstance(module.weight, torch.Tensor):\n",
    "            module.weight.data = quant_function(module.weight.data)\n",
    "            module.quantized = True  # Add this line\n",
    "            module.num_bits = self.num_bits  # Assuming num_bits is the first default argument\n",
    "        if hasattr(module, 'bias') and isinstance(module.bias, torch.Tensor):\n",
=======
    "        if hasattr(module, \"weight\") and isinstance(module.weight, torch.Tensor):\n",
    "            module.weight.data = quant_function(module.weight.data)\n",
    "            module.quantized = True  # Add this line\n",
    "            module.num_bits = self.num_bits  # Assuming num_bits is the first default argument\n",
    "        if hasattr(module, \"bias\") and isinstance(module.bias, torch.Tensor):\n",
>>>>>>> cleaned_old
    "            module.bias.data = quant_function(module.bias.data)\n",
    "\n",
    "    def count_quantized_layers(self):\n",
    "        count = 0\n",
    "        for name, module in self.model.named_modules():\n",
<<<<<<< HEAD
    "            if hasattr(module, 'quantized') and module.quantized:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "def print_model_structure(model, indent=0):\n",
    "    for name, module in model.named_children():\n",
    "        print('  ' * indent + name + ': ' + module.__class__.__name__, end='')\n",
    "        if hasattr(module, 'quantized'):\n",
    "            print(f\" (Quantized: {module.num_bits} bits)\", end='')\n",
=======
    "            if hasattr(module, \"quantized\") and module.quantized:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "\n",
    "def print_model_structure(model, indent=0):\n",
    "    for name, module in model.named_children():\n",
    "        print(\"  \" * indent + name + \": \" + module.__class__.__name__, end=\"\")\n",
    "        if hasattr(module, \"quantized\"):\n",
    "            print(f\" (Quantized: {module.num_bits} bits)\", end=\"\")\n",
>>>>>>> cleaned_old
    "        print()\n",
    "        if len(list(module.children())) > 0:\n",
    "            print_model_structure(module, indent + 1)\n",
    "\n",
<<<<<<< HEAD
    "def quant_function(x: Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "    \n",
    "    alpha = max_val - min_val\n",
    "    x = (x - min_val) / alpha\n",
    "    \n",
    "    scale = (2**num_bits - 1)\n",
    "    result = (scale * x).round()\n",
    "    result /= scale\n",
    "    \n",
    "    result = alpha * result + min_val\n",
    "    \n",
=======
    "\n",
    "def quant_function(x: Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    alpha = max_val - min_val\n",
    "    x = (x - min_val) / alpha\n",
    "\n",
    "    scale = 2**num_bits - 1\n",
    "    result = (scale * x).round()\n",
    "    result /= scale\n",
    "\n",
    "    result = alpha * result + min_val\n",
    "\n",
>>>>>>> cleaned_old
    "    return result"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
   "id": "88e14d63-f4a8-4f7d-8368-7a092c4671bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model...\n",
      "running 4 quant\n",
      "running 8 quant\n",
      "running 2 quant\n",
      "vision_model: Blip2VisionModel\n",
      "  embeddings: Blip2VisionEmbeddings\n",
      "    patch_embedding: Conv2d\n",
      "  encoder: Blip2Encoder\n",
      "    layers: ModuleList\n",
      "      0: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      1: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      2: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      3: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      4: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      5: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      6: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      7: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      8: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      9: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      10: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      11: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      12: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      13: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      14: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      15: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      16: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      17: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      18: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      19: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      20: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      21: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      22: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      23: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      24: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      25: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      26: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      27: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      28: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      29: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      30: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      31: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      32: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      33: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      34: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      35: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      36: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      37: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      38: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "  post_layernorm: LayerNorm\n",
      "qformer: Blip2QFormerModel\n",
      "  layernorm: LayerNorm\n",
      "  dropout: Dropout\n",
      "  encoder: Blip2QFormerEncoder\n",
      "    layer: ModuleList\n",
      "      0: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      1: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      2: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      3: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      4: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      5: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      6: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      7: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      8: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      9: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      10: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      11: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "language_projection: Linear\n",
      "language_model: OPTForCausalLM\n",
      "  model: OPTModel\n",
      "    decoder: OPTDecoder\n",
      "      embed_tokens: Embedding\n",
      "      embed_positions: OPTLearnedPositionalEmbedding\n",
      "      final_layer_norm: LayerNorm\n",
      "      layers: ModuleList\n",
      "        0: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        1: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        2: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        3: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        4: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        5: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        6: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        7: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        8: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        9: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        10: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        11: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        12: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        13: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        14: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        15: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        16: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        17: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        18: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        19: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        20: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        21: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        22: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        23: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        24: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        25: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        26: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        27: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        28: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        29: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        30: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        31: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "  lm_head: Linear\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "88e14d63-f4a8-4f7d-8368-7a092c4671bb",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "from quant_functions import uniform_quantization\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "\n",
    "configs = [\n",
<<<<<<< HEAD
    "    QuantConfig(ModelPart.VIT, LayerGroup.ALL, LayerType.BOTH, \n",
    "                uniform_quantization, num_bits=4),\n",
    "    QuantConfig(ModelPart.QFORMER, LayerGroup.ALL, LayerType.BOTH, \n",
    "                uniform_quantization, num_bits=8),\n",
    "    QuantConfig(ModelPart.LLM, LayerGroup.ALL, LayerType.BOTH, \n",
    "                uniform_quantization, num_bits=2)\n",
=======
    "    QuantConfig(ModelPart.VIT, LayerGroup.ALL, LayerType.BOTH, uniform_quantization, num_bits=4),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(ModelPart.LLM, LayerGroup.ALL, LayerType.BOTH, uniform_quantization, num_bits=2),\n",
>>>>>>> cleaned_old
    "]\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "quantizer.apply_quantization(configs)\n",
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e037-f443-4722-8bc4-4e2843301a73",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "\n",
    "configs = [\n",
    "    QuantConfig(ModelPart.VIT, LayerGroup.ALL, LayerType.BOTH, \n",
    "                lambda b: lambda x: quant_function(x, num_bits=b), num_bits=4),\n",
    "    QuantConfig(ModelPart.QFORMER, LayerGroup.ALL, LayerType.BOTH, \n",
    "                lambda b: lambda x: quant_function(x, num_bits=b), num_bits=8),\n",
    "    QuantConfig(ModelPart.LLM, LayerGroup.ALL, LayerType.BOTH, \n",
    "                lambda b: lambda x: quant_function(x, num_bits=b), num_bits=2)\n",
    "]\n",
    "\n",
    "quantizer.apply_quantization(configs)\n"
=======
    "quantizer = BlipQuantizer(model)\n",
    "\n",
    "configs = [\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "        num_bits=4,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.LLM,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "        num_bits=2,\n",
    "    ),\n",
    "]\n",
    "\n",
    "quantizer.apply_quantization(configs)"
>>>>>>> cleaned_old
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "id": "278e076e-90df-44be-bbda-b8a6c8beff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP-2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc5630808ac4675b2b2bb21b0cbff1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP-2 model loaded.\n",
      "Testing BlipQuantizer...\n",
      "Applying quantization...\n",
      "running 4 quant\n",
      "running 8 quant\n",
      "running 2 quant\n",
      "Quantization applied.\n",
      "Number of quantized layers: 268\n",
      "\n",
      "Sampling quantized layers:\n",
      "\n",
      "Checking vision_model.encoder.layers.0:\n",
      "  self_attn.qkv: Quantized to 4 bits\n",
      "  self_attn.projection: Quantized to 4 bits\n",
      "  mlp.fc1: Quantized to 4 bits\n",
      "  mlp.fc2: Quantized to 4 bits\n",
      "\n",
      "Checking qformer.encoder.layer.0:\n",
      "  attention.attention.query: Quantized to 8 bits\n",
      "  attention.attention.key: Quantized to 8 bits\n",
      "  attention.attention.value: Quantized to 8 bits\n",
      "  attention.output.dense: Quantized to 8 bits\n",
      "\n",
      "Checking language_model.model.decoder.layers.0:\n",
      "  fc1: Quantized to 2 bits\n",
      "  fc2: Quantized to 2 bits\n",
      "\n",
      "Verifying quantization effects:\n",
      "vision_model.encoder.layers.0.self_attn.qkv.weight: Mean absolute difference after quantization: 0.030251\n",
      "Quantized to 4 bits\n",
      "Sample values:\n",
      "Original: [-2.3543834686279297e-05, 4.231929779052734e-06, 0.00011557340621948242, 0.00534820556640625, -0.03228759765625]\n",
      "Quantized: [0.03138023614883423, 0.03138023614883423, 0.03138023614883423, 0.03138023614883423, -0.05286455154418945]\n",
      "\n",
      "qformer.encoder.layer.0.attention.attention.query.weight: Mean absolute difference after quantization: 0.000424\n",
      "Quantized to 8 bits\n",
      "Sample values:\n",
      "Original: [-0.01729758270084858, -0.01104929018765688, -0.02231333591043949, 0.006539066322147846, -0.012257079593837261]\n",
      "Quantized: [-0.01770883798599243, -0.010913178324699402, -0.022805586457252502, 0.006075963377952576, -0.01261208951473236]\n",
      "\n",
      "language_model.model.decoder.layers.0.self_attn.k_proj.weight: Not quantized\n",
      "Sample values:\n",
      "Original: [0.0093536376953125, 0.0020751953125, -0.006908416748046875, 0.002712249755859375, 0.0384521484375]\n",
      "Quantized: [0.0093536376953125, 0.0020751953125, -0.006908416748046875, 0.002712249755859375, 0.0384521484375]\n",
      "\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "278e076e-90df-44be-bbda-b8a6c8beff14",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "import torch\n",
    "from transformers import Blip2ForConditionalGeneration\n",
    "import gc\n",
    "\n",
<<<<<<< HEAD
=======
    "\n",
>>>>>>> cleaned_old
    "def test_blip_quantizer(model: nn.Module):\n",
    "    # Store original parameters\n",
    "    original_params = {}\n",
    "    sample_params = [\n",
<<<<<<< HEAD
    "        'vision_model.encoder.layers.0.self_attn.qkv.weight',\n",
    "        'qformer.encoder.layer.0.attention.attention.query.weight',\n",
    "        'language_model.model.decoder.layers.0.self_attn.k_proj.weight'\n",
    "    ]\n",
    "    for name in sample_params:\n",
    "        param = model\n",
    "        for attr in name.split('.'):\n",
=======
    "        \"vision_model.encoder.layers.0.self_attn.qkv.weight\",\n",
    "        \"qformer.encoder.layer.0.attention.attention.query.weight\",\n",
    "        \"language_model.model.decoder.layers.0.self_attn.k_proj.weight\",\n",
    "    ]\n",
    "    for name in sample_params:\n",
    "        param = model\n",
    "        for attr in name.split(\".\"):\n",
>>>>>>> cleaned_old
    "            param = getattr(param, attr)\n",
    "        original_params[name] = param.detach().clone()\n",
    "\n",
    "    quantizer = BlipQuantizer(model)\n",
    "    configs = [\n",
<<<<<<< HEAD
    "        QuantConfig(ModelPart.VIT, LayerGroup.ALL, LayerType.BOTH, \n",
    "                    lambda b: lambda x: quant_function(x, num_bits=b), num_bits=4),\n",
    "        QuantConfig(ModelPart.QFORMER, LayerGroup.ALL, LayerType.BOTH, \n",
    "                    lambda b: lambda x: quant_function(x, num_bits=b), num_bits=8),\n",
    "        QuantConfig(ModelPart.LLM, LayerGroup.ALL, LayerType.BOTH, \n",
    "                    lambda b: lambda x: quant_function(x, num_bits=b), num_bits=2)\n",
    "    ]\n",
    "    print(\"Testing BlipQuantizer...\")\n",
    "    \n",
=======
    "        QuantConfig(\n",
    "            ModelPart.VIT,\n",
    "            LayerGroup.ALL,\n",
    "            LayerType.BOTH,\n",
    "            lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "            num_bits=4,\n",
    "        ),\n",
    "        QuantConfig(\n",
    "            ModelPart.QFORMER,\n",
    "            LayerGroup.ALL,\n",
    "            LayerType.BOTH,\n",
    "            lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "            num_bits=8,\n",
    "        ),\n",
    "        QuantConfig(\n",
    "            ModelPart.LLM,\n",
    "            LayerGroup.ALL,\n",
    "            LayerType.BOTH,\n",
    "            lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "            num_bits=2,\n",
    "        ),\n",
    "    ]\n",
    "    print(\"Testing BlipQuantizer...\")\n",
    "\n",
>>>>>>> cleaned_old
    "    # Test 1: Apply quantization\n",
    "    print(\"Applying quantization...\")\n",
    "    quantizer.apply_quantization(configs)\n",
    "    print(\"Quantization applied.\")\n",
    "\n",
    "    # Test 2: Count quantized layers\n",
    "    quantized_layers = quantizer.count_quantized_layers()\n",
    "    print(f\"Number of quantized layers: {quantized_layers}\")\n",
    "\n",
    "    # Test 3: Check quantization bits for a sample of layers\n",
    "    print(\"\\nSampling quantized layers:\")\n",
    "    sampled_modules = [\n",
<<<<<<< HEAD
    "        ('vision_model.encoder.layers.0', model.vision_model.encoder.layers[0]),\n",
    "        ('qformer.encoder.layer.0', model.qformer.encoder.layer[0]),\n",
    "        ('language_model.model.decoder.layers.0', model.language_model.model.decoder.layers[0])\n",
    "    ]\n",
    "    \n",
    "    for name, module in sampled_modules:\n",
    "        print(f\"\\nChecking {name}:\")\n",
    "        for sub_name, sub_module in module.named_modules():\n",
    "            if hasattr(sub_module, 'quantized'):\n",
=======
    "        (\"vision_model.encoder.layers.0\", model.vision_model.encoder.layers[0]),\n",
    "        (\"qformer.encoder.layer.0\", model.qformer.encoder.layer[0]),\n",
    "        (\n",
    "            \"language_model.model.decoder.layers.0\",\n",
    "            model.language_model.model.decoder.layers[0],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for name, module in sampled_modules:\n",
    "        print(f\"\\nChecking {name}:\")\n",
    "        for sub_name, sub_module in module.named_modules():\n",
    "            if hasattr(sub_module, \"quantized\"):\n",
>>>>>>> cleaned_old
    "                print(f\"  {sub_name}: Quantized to {sub_module.num_bits} bits\")\n",
    "\n",
    "    # Test 4: Verify quantization effects for a sample parameter\n",
    "    print(\"\\nVerifying quantization effects:\")\n",
    "    sample_params = [\n",
<<<<<<< HEAD
    "        ('vision_model.encoder.layers.0.self_attn.qkv', 'weight'),\n",
    "        ('qformer.encoder.layer.0.attention.attention.query', 'weight'),\n",
    "        ('language_model.model.decoder.layers.0.self_attn.k_proj', 'weight')\n",
    "    ]\n",
    "    \n",
    "    for module_name, param_name in sample_params:\n",
    "        module = model\n",
    "        for attr in module_name.split('.'):\n",
    "            module = getattr(module, attr)\n",
    "        \n",
    "        param = getattr(module, param_name)\n",
    "        original_param = original_params[f\"{module_name}.{param_name}\"]\n",
    "        \n",
    "        if hasattr(module, 'quantized'):\n",
=======
    "        (\"vision_model.encoder.layers.0.self_attn.qkv\", \"weight\"),\n",
    "        (\"qformer.encoder.layer.0.attention.attention.query\", \"weight\"),\n",
    "        (\"language_model.model.decoder.layers.0.self_attn.k_proj\", \"weight\"),\n",
    "    ]\n",
    "\n",
    "    for module_name, param_name in sample_params:\n",
    "        module = model\n",
    "        for attr in module_name.split(\".\"):\n",
    "            module = getattr(module, attr)\n",
    "\n",
    "        param = getattr(module, param_name)\n",
    "        original_param = original_params[f\"{module_name}.{param_name}\"]\n",
    "\n",
    "        if hasattr(module, \"quantized\"):\n",
>>>>>>> cleaned_old
    "            diff = torch.abs(param - original_param).mean().item()\n",
    "            print(f\"{module_name}.{param_name}: Mean absolute difference after quantization: {diff:.6f}\")\n",
    "            print(f\"Quantized to {module.num_bits} bits\")\n",
    "        else:\n",
    "            print(f\"{module_name}.{param_name}: Not quantized\")\n",
<<<<<<< HEAD
    "    \n",
=======
    "\n",
>>>>>>> cleaned_old
    "        # Print a small sample of the parameter values before and after quantization\n",
    "        print(\"Sample values:\")\n",
    "        print(\"Original:\", original_param.flatten()[:5].tolist())\n",
    "        print(\"Quantized:\", param.flatten()[:5].tolist())\n",
    "        print()\n",
    "\n",
    "    # Clean up\n",
    "    del model, quantizer, original_params\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"Loading BLIP-2 model...\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "print(\"BLIP-2 model loaded.\")\n",
    "\n",
    "# Run the tests\n",
    "test_blip_quantizer(model)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "id": "0e0b3918-48d9-408f-ad36-afefa402db87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model: Blip2VisionModel\n",
      "  embeddings: Blip2VisionEmbeddings\n",
      "    patch_embedding: Conv2d\n",
      "  encoder: Blip2Encoder\n",
      "    layers: ModuleList\n",
      "      0: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      1: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      2: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      3: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      4: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      5: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      6: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      7: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      8: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      9: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      10: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      11: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      12: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      13: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      14: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      15: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      16: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      17: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      18: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      19: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      20: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      21: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      22: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      23: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      24: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      25: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      26: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      27: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      28: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      29: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      30: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      31: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      32: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      33: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      34: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      35: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      36: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      37: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "      38: Blip2EncoderLayer\n",
      "        self_attn: Blip2Attention\n",
      "          dropout: Dropout\n",
      "          qkv: Linear (Quantized: 4 bits)\n",
      "          projection: Linear (Quantized: 4 bits)\n",
      "        layer_norm1: LayerNorm\n",
      "        mlp: Blip2MLP\n",
      "          activation_fn: GELUActivation\n",
      "          fc1: Linear (Quantized: 4 bits)\n",
      "          fc2: Linear (Quantized: 4 bits)\n",
      "        layer_norm2: LayerNorm\n",
      "  post_layernorm: LayerNorm\n",
      "qformer: Blip2QFormerModel\n",
      "  layernorm: LayerNorm\n",
      "  dropout: Dropout\n",
      "  encoder: Blip2QFormerEncoder\n",
      "    layer: ModuleList\n",
      "      0: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      1: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      2: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      3: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      4: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      5: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      6: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      7: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      8: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      9: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      10: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        crossattention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear\n",
      "            key: Linear\n",
      "            value: Linear\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "      11: Blip2QFormerLayer\n",
      "        attention: Blip2QFormerAttention\n",
      "          attention: Blip2QFormerMultiHeadAttention\n",
      "            query: Linear (Quantized: 8 bits)\n",
      "            key: Linear (Quantized: 8 bits)\n",
      "            value: Linear (Quantized: 8 bits)\n",
      "            dropout: Dropout\n",
      "          output: Blip2QFormerSelfOutput\n",
      "            dense: Linear (Quantized: 8 bits)\n",
      "            LayerNorm: LayerNorm\n",
      "            dropout: Dropout\n",
      "        intermediate_query: Blip2QFormerIntermediate\n",
      "          dense: Linear\n",
      "          intermediate_act_fn: GELUActivation\n",
      "        output_query: Blip2QFormerOutput\n",
      "          dense: Linear\n",
      "          LayerNorm: LayerNorm\n",
      "          dropout: Dropout\n",
      "language_projection: Linear\n",
      "language_model: OPTForCausalLM\n",
      "  model: OPTModel\n",
      "    decoder: OPTDecoder\n",
      "      embed_tokens: Embedding\n",
      "      embed_positions: OPTLearnedPositionalEmbedding\n",
      "      final_layer_norm: LayerNorm\n",
      "      layers: ModuleList\n",
      "        0: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        1: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        2: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        3: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        4: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        5: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        6: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        7: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        8: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        9: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        10: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        11: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        12: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        13: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        14: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        15: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        16: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        17: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        18: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        19: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        20: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        21: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        22: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        23: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        24: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        25: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        26: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        27: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        28: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        29: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        30: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "        31: OPTDecoderLayer\n",
      "          self_attn: OPTAttention\n",
      "            k_proj: Linear\n",
      "            v_proj: Linear\n",
      "            q_proj: Linear\n",
      "            out_proj: Linear\n",
      "          activation_fn: ReLU\n",
      "          self_attn_layer_norm: LayerNorm\n",
      "          fc1: Linear (Quantized: 2 bits)\n",
      "          fc2: Linear (Quantized: 2 bits)\n",
      "          final_layer_norm: LayerNorm\n",
      "  lm_head: Linear\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "0e0b3918-48d9-408f-ad36-afefa402db87",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "id": "508deaa7-6ff9-48c4-9800-2f67b64ab547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blip2ForConditionalGeneration(\n",
       "  (vision_model): Blip2VisionModel(\n",
       "    (embeddings): Blip2VisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
       "    )\n",
       "    (encoder): Blip2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-38): 39 x Blip2EncoderLayer(\n",
       "          (self_attn): Blip2Attention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
       "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Blip2MLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
       "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (qformer): Blip2QFormerModel(\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (encoder): Blip2QFormerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Blip2QFormerLayer(\n",
       "          (attention): Blip2QFormerAttention(\n",
       "            (attention): Blip2QFormerMultiHeadAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): Blip2QFormerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate_query): Blip2QFormerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output_query): Blip2QFormerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n",
       "  (language_model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 2560, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
       "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "id": "508deaa7-6ff9-48c4-9800-2f67b64ab547",
   "metadata": {},
   "outputs": [],
>>>>>>> cleaned_old
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2203a-2b7b-465e-9a45-aa4c61057851",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.19"
=======
   "version": "3.12.4"
>>>>>>> cleaned_old
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
