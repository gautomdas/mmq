{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde8c20-51b4-490d-8bf0-9e67f5277856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d5ffd-c6e7-4b63-9898-f7f5cd07575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Callable, Union\n",
    "from enum import Enum, auto\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class ModelPart(Enum):\n",
    "    VIT = auto()\n",
    "    QFORMER = auto()\n",
    "    LLM = auto()\n",
    "\n",
    "\n",
    "class LayerGroup(Enum):\n",
    "    FIRST = auto()\n",
    "    MIDDLE = auto()\n",
    "    LAST = auto()\n",
    "    ALL = auto()\n",
    "\n",
    "\n",
    "class LayerType(Enum):\n",
    "    MLP = auto()\n",
    "    ATTENTION = auto()\n",
    "    BOTH = auto()\n",
    "\n",
    "class QuantConfig:\n",
    "    def __init__(self, \n",
    "        self,\n",
    "        model_part: ModelPart,\n",
    "        layer_group: LayerGroup,\n",
    "        layer_type: LayerType,\n",
    "        quant_function: Callable,\n",
    "        num_bits: int,\n",
    "    ):\n",
    "        self.model_part = model_part\n",
    "        self.layer_group = layer_group\n",
    "        self.quant_function = quant_function\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "\n",
    "class BlipQuantizer:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.num_bits = 0\n",
    "\n",
    "    def apply_quantization(self, configs: List[QuantConfig]):\n",
    "        for config in configs:\n",
    "            self._quantize_part(config)\n",
    "\n",
    "    def _quantize_part(self, config: QuantConfig):\n",
    "        if config.model_part == ModelPart.VIT:\n",
    "            layers = self.model.vision_model.encoder.layers\n",
    "        elif config.model_part == ModelPart.QFORMER:\n",
    "            layers = self.model.qformer.encoder.layer\n",
    "        else:  # LLM\n",
    "            layers = self.model.language_model.model.decoder.layers\n",
    "\n",
    "        total_layers = len(layers)\n",
    "        start, end = self._get_layer_range(config.layer_group, total_layers)\n",
    "\n",
    "        self.num_bits = config.num_bits\n",
    "        print(f\"running {self.num_bits} quant\")\n",
    "        bit_quant_function = config.quant_function(config.num_bits)\n",
    "\n",
    "        for layer in layers[start:end]:\n",
    "            if config.layer_type in [LayerType.MLP, LayerType.BOTH]:\n",
    "                self._quantize_mlp(layer, bit_quant_function)\n",
    "            if config.layer_type in [LayerType.ATTENTION, LayerType.BOTH]:\n",
    "                self._quantize_attention(layer, bit_quant_function)\n",
    "\n",
    "    def _get_layer_range(self, group: LayerGroup, total_layers: int):\n",
    "        if group == LayerGroup.FIRST:\n",
    "            return 0, total_layers // 3\n",
    "        elif group == LayerGroup.LAST:\n",
    "            return 2 * total_layers // 3, total_layers\n",
    "        elif group == LayerGroup.MIDDLE:\n",
    "            return total_layers // 3, 2 * total_layers // 3\n",
    "        else:  # ALL\n",
    "            return 0, total_layers\n",
    "\n",
    "    def _quantize_mlp(self, layer: nn.Module, quant_function: Callable):\n",
    "        if hasattr(layer, \"mlp\"):\n",
    "            self._quantize_linear(layer.mlp.fc1, quant_function)\n",
    "            self._quantize_linear(layer.mlp.fc2, quant_function)\n",
    "        elif hasattr(layer, \"fc1\") and hasattr(layer, \"fc2\"):\n",
    "            self._quantize_linear(layer.fc1, quant_function)\n",
    "            self._quantize_linear(layer.fc2, quant_function)\n",
    "\n",
    "    def _quantize_attention(self, layer: nn.Module, quant_function: Callable):\n",
    "        if hasattr(layer, \"self_attn\"):\n",
    "            if hasattr(layer.self_attn, \"qkv\"):\n",
    "                self._quantize_linear(layer.self_attn.qkv, quant_function)\n",
    "            if hasattr(layer.self_attn, \"projection\"):\n",
    "                self._quantize_linear(layer.self_attn.projection, quant_function)\n",
    "        elif hasattr(layer, \"attention\"):\n",
    "            if hasattr(layer.attention, \"attention\"):\n",
    "                self._quantize_linear(layer.attention.attention.query, quant_function)\n",
    "                self._quantize_linear(layer.attention.attention.key, quant_function)\n",
    "                self._quantize_linear(layer.attention.attention.value, quant_function)\n",
    "            if hasattr(layer.attention, \"output\"):\n",
    "                self._quantize_linear(layer.attention.output.dense, quant_function)\n",
    "        elif hasattr(layer, \"k_proj\"):\n",
    "            self._quantize_linear(layer.k_proj, quant_function)\n",
    "            self._quantize_linear(layer.v_proj, quant_function)\n",
    "            self._quantize_linear(layer.q_proj, quant_function)\n",
    "            self._quantize_linear(layer.out_proj, quant_function)\n",
    "\n",
    "    def _quantize_linear(self, module: nn.Module, quant_function: Callable):\n",
    "        if hasattr(module, \"weight\") and isinstance(module.weight, torch.Tensor):\n",
    "            module.weight.data = quant_function(module.weight.data)\n",
    "            module.quantized = True  # Add this line\n",
    "            module.num_bits = self.num_bits  # Assuming num_bits is the first default argument\n",
    "        if hasattr(module, \"bias\") and isinstance(module.bias, torch.Tensor):\n",
    "            module.bias.data = quant_function(module.bias.data)\n",
    "\n",
    "    def count_quantized_layers(self):\n",
    "        count = 0\n",
    "        for name, module in self.model.named_modules():\n",
    "            if hasattr(module, \"quantized\") and module.quantized:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "\n",
    "def print_model_structure(model, indent=0):\n",
    "    for name, module in model.named_children():\n",
    "        print(\"  \" * indent + name + \": \" + module.__class__.__name__, end=\"\")\n",
    "        if hasattr(module, \"quantized\"):\n",
    "            print(f\" (Quantized: {module.num_bits} bits)\", end=\"\")\n",
    "        print()\n",
    "        if len(list(module.children())) > 0:\n",
    "            print_model_structure(module, indent + 1)\n",
    "\n",
    "\n",
    "def quant_function(x: Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    alpha = max_val - min_val\n",
    "    x = (x - min_val) / alpha\n",
    "\n",
    "    scale = 2**num_bits - 1\n",
    "    result = (scale * x).round()\n",
    "    result /= scale\n",
    "\n",
    "    result = alpha * result + min_val\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e14d63-f4a8-4f7d-8368-7a092c4671bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quant_functions import uniform_quantization\n",
    "\n",
    "quantizer = BlipQuantizer(model)\n",
    "\n",
    "configs = [\n",
    "    QuantConfig(ModelPart.VIT, LayerGroup.ALL, LayerType.BOTH, uniform_quantization, num_bits=4),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        uniform_quantization,\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(ModelPart.LLM, LayerGroup.ALL, LayerType.BOTH, uniform_quantization, num_bits=2),\n",
    "]\n",
    "\n",
    "print(\"Quantizing model...\")\n",
    "quantizer.apply_quantization(configs)\n",
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e037-f443-4722-8bc4-4e2843301a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = BlipQuantizer(model)\n",
    "\n",
    "configs = [\n",
    "    QuantConfig(\n",
    "        ModelPart.VIT,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "        num_bits=4,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.QFORMER,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "        num_bits=8,\n",
    "    ),\n",
    "    QuantConfig(\n",
    "        ModelPart.LLM,\n",
    "        LayerGroup.ALL,\n",
    "        LayerType.BOTH,\n",
    "        lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "        num_bits=2,\n",
    "    ),\n",
    "]\n",
    "\n",
    "quantizer.apply_quantization(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e076e-90df-44be-bbda-b8a6c8beff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2ForConditionalGeneration\n",
    "import gc\n",
    "\n",
    "\n",
    "def test_blip_quantizer(model: nn.Module):\n",
    "    # Store original parameters\n",
    "    original_params = {}\n",
    "    sample_params = [\n",
    "        \"vision_model.encoder.layers.0.self_attn.qkv.weight\",\n",
    "        \"qformer.encoder.layer.0.attention.attention.query.weight\",\n",
    "        \"language_model.model.decoder.layers.0.self_attn.k_proj.weight\",\n",
    "    ]\n",
    "    for name in sample_params:\n",
    "        param = model\n",
    "        for attr in name.split(\".\"):\n",
    "            param = getattr(param, attr)\n",
    "        original_params[name] = param.detach().clone()\n",
    "\n",
    "    quantizer = BlipQuantizer(model)\n",
    "    configs = [\n",
    "        QuantConfig(\n",
    "            ModelPart.VIT,\n",
    "            LayerGroup.ALL,\n",
    "            LayerType.BOTH,\n",
    "            lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "            num_bits=4,\n",
    "        ),\n",
    "        QuantConfig(\n",
    "            ModelPart.QFORMER,\n",
    "            LayerGroup.ALL,\n",
    "            LayerType.BOTH,\n",
    "            lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "            num_bits=8,\n",
    "        ),\n",
    "        QuantConfig(\n",
    "            ModelPart.LLM,\n",
    "            LayerGroup.ALL,\n",
    "            LayerType.BOTH,\n",
    "            lambda b: lambda x: quant_function(x, num_bits=b),\n",
    "            num_bits=2,\n",
    "        ),\n",
    "    ]\n",
    "    print(\"Testing BlipQuantizer...\")\n",
    "\n",
    "    # Test 1: Apply quantization\n",
    "    print(\"Applying quantization...\")\n",
    "    quantizer.apply_quantization(configs)\n",
    "    print(\"Quantization applied.\")\n",
    "\n",
    "    # Test 2: Count quantized layers\n",
    "    quantized_layers = quantizer.count_quantized_layers()\n",
    "    print(f\"Number of quantized layers: {quantized_layers}\")\n",
    "\n",
    "    # Test 3: Check quantization bits for a sample of layers\n",
    "    print(\"\\nSampling quantized layers:\")\n",
    "    sampled_modules = [\n",
    "        (\"vision_model.encoder.layers.0\", model.vision_model.encoder.layers[0]),\n",
    "        (\"qformer.encoder.layer.0\", model.qformer.encoder.layer[0]),\n",
    "        (\n",
    "            \"language_model.model.decoder.layers.0\",\n",
    "            model.language_model.model.decoder.layers[0],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for name, module in sampled_modules:\n",
    "        print(f\"\\nChecking {name}:\")\n",
    "        for sub_name, sub_module in module.named_modules():\n",
    "            if hasattr(sub_module, \"quantized\"):\n",
    "                print(f\"  {sub_name}: Quantized to {sub_module.num_bits} bits\")\n",
    "\n",
    "    # Test 4: Verify quantization effects for a sample parameter\n",
    "    print(\"\\nVerifying quantization effects:\")\n",
    "    sample_params = [\n",
    "        (\"vision_model.encoder.layers.0.self_attn.qkv\", \"weight\"),\n",
    "        (\"qformer.encoder.layer.0.attention.attention.query\", \"weight\"),\n",
    "        (\"language_model.model.decoder.layers.0.self_attn.k_proj\", \"weight\"),\n",
    "    ]\n",
    "\n",
    "    for module_name, param_name in sample_params:\n",
    "        module = model\n",
    "        for attr in module_name.split(\".\"):\n",
    "            module = getattr(module, attr)\n",
    "\n",
    "        param = getattr(module, param_name)\n",
    "        original_param = original_params[f\"{module_name}.{param_name}\"]\n",
    "\n",
    "        if hasattr(module, \"quantized\"):\n",
    "            diff = torch.abs(param - original_param).mean().item()\n",
    "            print(f\"{module_name}.{param_name}: Mean absolute difference after quantization: {diff:.6f}\")\n",
    "            print(f\"Quantized to {module.num_bits} bits\")\n",
    "        else:\n",
    "            print(f\"{module_name}.{param_name}: Not quantized\")\n",
    "\n",
    "        # Print a small sample of the parameter values before and after quantization\n",
    "        print(\"Sample values:\")\n",
    "        print(\"Original:\", original_param.flatten()[:5].tolist())\n",
    "        print(\"Quantized:\", param.flatten()[:5].tolist())\n",
    "        print()\n",
    "\n",
    "    # Clean up\n",
    "    del model, quantizer, original_params\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"Loading BLIP-2 model...\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "print(\"BLIP-2 model loaded.\")\n",
    "\n",
    "# Run the tests\n",
    "test_blip_quantizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b3918-48d9-408f-ad36-afefa402db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508deaa7-6ff9-48c4-9800-2f67b64ab547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2203a-2b7b-465e-9a45-aa4c61057851",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
