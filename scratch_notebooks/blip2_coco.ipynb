{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2252dab7-667f-454b-bba1-6a75f57aec63",
   "metadata": {},
   "source": [
    "# Blip2 COCO Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f543991-f107-4ae1-9988-ec55ff36af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36001a9a-7ab2-4978-bd25-5bfacb1b7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57227e-4ed7-4776-821e-b3222b46dd68",
   "metadata": {},
   "source": [
    "## Collect Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe866692-f447-431f-8e3d-2a37df47f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def eval_model(qmodel, results_file=\"./results/inference.json\"):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, min(1000, len(coco_dataset)))):\n",
    "        image, _ = coco_dataset[i]\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = qmodel.generate(**inputs)\n",
    "\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        image_id = coco_dataset.ids[i]\n",
    "        results.append({\"image_id\": image_id, \"caption\": caption})\n",
    "\n",
    "    with open(\"./results/coco_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1756a-0c5f-482b-a82d-eee5584e1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, min(1000, len(coco_dataset)))):\n",
    "    image, _ = coco_dataset[i]\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    image_id = coco_dataset.ids[i]\n",
    "    results.append({\"image_id\": image_id, \"caption\": caption})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0e1e8-b182-4e4c-a0ab-b4f9b3d462f3",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9365a6f-22fb-4373-927c-cb13859dabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./results/coco_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff17880-b275-413c-af04-e4cac6f4ef9c",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a309c82-1bdc-4529-ab93-d5b50fc51bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pycocoevalfolder\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "# Verify the path\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d523f9-0926-42e0-b15a-3d95a43b9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "\n",
    "class SimpleCIDErEval:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = PTBTokenizer()\n",
    "        self.cider_scorer = Cider()\n",
    "\n",
    "    def evaluate(self, predictions, references):\n",
    "        # Format the input for the tokenizer\n",
    "        gts = {i: [{\"caption\": c} for c in refs] for i, refs in enumerate(references)}\n",
    "        res = {i: [{\"caption\": p}] for i, p in enumerate(predictions)}\n",
    "\n",
    "        # Tokenize\n",
    "        gts_tokenized = self.tokenizer.tokenize(gts)\n",
    "        res_tokenized = self.tokenizer.tokenize(res)\n",
    "\n",
    "        # Compute CIDEr score\n",
    "        score, scores = self.cider_scorer.compute_score(gts_tokenized, res_tokenized)\n",
    "\n",
    "        return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a93018-94ff-4b9b-9eba-b3dc01d9635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")\n",
    "\n",
    "f = open(\"./results/coco_results.json\")\n",
    "results = json.load(f)\n",
    "f.close()\n",
    "\n",
    "candidates = [result[\"caption\"] for result in results]\n",
    "references = [coco_dataset.get_captions(result[\"image_id\"]) for result in results]\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = SimpleCIDErEval()\n",
    "\n",
    "overall_score, individual_scores = evaluator.evaluate(candidates, references)\n",
    "\n",
    "print(f\"Overall CIDEr score: {overall_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4533abd-f500-41be-b054-ca93072fc93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cidereval import cider\n",
    "\n",
    "cider_scores = cider(candidates, references)\n",
    "\n",
    "print(f\"Average CIDEr score: {cider_scores['avg_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f74bbf-5aac-4172-9b22-47ec603f94c3",
   "metadata": {},
   "source": [
    "## Replace and test replacement for BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e304ed3-e8d7-4784-94bd-19a1da82c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union, Tuple\n",
    "\n",
    "\n",
    "def replace_linear_with_quantized(model: nn.Module, weight_bits: int = 8, activation_bits: int = 8) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replaces nn.Linear layers in a PyTorch model with NBitLinearDynamic layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to modify.\n",
    "        weight_bits (int): Number of bits for weight quantization. Default is 8.\n",
    "        activation_bits (int): Number of bits for activation quantization. Default is 8.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The modified model with quantized linear layers.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            quantized_linear = NBitLinearDynamic(\n",
    "                in_features=module.in_features,\n",
    "                out_features=module.out_features,\n",
    "                bias=module.bias is not None,\n",
    "                weight_bits=weight_bits,\n",
    "                activation_bits=activation_bits,\n",
    "            )\n",
    "\n",
    "            # Copy the weights and bias\n",
    "            quantized_linear.weight.data = module.weight.data\n",
    "            if module.bias is not None:\n",
    "                quantized_linear.bias.data = module.bias.data\n",
    "\n",
    "            # Replace the original linear layer with the quantized version\n",
    "            setattr(model, name, quantized_linear)\n",
    "        elif isinstance(module, nn.Module):\n",
    "            # Recursively apply to child modules\n",
    "            setattr(\n",
    "                model,\n",
    "                name,\n",
    "                replace_linear_with_quantized(module, weight_bits, activation_bits),\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7817eb0-e1d7-4b7c-8b73-c681fb9f3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a PyTorch model called 'my_model'\n",
    "quantized_model = replace_linear_with_quantized(model, weight_bits=16, activation_bits=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629218a-ea53-4687-93b0-b3a846273c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22289e5e-05d3-4b5c-8002-bce31ec61758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Applies N-bit Uniform min-max quantization to both activations and weights for dynamic PT\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Observers compute quantization parameters (scaling factor and zero-point)\n",
    "# TODO: experiment with different observers\n",
    "# from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver\n",
    "\n",
    "\n",
    "# adapted from: https://pocketflow.github.io/uq_learner/#algorithm\n",
    "def quant(x: Tensor, num_bits):\n",
    "    # per-sample min/max\n",
    "    # NOTE: granularity could be adjusted\n",
    "    min_val = x.min(dim=-1).values.unsqueeze(-1)\n",
    "    max_val = x.max(dim=-1).values.unsqueeze(-1)\n",
    "\n",
    "    alpha = max_val - min_val\n",
    "\n",
    "    # normalize to [0,1]\n",
    "    x = (x - min_val) / alpha\n",
    "\n",
    "    scale = 2**num_bits - 1\n",
    "\n",
    "    # quantize [0,1] --> [-2^B-1, 2^B-1]\n",
    "    result = (scale * x).round()\n",
    "\n",
    "    # dequantize [-2^B-1, 2^B-1] --> [0,1]\n",
    "    result /= scale\n",
    "\n",
    "    # back to original scale\n",
    "    result = alpha * result + min_val\n",
    "\n",
    "    return result\n",
    "\n",
    "    # # pass input to observer for metric computing\n",
    "    # obs(x)\n",
    "\n",
    "    # # computed quantization parameters\n",
    "    # s,z = obs.calculate_qparams()\n",
    "\n",
    "    # # quantize\n",
    "    # result = ((x / s) + z).round()\n",
    "\n",
    "    # # --> dequantize\n",
    "    # result = (result - z) * s\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class NBitLinearDynamic(nn.Linear):\n",
    "    \"\"\"\n",
    "    Custom linear layer with N-bit uniform quantization.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the layer.\n",
    "        training (bool, optional): Whether the layer is in training mode or not. Defaults to False.\n",
    "        *args: Variable length argument list.\n",
    "        **kwargs: Arbitrary keyword arguments.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): The input dimension of the layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *kargs, weight_bits=8, activation_bits=8, **kwargs):\n",
    "        # super(NBitLinearDynamic, self).__init__(*kargs, **kwargs)\n",
    "        super().__init__(*kargs, **kwargs)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.activation_bits = activation_bits\n",
    "\n",
    "        # TODO: mess with observer modules instead of computing min,max per sample\n",
    "        # Q_low = -2 ** (self.weight_bits - 1)\n",
    "        # Q_high = 2 ** (self.weight_bits - 1) - 1\n",
    "        # self.weight_observer = MinMaxObserver(quant_min=Q_low, quant_max=Q_high)\n",
    "\n",
    "        # Q_low = -2 ** (self.activation_bits - 1)\n",
    "        # Q_high = 2 ** (self.activation_bits - 1) - 1\n",
    "        # self.activation_observer = MovingAverageMinMaxObserver(quant_min=Q_low, quant_max=Q_high, is_dynamic=True, averaging_constant=1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the NBitLinear layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        w = self.weight\n",
    "        b = self.bias\n",
    "\n",
    "        # STE (Straight-through estimator) trick using detach, not really necessary for just PTQ inference\n",
    "        x_quant = x + (quant(x, self.activation_bits) - x).detach()\n",
    "        w_quant = w + (quant(w, self.weight_bits) - w).detach()\n",
    "\n",
    "        # quantize bias term if present\n",
    "        if b != None:\n",
    "            b = b + (quant(b, self.weight_bits) - b).detach()\n",
    "\n",
    "        y = F.linear(x_quant, w_quant, bias=b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    # print out bitwidth info!\n",
    "    def extra_repr(self) -> str:\n",
    "        return super().extra_repr() + f\" | w={self.weight_bits}, a={self.activation_bits}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81df77-8aad-43ac-bd98-78ab9a38ba01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
