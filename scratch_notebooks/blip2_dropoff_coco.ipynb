{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2252dab7-667f-454b-bba1-6a75f57aec63",
   "metadata": {},
   "source": [
    "# Blip2 COCO Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f543991-f107-4ae1-9988-ec55ff36af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36001a9a-7ab2-4978-bd25-5bfacb1b7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57227e-4ed7-4776-821e-b3222b46dd68",
   "metadata": {},
   "source": [
    "## Collect Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe866692-f447-431f-8e3d-2a37df47f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def eval_model(qmodel, results_file=\"./results/inference.json\", eval_size=50):\n",
    "    if results_file == \"./results/inference.json\":\n",
    "        print('No results file provided, using default \"./results/inference.json\"')\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, min(eval_size, len(coco_dataset)))):\n",
    "        image, _ = coco_dataset[i]\n",
    "\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = qmodel.generate(**inputs)\n",
    "\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        image_id = coco_dataset.ids[i]\n",
    "        results.append({\"image_id\": image_id, \"caption\": caption})\n",
    "\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aea1df-e482-4089-bdac-4e1a029279c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "\n",
    "class SimpleCIDErEval:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = PTBTokenizer()\n",
    "        self.cider_scorer = Cider()\n",
    "\n",
    "    def evaluate(self, predictions, references):\n",
    "        # Format the input for the tokenizer\n",
    "        gts = {i: [{\"caption\": c} for c in refs] for i, refs in enumerate(references)}\n",
    "        res = {i: [{\"caption\": p}] for i, p in enumerate(predictions)}\n",
    "\n",
    "        # Tokenize\n",
    "        gts_tokenized = self.tokenizer.tokenize(gts)\n",
    "        res_tokenized = self.tokenizer.tokenize(res)\n",
    "\n",
    "        # Compute CIDEr score\n",
    "        score, scores = self.cider_scorer.compute_score(gts_tokenized, res_tokenized)\n",
    "\n",
    "        return score, scores\n",
    "\n",
    "\n",
    "def eval_results(results_file):\n",
    "    f = open(results_file)\n",
    "    results = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    candidates = [result[\"caption\"] for result in results]\n",
    "    references = [coco_dataset.get_captions(result[\"image_id\"]) for result in results]\n",
    "\n",
    "    evaluator = SimpleCIDErEval()\n",
    "\n",
    "    overall_score, individual_scores = evaluator.evaluate(candidates, references)\n",
    "\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d0dea-3f33-467d-94ea-94297d3e2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb5fbd-127b-4e0b-967c-108539450f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "def quant(x: torch.Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    if min_val == max_val:\n",
    "        return x\n",
    "\n",
    "    scale = (2**num_bits - 1) / (max_val - min_val)\n",
    "    zero_point = (-min_val * scale).round()\n",
    "\n",
    "    x_int = (x * scale + zero_point).round().clamp(0, 2**num_bits - 1)\n",
    "    x_quant = (x_int - zero_point) / scale\n",
    "\n",
    "    return x_quant\n",
    "\n",
    "\n",
    "def replace_linear_with_quantized(model: nn.Module, weight_bits: int = 2, dynamic=False) -> nn.Module:\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data = quant(module.weight.data, weight_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quant(module.bias.data, weight_bits)\n",
    "        elif isinstance(module, nn.Module):\n",
    "            replace_linear_with_quantized(module, weight_bits, dynamic)\n",
    "    return model\n",
    "\n",
    "\n",
    "bit_sizes = [16, 8, 6, 4, 2]\n",
    "eval_size = 1000\n",
    "scores = {}\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "print(\"Evaluating Base Model\")\n",
    "results_file = f\"./results/quant_32.json\"\n",
    "eval_model(model, results_file, eval_size)\n",
    "print(\"Finished Evaluating\")\n",
    "score = eval_results(results_file)\n",
    "scores[32] = score\n",
    "print(f\"Score: {score}\")\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "time.sleep(2)\n",
    "\n",
    "for i, bits in enumerate(bit_sizes):\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(f\"\\nReplacing linear layers with weight bits:{bits}\")\n",
    "    model = replace_linear_with_quantized(model, weight_bits=bits, dynamic=False)\n",
    "    print(\"Evaluating Model\")\n",
    "    results_file = f\"./results/quant_{bits}.json\"\n",
    "    eval_model(model, results_file, eval_size)\n",
    "    print(\"Finished Evaluating\")\n",
    "    score = eval_results(results_file)\n",
    "    scores[bits] = score\n",
    "\n",
    "    print(f\"Score: {score}\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142f5dc-15eb-4275-96c2-9f36082107d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292bdee9-b29e-42da-aa8f-209a4b8b7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keys = list(scores.keys())\n",
    "values = list(scores.values())\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(keys, values, marker=\"o\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Scores Plot\")\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Set x-axis ticks to only show the actual key values\n",
    "plt.xticks(keys)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(keys[i], v, f\"{v:.4f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Adjust y-axis to start from 0\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d636d09-0b54-477a-94f5-8f09f2eeb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_size = 1000\n",
    "scores = {}\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "print(\"Evaluating Base Model\")\n",
    "results_file = f\"./results/quant_32.json\"\n",
    "eval_model(model, results_file, eval_size)\n",
    "print(\"Finished Evaluating\")\n",
    "score = eval_results(results_file)\n",
    "scores[32] = score\n",
    "print(f\"Score: {score}\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb560d5-78e6-4773-ae01-e204faf07915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def quant1(x: torch.Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    if min_val == max_val:\n",
    "        return x\n",
    "\n",
    "    scale = (2**num_bits - 1) / (max_val - min_val)\n",
    "    zero_point = (-min_val * scale).round()\n",
    "\n",
    "    x_int = (x * scale + zero_point).round().clamp(0, 2**num_bits - 1)\n",
    "    x_quant = (x_int - zero_point) / scale\n",
    "\n",
    "    return x_quant\n",
    "\n",
    "\n",
    "def quant2(x: torch.Tensor, num_bits):\n",
    "    min_val = x.min(dim=-1).values.unsqueeze(-1)\n",
    "    max_val = x.max(dim=-1).values.unsqueeze(-1)\n",
    "\n",
    "    alpha = max_val - min_val\n",
    "\n",
    "    x = (x - min_val) / alpha\n",
    "\n",
    "    scale = 2**num_bits - 1\n",
    "\n",
    "    result = (scale * x).round()\n",
    "\n",
    "    result /= scale\n",
    "\n",
    "    result = alpha * result + min_val\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def compare_quant_functions(num_samples=1000, tensor_size=100, num_bits=8):\n",
    "    max_diff = 0\n",
    "    avg_diff = 0\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Generate random tensor\n",
    "        x = torch.randn(tensor_size)\n",
    "\n",
    "        # Apply both quantization functions\n",
    "        q1 = quant1(x, num_bits)\n",
    "        q2 = quant2(x, num_bits)\n",
    "\n",
    "        # Calculate difference\n",
    "        diff = torch.abs(q1 - q2)\n",
    "        max_diff = max(max_diff, diff.max().item())\n",
    "        avg_diff += diff.mean().item()\n",
    "\n",
    "    avg_diff /= num_samples\n",
    "\n",
    "    print(f\"Maximum difference: {max_diff}\")\n",
    "    print(f\"Average difference: {avg_diff}\")\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "compare_quant_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085aec4-1339-4dc0-8aa9-cd342ed3d4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
