{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d69b7-a03f-468e-b097-d81f5ac9b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def quantize_model(model, num_bits, method=\"uniform\", block_size=128, lambda_param=1e-5):\n",
    "    def quantize_uniform(tensor, num_bits):\n",
    "        qmin, qmax = 0, 2**num_bits - 1\n",
    "        scale = (tensor.max() - tensor.min()) / (qmax - qmin)\n",
    "        zero_point = qmin - torch.round(tensor.min() / scale)\n",
    "\n",
    "        quantized = torch.clamp(torch.round(tensor / scale + zero_point), qmin, qmax)\n",
    "        dequantized = (quantized - zero_point) * scale\n",
    "\n",
    "        return dequantized\n",
    "\n",
    "    def quant(x, num_bits):\n",
    "        return quantize_uniform(x, num_bits)\n",
    "\n",
    "    def gptq(W, H_inv, num_bits, block_size):\n",
    "        d_row, d_col = W.shape\n",
    "        Q = torch.zeros_like(W)\n",
    "        E = torch.zeros(d_row, min(block_size, d_col), device=W.device)\n",
    "\n",
    "        for i in range(0, d_col, block_size):\n",
    "            curr_block_size = min(block_size, d_col - i)\n",
    "            H_inv_block = H_inv[i : i + curr_block_size, i : i + curr_block_size]\n",
    "\n",
    "            for j in range(curr_block_size):\n",
    "                idx = i + j\n",
    "                Q[:, idx] = quant(W[:, idx], num_bits)\n",
    "                E[:, j] = W[:, idx] - Q[:, idx]\n",
    "                update = E[:, j].unsqueeze(1) @ H_inv_block[j : j + 1, j:]\n",
    "                W[:, idx : i + curr_block_size] -= update\n",
    "\n",
    "            if i + curr_block_size < d_col:\n",
    "                W[:, i + curr_block_size :] -= (\n",
    "                    E[:, :curr_block_size] @ H_inv[i : i + curr_block_size, i + curr_block_size :]\n",
    "                )\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def compute_hessian(X, lambda_param):\n",
    "        X = X.T\n",
    "        H = 2 * X @ X.T\n",
    "        H_reg = H + lambda_param * torch.eye(H.shape[0], device=H.device)\n",
    "        return torch.inverse(H_reg)\n",
    "\n",
    "    def quantize_layer_gptq(layer, X):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            H_inv = compute_hessian(X, lambda_param)\n",
    "            layer.weight.data = gptq(layer.weight.data, H_inv, num_bits, block_size)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data = quantize_uniform(layer.bias.data, num_bits)\n",
    "        return layer\n",
    "\n",
    "    def quantize_layer_uniform(layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer.weight.data = quantize_uniform(layer.weight.data, num_bits)\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data = quantize_uniform(layer.bias.data, num_bits)\n",
    "        return layer\n",
    "\n",
    "    if method == \"uniform\":\n",
    "        return model.apply(quantize_layer_uniform)\n",
    "    elif method == \"gptq\":\n",
    "        device = next(model.parameters()).device\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                in_features = module.in_features\n",
    "                # Generate random input for this layer\n",
    "                X = torch.randn(100, in_features, device=device)  # Increased sample size\n",
    "                quantize_layer_gptq(module, X)\n",
    "        return model\n",
    "    elif method == \"awq\":\n",
    "        raise NotImplementedError(\"AWQ quantization not implemented\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported quantization method\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# quantized_model = quantize_model(model, num_bits=8, method='gptq', block_size=128, lambda_param=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57298cea-d0f7-4fd7-b236-753cf8c6a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f7498-7dac-4dbb-b003-956753f4d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_model(model, num_bits=8, method=\"gptq\", block_size=128, lambda_param=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638c63c-a0c2-4141-a446-bedcdd868478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
