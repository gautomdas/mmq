{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf77250-15a6-49b4-a87b-c54a31fb1967",
   "metadata": {},
   "source": [
    "# Dataloader Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d60f85-580c-4bda-82a8-b3fda6d65b5d",
   "metadata": {},
   "source": [
    "## Flickr30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a0631-148f-462c-aa8c-655940dc8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Flickr30k\n",
    "\n",
    "flickr_dataset = Flickr30k(csv_file='./data/flickr30k/results.csv', \n",
    "                          img_dir='./data/flickr30k/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d561e7-975f-4c02-af5c-b702cdf0a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116226f5-4722-492c-b2c3-61ccc8336ce8",
   "metadata": {},
   "source": [
    "## COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae094ad7-0998-40ad-86aa-4455fcf033e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import COCODataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8643f-6087-4c83-91c9-200143193819",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = COCODataset(ann_file='./data/coco/annotations/captions_val2017.json',\n",
    "                           img_dir='./data/coco/val2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452328e-9b83-41c8-a208-eb14df07f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66484b6-99dc-43cb-9cd8-d37cc90ec8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70649f75-aa37-4523-b00a-2402774c4c5b",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d795a-914c-4e12-829e-e630b508a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Flickr30k\n",
    "from torchvision import transforms\n",
    "import multiprocessing\n",
    "\n",
    "# Define your transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "flickr_dataset = Flickr30k(csv_file='./data/flickr30k/results.csv', \n",
    "                           img_dir='./data/flickr30k/images/',\n",
    "                           transform=transform)\n",
    "\n",
    "# Set up DataLoader parameters\n",
    "batch_size = 32\n",
    "num_workers = multiprocessing.cpu_count()  # Use all available CPU cores\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(\n",
    "    flickr_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True  # This can speed up data transfer to GPU\n",
    ")\n",
    "\n",
    "# Demonstrate loading data\n",
    "def process_batch(batch):\n",
    "    images, captions = batch\n",
    "    print(f\"Batch size: {images.shape[0]}\")\n",
    "    print(f\"Image shape: {images.shape}\")\n",
    "    print(f\"Number of captions: {len(captions)}\")\n",
    "    print(f\"First caption: {captions[0]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Iterate through a few batches\n",
    "for i, batch in enumerate(data_loader):\n",
    "    process_batch(batch)\n",
    "    if i == 2:  # Stop after 3 batches\n",
    "        break\n",
    "Steve Keen\n",
    "print(f\"DataLoader is using {num_workers} workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce4a83-2ac2-42e4-afa5-051dbed6f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde8858-2a68-4f1d-8806-0e9f28d54f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177da5b-e05a-4de2-b15d-21701849a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr_dataset[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819bfa4-d437-46c0-bb54-47c1058f3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "# Load an example image\n",
    "image = flickr_dataset[0][0]\n",
    "\n",
    "# Example text\n",
    "text = flickr_dataset[0][1][0]\n",
    "\n",
    "# 1. Encoding a single image\n",
    "def encode_image(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "image_inputs = encode_image(image)\n",
    "print(\"Image inputs:\", image_inputs.keys())\n",
    "\n",
    "# 2. Encoding a single text\n",
    "def encode_text(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "text_inputs = encode_text(text)\n",
    "print(\"Text inputs:\", text_inputs.keys())\n",
    "\n",
    "# 3. Encoding a single image + text\n",
    "def encode_image_and_text(image, text):\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "image_text_inputs = encode_image_and_text(image, text)\n",
    "print(\"Image + Text inputs:\", image_text_inputs.keys())\n",
    "\n",
    "# Decoding examples\n",
    "\n",
    "# For image captioning (decoding generated ids)\n",
    "def generate_and_decode_caption(image_inputs):\n",
    "    generated_ids = model.generate(**image_inputs)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n",
    "caption = generate_and_decode_caption(image_inputs)\n",
    "print(\"Generated caption:\", caption)\n",
    "\n",
    "# Decoding input ids (if you want to see the tokenized text)\n",
    "def decode_input_ids(input_ids):\n",
    "    decoded_text = processor.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return decoded_text\n",
    "\n",
    "if 'input_ids' in text_inputs:\n",
    "    decoded_input = decode_input_ids(text_inputs['input_ids'])\n",
    "    print(\"Decoded input:\", decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4839ce-0f22-4e50-b420-7c367a76ff96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
