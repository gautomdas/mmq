{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97476e32-0eb0-4d53-bea1-819a6a05cfc4",
   "metadata": {},
   "source": [
    "# Analyzing Uniform Quantization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d9fa3-3fe4-4df7-b98f-7bd089165b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nbitlinear.py from https://github.com/Vincent-La/lavis_clone/blob/main/lavis/layers/nbitlineardynamic.py\n",
    "'''\n",
    "\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def quant(x: Tensor, num_bits):\n",
    "    min_val = x.min(dim=-1).values.unsqueeze(-1)\n",
    "    max_val = x.max(dim=-1).values.unsqueeze(-1)\n",
    "    \n",
    "    alpha = max_val - min_val\n",
    "    x = (x-min_val)/alpha\n",
    "\n",
    "    result = alpha * result + min_val\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class NBitLinearDynamic(nn.Linear):\n",
    "    def __init__(self, *kargs, weight_bits=8, activation_bits=8, **kwargs):\n",
    "        super().__init__(*kargs, **kwargs)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.activation_bits = activation_bits\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        w = self.weight\n",
    "        b = self.bias\n",
    "\n",
    "        x_quant = x + (quant(x, self.activation_bits) - x).detach()\n",
    "        w_quant = w + (quant(w, self.weight_bits) - w).detach()\n",
    "\n",
    "        if b != None:\n",
    "            b = b + (quant(b, self.weight_bits) - b).detach()\n",
    "\n",
    "        y = F.linear(x_quant, w_quant, bias=b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return super().extra_repr() + f\" | w={self.weight_bits}, a={self.activation_bits}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ed2bb-be47-4845-8741-47b32387f115",
   "metadata": {},
   "source": [
    "## Testing quantization of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f0748-7b1e-4a75-959c-8a4ef0efb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def test_quantization(seed=0):\n",
    "    # Create a sample tensor\n",
    "    torch.manual_seed(seed)  # for reproducibility\n",
    "    x = torch.randn(1, 100)  # 1x100 tensor of random values\n",
    "\n",
    "    # Test different bit sizes\n",
    "    bit_sizes = [1, 2, 4, 8]\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(len(bit_sizes) + 1, 1, 1)\n",
    "    plt.plot(x.squeeze().numpy())\n",
    "    plt.title(\"Original Data\")\n",
    "\n",
    "    for i, bits in enumerate(bit_sizes):\n",
    "        quantized = quant(x, bits)\n",
    "\n",
    "        plt.subplot(len(bit_sizes) + 1, 1, i + 2)\n",
    "        plt.plot(quantized.squeeze().numpy())\n",
    "        plt.title(f\"{bits}-bit Quantization\")\n",
    "\n",
    "        print(f\"\\n{bits}-bit Quantization:\")\n",
    "        print(\"Min value:\", quantized.min().item())\n",
    "        print(\"Max value:\", quantized.max().item())\n",
    "        print(\"Unique values:\", torch.unique(quantized).numel())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_quantization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c2ad42-b16a-4f69-8736-b38e7d3d1d84",
   "metadata": {},
   "source": [
    "## Memory efficient nbitlinear static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38e888-7dd8-4f4a-999f-11ae71631491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def quant(x: torch.Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    if min_val == max_val:\n",
    "        return x\n",
    "\n",
    "    scale = (2**num_bits - 1) / (max_val - min_val)\n",
    "    zero_point = (-min_val * scale).round()\n",
    "\n",
    "    x_int = (x * scale + zero_point).round().clamp(0, 2**num_bits - 1)\n",
    "    x_quant = (x_int - zero_point) / scale\n",
    "\n",
    "    return x_quant\n",
    "\n",
    "\n",
    "def replace_linear_with_quantized(model: nn.Module, weight_bits: int = 2, dynamic=False) -> nn.Module:\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data = quant(module.weight.data, weight_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quant(module.bias.data, weight_bits)\n",
    "        elif isinstance(module, nn.Module):\n",
    "            replace_linear_with_quantized(module, weight_bits, dynamic)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_nn_quantization(seed=0, input_size=100, hidden_size=50, output_size=10):\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create a simple neural network\n",
    "    original_model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Create a sample input\n",
    "    x = torch.randn(1, input_size).to(device)\n",
    "\n",
    "    # Get original output\n",
    "    with torch.no_grad():\n",
    "        original_output = original_model(x)\n",
    "\n",
    "    print(\"Original model:\")\n",
    "    print(\"Hidden layer weights:\")\n",
    "    print(\"  Min value:\", original_model.hidden.weight.data.min().item())\n",
    "    print(\"  Max value:\", original_model.hidden.weight.data.max().item())\n",
    "    print(\"  Unique values:\", torch.unique(original_model.hidden.weight.data).numel())\n",
    "\n",
    "    # Move original model to CPU and clear GPU memory\n",
    "    original_model = original_model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Test different bit sizes\n",
    "    bit_sizes = [16, 8, 4, 2, 1]\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(len(bit_sizes) + 1, 1, 1)\n",
    "    plt.plot(original_output.cpu().squeeze().numpy())\n",
    "    plt.title(\"Original Output\")\n",
    "\n",
    "    for i, bits in enumerate(bit_sizes):\n",
    "        print(f\"\\nReplacing linear layers with weight bits:{bits}\")\n",
    "\n",
    "        # Move original model to CPU and clear GPU memory\n",
    "        original_model = original_model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Create a deep copy of the original model and move it to GPU\n",
    "        quantized_model = copy.deepcopy(original_model).to(device)\n",
    "        # Quantize the model\n",
    "        quantized_model = replace_linear_with_quantized(quantized_model, weight_bits=bits, dynamic=False)\n",
    "\n",
    "        # Get quantized output\n",
    "        with torch.no_grad():\n",
    "            quantized_output = quantized_model(x)\n",
    "\n",
    "        plt.subplot(len(bit_sizes) + 1, 1, i + 2)\n",
    "        plt.plot(quantized_output.cpu().squeeze().numpy())\n",
    "        plt.title(f\"{bits}-bit Quantization\")\n",
    "\n",
    "        print(f\"\\n{bits}-bit Quantization:\")\n",
    "        print(\"Output min value:\", quantized_output.min().item())\n",
    "        print(\"Output max value:\", quantized_output.max().item())\n",
    "        print(\"Output unique values:\", torch.unique(quantized_output).numel())\n",
    "\n",
    "        # Print some stats about the hidden layer weights\n",
    "        hidden_weights = quantized_model.hidden.weight.data\n",
    "        print(\"Hidden layer weights:\")\n",
    "        print(\"  Min value:\", hidden_weights.min().item())\n",
    "        print(\"  Max value:\", hidden_weights.max().item())\n",
    "        print(\"  Unique values:\", torch.unique(hidden_weights).numel())\n",
    "\n",
    "        # Clear the quantized model from GPU memory\n",
    "        del quantized_model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_nn_quantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716870f-1d07-44f4-b4c3-5d4da8fda975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "def compare_quantization_methods(model, processor, coco_dataset, device, weight_bits=2, num_samples=50):\n",
    "    results = {\"original\": [], \"dynamic\": [], \"static\": []}\n",
    "    times = {\"original\": 0, \"dynamic\": 0, \"static\": 0}\n",
    "\n",
    "    # Original model evaluation\n",
    "    print(\"Evaluating original model...\")\n",
    "    start_time = time.time()\n",
    "    results[\"original\"] = eval_model(model, coco_dataset, processor, device, num_samples)\n",
    "    times[\"original\"] = time.time() - start_time\n",
    "\n",
    "    # # Save original state\n",
    "    # original_state = {name: param.clone() for name, param in model.state_dict().items()}\n",
    "\n",
    "    # # Dynamic quantization\n",
    "    # print(\"Evaluating dynamically quantized model...\")\n",
    "    # model = replace_linear_with_quantized(model, weight_bits=weight_bits, dynamic=True)\n",
    "    # model.to(device)  # Ensure the entire model is on the correct device\n",
    "    # start_time = time.time()\n",
    "    # results['dynamic'] = eval_model(model, coco_dataset, processor, device, num_samples)\n",
    "    # times['dynamic'] = time.time() - start_time\n",
    "\n",
    "    # # Restore original state\n",
    "    # model.load_state_dict(original_state)\n",
    "\n",
    "    # Static quantization\n",
    "    print(\"Evaluating statically quantized model...\")\n",
    "    # Move original model to CPU and clear GPU memory\n",
    "    model = model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    # Create a deep copy of the original model and move it to GPU\n",
    "    print(\"Attempting copy\")\n",
    "    model = copy.deepcopy(model).to(device)\n",
    "    print(\"Copy succeeded\")\n",
    "    model = replace_linear_with_quantized(model, weight_bits=4, dynamic=False)\n",
    "    start_time = time.time()\n",
    "    results[\"static\"] = eval_model(model, coco_dataset, processor, device, num_samples)\n",
    "    times[\"static\"] = time.time() - start_time\n",
    "\n",
    "    return results, times\n",
    "\n",
    "\n",
    "def eval_model(model, dataset, processor, device, num_samples):\n",
    "    results = []\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(num_samples, len(dataset)))):\n",
    "            image, _ = dataset[i]\n",
    "\n",
    "            inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            # Ensure all input tensors are on the correct device\n",
    "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            out = model.generate(**inputs)\n",
    "\n",
    "            caption = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            image_id = dataset.ids[i]\n",
    "            results.append({\"image_id\": image_id, \"caption\": caption})\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results, times, filename):\n",
    "    output = {\"results\": results, \"times\": times}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load BLIP-2 model and processor\n",
    "    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Load COCO dataset\n",
    "    coco_dataset = COCODataset(\n",
    "        ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "        img_dir=\"./data/coco/val2017\",\n",
    "    )\n",
    "\n",
    "    # Compare quantization methods\n",
    "    results, times = compare_quantization_methods(model, processor, coco_dataset, device, weight_bits=2, num_samples=50)\n",
    "\n",
    "    # Save results\n",
    "    save_results(results, times, \"./results/quantization_comparison.json\")\n",
    "\n",
    "    print(\"Evaluation complete. Results saved to ./results/quantization_comparison.json\")\n",
    "    print(\"Execution times:\")\n",
    "    for method, time in times.items():\n",
    "        print(f\"{method}: {time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a505c-9789-45ba-939e-541d5bb39024",
   "metadata": {},
   "outputs": [],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffb172-03c2-438d-b3e9-37e8d0ad1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from datasets import COCODataset\n",
    "import json\n",
    "\n",
    "\n",
    "def test_static_quantization(model, processor, coco_dataset, device, weight_bits=2, num_samples=5):\n",
    "    print(\"Testing static quantization...\")\n",
    "\n",
    "    # Apply static quantization\n",
    "    model = replace_linear_with_quantized(model, weight_bits=weight_bits, dynamic=False)\n",
    "    model.to(device)\n",
    "\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(num_samples, len(coco_dataset)))):\n",
    "            image, _ = coco_dataset[i]\n",
    "\n",
    "            inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            # Ensure all input tensors are on the correct device\n",
    "            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "\n",
    "            try:\n",
    "                out = model.generate(**inputs)\n",
    "                caption = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "                results.append({\"image_id\": coco_dataset.ids[i], \"caption\": caption})\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing image {i}: {str(e)}\")\n",
    "                # Print device information for debugging\n",
    "                print(f\"Input device: {inputs['pixel_values'].device}\")\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.device != device:\n",
    "                        print(f\"Parameter {name} is on {param.device}, expected {device}\")\n",
    "                break\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Number of successful inferences: {len(results)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP-2 model and processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load COCO dataset\n",
    "coco_dataset = COCODataset(\n",
    "    ann_file=\"./data/coco/annotations/captions_val2017.json\",\n",
    "    img_dir=\"./data/coco/val2017\",\n",
    ")\n",
    "\n",
    "# Test static quantization\n",
    "results = test_static_quantization(model, processor, coco_dataset, device, weight_bits=2, num_samples=5)\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Image ID: {result['image_id']}\")\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf5136-c906-420d-b0fb-b88f5fd3311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def quant(x: torch.Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    if min_val == max_val:\n",
    "        return x\n",
    "\n",
    "    scale = (2**num_bits - 1) / (max_val - min_val)\n",
    "    zero_point = (-min_val * scale).round()\n",
    "\n",
    "    x_int = (x * scale + zero_point).round().clamp(0, 2**num_bits - 1)\n",
    "    x_quant = (x_int - zero_point) / scale\n",
    "\n",
    "    return x_quant\n",
    "\n",
    "\n",
    "def replace_linear_with_quantized(model: nn.Module, weight_bits: int = 2, dynamic=False) -> nn.Module:\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data = quant(module.weight.data, weight_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quant(module.bias.data, weight_bits)\n",
    "        elif isinstance(module, nn.Module):\n",
    "            replace_linear_with_quantized(module, weight_bits, dynamic)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_nn_quantization(seed=0, input_size=100, hidden_size=50, output_size=10):\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create a simple neural network\n",
    "    original_model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Create a sample input\n",
    "    x = torch.randn(1, input_size).to(device)\n",
    "\n",
    "    # Get original output\n",
    "    with torch.no_grad():\n",
    "        original_output = original_model(x)\n",
    "\n",
    "    print(\"Original model:\")\n",
    "    print(\"Hidden layer weights:\")\n",
    "    print(\"  Min value:\", original_model.hidden.weight.data.min().item())\n",
    "    print(\"  Max value:\", original_model.hidden.weight.data.max().item())\n",
    "    print(\"  Unique values:\", torch.unique(original_model.hidden.weight.data).numel())\n",
    "\n",
    "    # Move original model to CPU and clear GPU memory\n",
    "    original_model = original_model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Test different bit sizes\n",
    "    bit_sizes = [16, 8, 4, 2, 1]\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(len(bit_sizes) + 1, 1, 1)\n",
    "    plt.plot(original_output.cpu().squeeze().numpy())\n",
    "    plt.title(\"Original Output\")\n",
    "\n",
    "    for i, bits in enumerate(bit_sizes):\n",
    "        print(f\"\\nReplacing linear layers with weight bits:{bits}\")\n",
    "\n",
    "        # Create a deep copy of the original model and move it to GPU\n",
    "        quantized_model = copy.deepcopy(original_model).to(device)\n",
    "\n",
    "        # Quantize the model\n",
    "        quantized_model = replace_linear_with_quantized(quantized_model, weight_bits=bits, dynamic=False)\n",
    "\n",
    "        # Get quantized output\n",
    "        with torch.no_grad():\n",
    "            quantized_output = quantized_model(x)\n",
    "\n",
    "        plt.subplot(len(bit_sizes) + 1, 1, i + 2)\n",
    "        plt.plot(quantized_output.cpu().squeeze().numpy())\n",
    "        plt.title(f\"{bits}-bit Quantization\")\n",
    "\n",
    "        print(f\"\\n{bits}-bit Quantization:\")\n",
    "        print(\"Output min value:\", quantized_output.min().item())\n",
    "        print(\"Output max value:\", quantized_output.max().item())\n",
    "        print(\"Output unique values:\", torch.unique(quantized_output).numel())\n",
    "\n",
    "        # Print some stats about the hidden layer weights\n",
    "        hidden_weights = quantized_model.hidden.weight.data\n",
    "        print(\"Hidden layer weights:\")\n",
    "        print(\"  Min value:\", hidden_weights.min().item())\n",
    "        print(\"  Max value:\", hidden_weights.max().item())\n",
    "        print(\"  Unique values:\", torch.unique(hidden_weights).numel())\n",
    "\n",
    "        # Clear the quantized model from GPU memory\n",
    "        del quantized_model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_nn_quantization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5d906-9cc0-41d4-ad6a-0f44861943f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant(x: Tensor, num_bits):\n",
    "    min_val = x.min()\n",
    "    max_val = x.max()\n",
    "\n",
    "    if min_val == max_val:\n",
    "        return x\n",
    "\n",
    "    scale = (2**num_bits - 1) / (max_val - min_val)\n",
    "    zero_point = (-min_val * scale).round()\n",
    "\n",
    "    x_int = (x * scale + zero_point).round()\n",
    "    x_quant = (x_int - zero_point) / scale\n",
    "\n",
    "    return x_quant.clamp(min_val, max_val)\n",
    "\n",
    "\n",
    "class NBitLinearStatic(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, weight_bits=8):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.weight_bits = weight_bits\n",
    "        self.quantize_weights()\n",
    "\n",
    "    def quantize_weights(self):\n",
    "        self.weight.data = quant(self.weight.data, self.weight_bits)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data = quant(self.bias.data, self.weight_bits)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return super().extra_repr() + f\" | w={self.weight_bits}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53c58d-53ca-4970-97b2-064c5528712d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
