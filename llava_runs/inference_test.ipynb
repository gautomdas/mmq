{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8777b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers.models.llava.image_processing_llava import LlavaImageProcessor\n",
    "import transformers\n",
    "\n",
    "from dataset import VQAv2Eval, GQAEval\n",
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a122f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_root = \"./data/vqav2/annotations\"\n",
    "q_root = \"./data/vqav2/questions\"\n",
    "image_root = \"./data/vqav2/val2014\"\n",
    "# short answer prompting according to: https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n",
    "llava_prompt = \"USER: <image>\\n{}\\nAnswer the question using a single word or phrase. ASSISTANT:\"\n",
    "\n",
    "dataset = VQAv2Eval(image_root=image_root, ann_root=ann_root, q_root=q_root, prompt=llava_prompt)\n",
    "\n",
    "# dataset.set_max_samples(21435)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5bd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16)\n",
    "model.to(\"cuda\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", pad_token=\"<pad>\", use_fast=False)\n",
    "\n",
    "# need to use this image processor w/ do_pad=True according to \"Note regarding reproducing original implementation\"\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "image_processor = LlavaImageProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", do_pad=True)\n",
    "\n",
    "processor.image_processor = image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ef9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# short answer prompting according to: https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n",
    "llava_prompt = \"USER: <image>\\n{}\\nAnswer the question using a single word or phrase. ASSISTANT:\"\n",
    "\n",
    "# GQA dataset paths\n",
    "image_root = \"./data/gqa/images\"\n",
    "q_root = \"./data/gqa/questions\"\n",
    "\n",
    "dataset = GQAEval(image_root, q_root, prompt=llava_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferencePipeline(model, device, processor)\n",
    "\n",
    "# set this according to huggingface usage tips: https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "processor_kwargs = dict(padding=True)\n",
    "\n",
    "# greedy decoding\n",
    "# generate_kwargs = {\n",
    "#     'num_beams': 1,\n",
    "#     'do_sample': False\n",
    "# }\n",
    "\n",
    "results = inferencer.run_inference(dataloader, task=\"gqa\", processor_kwargs=processor_kwargs, generate_kwargs=None)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a5290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "for res in results:\n",
    "    res[\"answer\"] = res[\"answer\"].split(\"ASSISTANT: \")[-1]\n",
    "\n",
    "\n",
    "def compute_gqa_results(results, scorer, save_path=None):\n",
    "    gqa_results = scorer.compute_scores(results, \"gqa\")\n",
    "    print(gqa_results)\n",
    "\n",
    "\n",
    "# if save_path:\n",
    "#     with open(save_path, \"w\") as f:\n",
    "#         json.dump(gqa_results, f)\n",
    "\n",
    "scorer = ScoringPipeline()\n",
    "compute_gqa_results(results, scorer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
