{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebc607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers.models.llava.image_processing_llava import LlavaImageProcessor\n",
    "\n",
    "from dataset import VQAv2Eval\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c285b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5e4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# ====================================================\n",
    "# Quantization Classes and Functions\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "def quantize(x, scale, zero, maxq):\n",
    "    if maxq < 0:\n",
    "        return (x > scale / 2).float() * scale + (x < zero / 2).float() * zero\n",
    "    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n",
    "    return scale * (q - zero)\n",
    "\n",
    "\n",
    "class Quantizer(nn.Module):\n",
    "    def __init__(self, shape=1):\n",
    "        super(Quantizer, self).__init__()\n",
    "        self.register_buffer(\"maxq\", torch.tensor(0))\n",
    "        self.register_buffer(\"scale\", torch.zeros(shape))\n",
    "        self.register_buffer(\"zero\", torch.zeros(shape))\n",
    "\n",
    "    def configure(\n",
    "        self,\n",
    "        bits,\n",
    "        perchannel=False,\n",
    "        sym=True,\n",
    "        mse=False,\n",
    "        norm=2.4,\n",
    "        grid=100,\n",
    "        maxshrink=0.8,\n",
    "        trits=False,\n",
    "    ):\n",
    "        device = self.maxq.device\n",
    "        self.maxq = torch.tensor(2**bits - 1, device=device)\n",
    "        self.perchannel = perchannel\n",
    "        self.sym = sym\n",
    "        self.mse = mse\n",
    "        self.norm = norm\n",
    "        self.grid = grid\n",
    "        self.maxshrink = maxshrink\n",
    "        if trits:\n",
    "            self.maxq = torch.tensor(-1, device=device)\n",
    "\n",
    "    def find_params(self, x, weight=False):\n",
    "        dev = x.device\n",
    "        self.maxq = self.maxq.to(dev)\n",
    "\n",
    "        shape = x.shape\n",
    "        if self.perchannel:\n",
    "            if weight:\n",
    "                x = x.flatten(1)\n",
    "            else:\n",
    "                if len(shape) == 4:\n",
    "                    x = x.permute([1, 0, 2, 3])\n",
    "                    x = x.flatten(1)\n",
    "                if len(shape) == 3:\n",
    "                    x = x.reshape((-1, shape[-1])).t()\n",
    "                if len(shape) == 2:\n",
    "                    x = x.t()\n",
    "        else:\n",
    "            x = x.flatten().unsqueeze(0)\n",
    "\n",
    "        tmp = torch.zeros(x.shape[0], device=dev)\n",
    "        xmin = torch.minimum(x.min(1)[0], tmp)\n",
    "        xmax = torch.maximum(x.max(1)[0], tmp)\n",
    "\n",
    "        if self.sym:\n",
    "            xmax = torch.maximum(torch.abs(xmin), xmax)\n",
    "            tmp = xmin < 0\n",
    "            if torch.any(tmp):\n",
    "                xmin[tmp] = -xmax[tmp]\n",
    "        tmp = (xmin == 0) & (xmax == 0)\n",
    "        xmin[tmp] = -1\n",
    "        xmax[tmp] = +1\n",
    "\n",
    "        if self.maxq < 0:\n",
    "            self.scale = xmax\n",
    "            self.zero = xmin\n",
    "        else:\n",
    "            self.scale = (xmax - xmin) / self.maxq\n",
    "            if self.sym:\n",
    "                self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n",
    "            else:\n",
    "                self.zero = torch.round(-xmin / self.scale)\n",
    "\n",
    "        if self.mse:\n",
    "            best = torch.full([x.shape[0]], float(\"inf\"), device=dev)\n",
    "            for i in range(int(self.maxshrink * self.grid)):\n",
    "                p = 1 - i / self.grid\n",
    "                xmin1 = p * xmin\n",
    "                xmax1 = p * xmax\n",
    "                scale1 = (xmax1 - xmin1) / self.maxq\n",
    "                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n",
    "                q = quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n",
    "                q -= x\n",
    "                q.abs_()\n",
    "                q.pow_(self.norm)\n",
    "                err = torch.sum(q, 1)\n",
    "                tmp = err < best\n",
    "                if torch.any(tmp):\n",
    "                    best[tmp] = err[tmp]\n",
    "                    self.scale[tmp] = scale1[tmp]\n",
    "                    self.zero[tmp] = zero1[tmp]\n",
    "        if not self.perchannel:\n",
    "            if weight:\n",
    "                tmp = shape[0]\n",
    "            else:\n",
    "                tmp = shape[1] if len(shape) != 3 else shape[2]\n",
    "            self.scale = self.scale.repeat(tmp)\n",
    "            self.zero = self.zero.repeat(tmp)\n",
    "\n",
    "        if weight:\n",
    "            shape = [-1] + [1] * (len(shape) - 1)\n",
    "            self.scale = self.scale.reshape(shape)\n",
    "            self.zero = self.zero.reshape(shape)\n",
    "            return\n",
    "        if len(shape) == 4:\n",
    "            self.scale = self.scale.reshape((1, -1, 1, 1))\n",
    "            self.zero = self.zero.reshape((1, -1, 1, 1))\n",
    "        if len(shape) == 3:\n",
    "            self.scale = self.scale.reshape((1, 1, -1))\n",
    "            self.zero = self.zero.reshape((1, 1, -1))\n",
    "        if len(shape) == 2:\n",
    "            self.scale = self.scale.unsqueeze(0)\n",
    "            self.zero = self.zero.unsqueeze(0)\n",
    "\n",
    "        # Ensure buffers are on the same device as input x\n",
    "        self.scale = self.scale.to(dev)\n",
    "        self.zero = self.zero.to(dev)\n",
    "\n",
    "    def quantize(self, x):\n",
    "        if self.ready():\n",
    "            # Ensure buffers are on the same device as x\n",
    "            self.scale = self.scale.to(x.device)\n",
    "            self.zero = self.zero.to(x.device)\n",
    "            self.maxq = self.maxq.to(x.device)\n",
    "            return quantize(x, self.scale, self.zero, self.maxq)\n",
    "        return x\n",
    "\n",
    "    def enabled(self):\n",
    "        return self.maxq > 0\n",
    "\n",
    "    def ready(self):\n",
    "        return torch.all(self.scale != 0)\n",
    "\n",
    "\n",
    "class GPTQ:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.dev = self.layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "        self.quantizer = Quantizer()\n",
    "        self.quantizer.to(self.dev)\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        if DEBUG:\n",
    "            self.inp1 = inp\n",
    "            self.out1 = out\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            unfold = nn.Unfold(\n",
    "                self.layer.kernel_size,\n",
    "                dilation=self.layer.dilation,\n",
    "                padding=self.layer.padding,\n",
    "                stride=self.layer.stride,\n",
    "            )\n",
    "            inp = unfold(inp)\n",
    "            inp = inp.permute([1, 0, 2])\n",
    "            inp = inp.flatten(1)\n",
    "\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = math.sqrt(2 / self.nsamples) * inp.float()\n",
    "        self.H += inp.matmul(inp.t())\n",
    "\n",
    "    def fasterquant(\n",
    "        self,\n",
    "        blocksize=128,\n",
    "        percdamp=0.01,\n",
    "        groupsize=-1,\n",
    "        actorder=False,\n",
    "        static_groups=False,\n",
    "    ):\n",
    "        W = self.layer.weight.data.clone()\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            W = W.flatten(1)\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            W = W.t()\n",
    "        W = W.float()\n",
    "\n",
    "        tick = time.time()\n",
    "\n",
    "        if not self.quantizer.ready():\n",
    "            self.quantizer.find_params(W, weight=True)\n",
    "\n",
    "        H = self.H\n",
    "        del self.H\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "\n",
    "        if static_groups:\n",
    "            import copy\n",
    "\n",
    "            groups = []\n",
    "            for i in range(0, self.columns, groupsize):\n",
    "                quantizer = copy.deepcopy(self.quantizer)\n",
    "                quantizer.find_params(W[:, i : (i + groupsize)], weight=True)\n",
    "                groups.append(quantizer)\n",
    "\n",
    "        if actorder:\n",
    "            perm = torch.argsort(torch.diag(H), descending=True)\n",
    "            W = W[:, perm]\n",
    "            H = H[perm][:, perm]\n",
    "            invperm = torch.argsort(perm)\n",
    "\n",
    "        Losses = torch.zeros_like(W)\n",
    "        Q = torch.zeros_like(W)\n",
    "\n",
    "        damp = percdamp * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(self.columns, device=self.dev)\n",
    "        H[diag, diag] += damp\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "\n",
    "        for i1 in range(0, self.columns, blocksize):\n",
    "            i2 = min(i1 + blocksize, self.columns)\n",
    "            count = i2 - i1\n",
    "\n",
    "            W1 = W[:, i1:i2].clone()\n",
    "            Q1 = torch.zeros_like(W1)\n",
    "            Err1 = torch.zeros_like(W1)\n",
    "            Losses1 = torch.zeros_like(W1)\n",
    "            Hinv1 = Hinv[i1:i2, i1:i2]\n",
    "\n",
    "            for i in range(count):\n",
    "                w = W1[:, i]\n",
    "                d = Hinv1[i, i]\n",
    "\n",
    "                if groupsize != -1:\n",
    "                    if not static_groups:\n",
    "                        if (i1 + i) % groupsize == 0:\n",
    "                            self.quantizer.find_params(W[:, (i1 + i) : (i1 + i + groupsize)], weight=True)\n",
    "                    else:\n",
    "                        idx = i1 + i\n",
    "                        if actorder:\n",
    "                            idx = perm[idx]\n",
    "                        self.quantizer = groups[idx // groupsize]\n",
    "\n",
    "                q = quantize(\n",
    "                    w.unsqueeze(1),\n",
    "                    self.quantizer.scale,\n",
    "                    self.quantizer.zero,\n",
    "                    self.quantizer.maxq,\n",
    "                ).flatten()\n",
    "                Q1[:, i] = q\n",
    "                Losses1[:, i] = (w - q) ** 2 / d**2\n",
    "\n",
    "                err1 = (w - q) / d\n",
    "                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n",
    "                Err1[:, i] = err1\n",
    "\n",
    "            Q[:, i1:i2] = Q1\n",
    "            Losses[:, i1:i2] = Losses1 / 2\n",
    "\n",
    "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n",
    "\n",
    "            if DEBUG:\n",
    "                self.layer.weight.data[:, :i2] = Q[:, :i2]\n",
    "                self.layer.weight.data[:, i2:] = W[:, i2:]\n",
    "                print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "                print(torch.sum(Losses))\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"Time for quantization: %.2f seconds\" % (time.time() - tick))\n",
    "        print(\"Total quantization error:\", torch.sum(Losses).item())\n",
    "\n",
    "        if actorder:\n",
    "            Q = Q[:, invperm]\n",
    "\n",
    "        if isinstance(self.layer, transformers.Conv1D):\n",
    "            Q = Q.t()\n",
    "        self.layer.weight.data = Q.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n",
    "        if DEBUG:\n",
    "            print(torch.sum((self.layer(self.inp1) - self.out1) ** 2))\n",
    "\n",
    "    def free(self):\n",
    "        if DEBUG:\n",
    "            self.inp1 = None\n",
    "            self.out1 = None\n",
    "        self.H = None\n",
    "        self.Losses = None\n",
    "        self.Trace = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def find_linear_layers_in_model(model):\n",
    "    layers = {}\n",
    "\n",
    "    def recurse(module, prefix=\"\"):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            layers[prefix.rstrip(\".\")] = module\n",
    "        for name, child in module.named_children():\n",
    "            recurse(child, prefix + name + \".\")\n",
    "\n",
    "    recurse(model)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaQuantizer:\n",
    "    def __init__(self, model, processor, device, chunk_size=32, task=\"vqav2\"):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        self.chunk_size = chunk_size\n",
    "        self.task = task\n",
    "\n",
    "        # Component-specific configuration parameters\n",
    "        self.config = {\n",
    "            \"vision\": {\n",
    "                \"bits\": 4,\n",
    "                \"percent_dampening\": 0.01,\n",
    "                \"group_size\": -1,\n",
    "                \"use_symmetric\": True,\n",
    "                \"use_act_order\": False,\n",
    "                \"use_static_groups\": False,\n",
    "            },\n",
    "            \"language\": {\n",
    "                \"bits\": 4,\n",
    "                \"percent_dampening\": 0.01,\n",
    "                \"group_size\": -1,\n",
    "                \"use_symmetric\": True,\n",
    "                \"use_act_order\": False,\n",
    "                \"use_static_groups\": False,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _prepare_quantizers(self, layers, component_type):\n",
    "        \"\"\"Initialize GPTQ quantizers for given layers with component-specific settings\"\"\"\n",
    "        config = self.config[component_type]\n",
    "        quantizers = {}\n",
    "        for name, layer in layers.items():\n",
    "            quantizers[name] = GPTQ(layer)\n",
    "            quantizers[name].quantizer.configure(\n",
    "                bits=config[\"bits\"],\n",
    "                perchannel=True,\n",
    "                sym=config[\"use_symmetric\"],\n",
    "                mse=False,\n",
    "            )\n",
    "        return quantizers\n",
    "\n",
    "    def _process_chunk(self, layers, start_idx, end_idx, forward_func, desc, component_type):\n",
    "        \"\"\"Process a chunk of layers with component-specific quantization settings\"\"\"\n",
    "        current_layers = dict(list(layers.items())[start_idx:end_idx])\n",
    "        print(\n",
    "            f\"\\nProcessing {desc} layers {start_idx} to {end_idx - 1} with {self.config[component_type]['bits']}-bit precision\"\n",
    "        )\n",
    "\n",
    "        # Initialize quantizers for current chunk\n",
    "        quantizers = self._prepare_quantizers(current_layers, component_type)\n",
    "        hooks = []\n",
    "\n",
    "        def get_hook(name):\n",
    "            def hook(module, inp, out):\n",
    "                if name in quantizers:\n",
    "                    quantizers[name].add_batch(inp[0].detach(), out.detach())\n",
    "\n",
    "            return hook\n",
    "\n",
    "        for name, layer in current_layers.items():\n",
    "            hooks.append(layer.register_forward_hook(get_hook(name)))\n",
    "\n",
    "        forward_func()\n",
    "\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        config = self.config[component_type]\n",
    "        for name, layer in current_layers.items():\n",
    "            print(f\"Quantizing layer {name}...\")\n",
    "            quantizer = quantizers[name]\n",
    "            quantizer.fasterquant(\n",
    "                blocksize=32,\n",
    "                percdamp=config[\"percent_dampening\"],\n",
    "                groupsize=config[\"group_size\"],\n",
    "                actorder=config[\"use_act_order\"],\n",
    "                static_groups=config[\"use_static_groups\"],\n",
    "            )\n",
    "\n",
    "            layer.weight.data = quantizer.quantizer.quantize(layer.weight.data).to(layer.weight.data.dtype)\n",
    "            quantizer.free()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def quantize_vision_model(self, calibration_set):\n",
    "        \"\"\"Quantize vision model with 8-bit precision\"\"\"\n",
    "        print(f\"Quantizing Vision Model with {self.config['vision']['bits']}-bit precision...\")\n",
    "\n",
    "        # some extra components need to be on device for vision model forward pass\n",
    "        # self.model.vision_tower.to(self.device)\n",
    "        self.model.to(self.device)\n",
    "        self.model.language_model.to(\"cpu\")\n",
    "\n",
    "        layers = find_linear_layers_in_model(self.model.vision_tower.vision_model)\n",
    "        total_layers = len(layers)\n",
    "\n",
    "        print(f\"total_layers: {total_layers}\")\n",
    "        print(layers)\n",
    "\n",
    "        def forward_pass():\n",
    "            vision_feature_layer = self.model.config.vision_feature_layer\n",
    "            vision_feature_select_strategy = self.model.config.vision_feature_select_strategy\n",
    "            image_sizes = None\n",
    "\n",
    "            # TODO: adjust for GQA if needed\n",
    "            if self.task == \"vqav2\":\n",
    "                for img, prompt in tqdm(calibration_set, desc=\"Processing vision model batch\"):\n",
    "                    inputs = self.processor(images=[img], text=[prompt], return_tensors=\"pt\", padding=True).to(\n",
    "                        self.device\n",
    "                    )\n",
    "\n",
    "                    # runs forward pass through vision_tower\n",
    "                    self.model.get_image_features(\n",
    "                        pixel_values=inputs[\"pixel_values\"],\n",
    "                        vision_feature_layer=vision_feature_layer,\n",
    "                        vision_feature_select_strategy=vision_feature_select_strategy,\n",
    "                        image_sizes=image_sizes,\n",
    "                    )\n",
    "\n",
    "        for start_idx in range(0, total_layers, self.chunk_size):\n",
    "            end_idx = min(start_idx + self.chunk_size, total_layers)\n",
    "            self._process_chunk(layers, start_idx, end_idx, forward_pass, \"vision model\", \"vision\")\n",
    "\n",
    "        self.model.vision_tower.vision_model.cpu()\n",
    "        print(\"Vision Model quantization complete.\\n\")\n",
    "\n",
    "    def quantize_language_model(self, calibration_set):\n",
    "        \"\"\"Quantize language model with 4-bit precision\"\"\"\n",
    "        print(f\"Quantizing Language Model with {self.config['language']['bits']}-bit precision...\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        layers = find_linear_layers_in_model(self.model.language_model.model)\n",
    "        # layers[\"language_projection\"] = self.model.language_projection\n",
    "        total_layers = len(layers)\n",
    "\n",
    "        def forward_pass():\n",
    "            # TODO: adjust for GQA if needed\n",
    "            if self.task == \"vqav2\":\n",
    "                for img, prompt in tqdm(calibration_set, desc=\"Processing language model batch\"):\n",
    "                    inputs = self.processor(images=[img], text=[prompt], return_tensors=\"pt\", padding=True).to(\n",
    "                        self.device\n",
    "                    )\n",
    "\n",
    "                    self.model.generate(**inputs)\n",
    "\n",
    "        for start_idx in range(0, total_layers, self.chunk_size):\n",
    "            end_idx = min(start_idx + self.chunk_size, total_layers)\n",
    "            self._process_chunk(layers, start_idx, end_idx, forward_pass, \"language model\", \"language\")\n",
    "\n",
    "        self.model.cpu()\n",
    "        print(\"Language Model quantization complete.\\n\")\n",
    "\n",
    "    def quantize(self, calibration_set):\n",
    "        \"\"\"Quantize all LLAVA components\"\"\"\n",
    "        print(\"Starting LLAVA model quantization...\")\n",
    "        self.quantize_vision_model(calibration_set)\n",
    "        self.quantize_language_model(calibration_set)\n",
    "        print(\"LLAVA model quantization complete.\")\n",
    "\n",
    "    # TODO:\n",
    "    def prepare_for_inference(self):\n",
    "        self.model.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# Load the model\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16)\n",
    "model.to(\"cpu\")\n",
    "# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", pad_token = '<pad>')\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", pad_token=\"<pad>\", use_fast=False)\n",
    "\n",
    "\n",
    "image_processor = LlavaImageProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", do_pad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor = image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # VQAv2 dataset paths\n",
    "# ann_root = './data/vqav2/annotations'\n",
    "# q_root = './data/vqav2/questions'\n",
    "# image_root = './data/vqav2/val2014'\n",
    "\n",
    "# # short answer prompting according to: https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md\n",
    "llava_prompt = \"USER: <image>\\n{}\\nAnswer the question using a single word or phrase. ASSISTANT:\"\n",
    "\n",
    "# dataset = VQAv2Eval(image_root=image_root,\n",
    "#                     ann_root=ann_root,\n",
    "#                     q_root=q_root,\n",
    "#                     prompt=llava_prompt)\n",
    "\n",
    "\n",
    "from dataset import GQAEval\n",
    "\n",
    "image_root = \"./data/gqa/images\"\n",
    "q_root = \"./data/gqa/questions\"\n",
    "\n",
    "dataset = GQAEval(image_root, q_root, prompt=llava_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "inferencer = InferencePipeline(model, device, processor)\n",
    "processor_kwargs = dict(padding=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = inferencer.run_inference(\n",
    "#     dataloader,\n",
    "#     task = 'vqav2',\n",
    "#     processor_kwargs = processor_kwargs,\n",
    "#     generate_kwargs = None\n",
    "# )\n",
    "\n",
    "# results = inferencer.run_inference(\n",
    "#     dataloader,\n",
    "#     task = 'gqa',\n",
    "#     processor_kwargs = processor_kwargs,\n",
    "#     generate_kwargs = None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_indices = range(len(dataset))\n",
    "total_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_SIZE = 128\n",
    "calibration_indices = random.sample(total_indices, CALIBRATION_SIZE)\n",
    "\n",
    "calibration_set = [(dataset[i][\"image\"], dataset[i][\"text_input\"]) for i in calibration_indices]\n",
    "len(calibration_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = LlavaQuantizer(model, processor, device)\n",
    "\n",
    "quantizer.config = {\n",
    "    \"vision\": {\n",
    "        \"bits\": 2,\n",
    "        \"percent_dampening\": 0.01,\n",
    "        \"group_size\": -1,\n",
    "        \"use_symmetric\": True,\n",
    "        \"use_act_order\": False,\n",
    "        \"use_static_groups\": False,\n",
    "    },\n",
    "    \"language\": {\n",
    "        \"bits\": 6,\n",
    "        \"percent_dampening\": 0.01,\n",
    "        \"group_size\": -1,\n",
    "        \"use_symmetric\": True,\n",
    "        \"use_act_order\": False,\n",
    "        \"use_static_groups\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "# quantizer.quantize_vision_model(calibration_set)\n",
    "# quantizer.quantize_language_model(calibration_set)\n",
    "\n",
    "start_time = time.time()\n",
    "quantizer.quantize(calibration_set)\n",
    "\n",
    "print(f\"Elapsed time: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f17f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset[42][\"image\"]\n",
    "# prompt = 'USER: <image>\\n' + dataset.qa_pairs[42]['question'] + '\\nAnswer the question using a single word or phrase. ASSISTANT:'\n",
    "\n",
    "prompt = dataset[42][\"text_input\"]\n",
    "print(prompt)\n",
    "\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# set this according to huggingface usage tips: https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "samples = processor(images=[img], text=[prompt], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "# Generate\n",
    "# generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generate_ids = model.generate(**samples, max_new_tokens=30)\n",
    "output = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].split(\"ASSISTANT: \")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68090ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[42]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
