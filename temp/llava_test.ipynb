{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "from dataset import VQAv2Eval\n",
    "# from inference_pipeline import InferencePipeline\n",
    "# import time\n",
    "# from scoring_pipeline import ScoringPipeline\n",
    "\n",
    "from dataset import VQAv2Eval\n",
    "\n",
    "# import os\n",
    "from awq.llava_quantizer import LlavaAWQQuantizer\n",
    "from transformers.models.llava.image_processing_llava import LlavaImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb450a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQAv2 dataset paths\n",
    "ann_root = \"./data/vqav2/annotations\"\n",
    "q_root = \"./data/vqav2/questions\"\n",
    "image_root = \"./data/vqav2/val2014\"\n",
    "\n",
    "llava_prompt = \"USER: <image>\\n{}\\nAnswer the question using a single word or phrase. ASSISTANT:\"\n",
    "\n",
    "dataset = VQAv2Eval(image_root=image_root, ann_root=ann_root, q_root=q_root, prompt=llava_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16)\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", pad_token=\"<pad>\", use_fast=False)\n",
    "# need to use this image processor w/ do_pad=True according to \"Note regarding reproducing original implementation\"\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "image_processor = LlavaImageProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", do_pad=True)\n",
    "\n",
    "processor.image_processor = image_processor\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP output\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, torch.float16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "\n",
    "    print(processor.batch_decode(generate_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642653d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# config['vision_layers'] = {\n",
    "#     'self_attn':16,\n",
    "#     'mlp': 16\n",
    "# }\n",
    "\n",
    "config[\"llm_layers\"] = {\"self_attn\": 4, \"mlp\": 4}\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = LlavaAWQQuantizer(model, device, processor, dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.n_samples = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ed47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantizer.n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff10b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "quantizer.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[100][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18399d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset[100][\"image\"]\n",
    "prompt = (\n",
    "    \"USER: <image>\\n\"\n",
    "    + dataset.qa_pairs[100][\"question\"]\n",
    "    + \"\\nAnswer the question using a single word or phrase. ASSISTANT:\"\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13756414",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "samples = processor(images=[img], text=[prompt], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "# generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "generate_ids = model.generate(**samples)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from inference_pipeline import InferencePipeline\n",
    "\n",
    "dataset.set_max_samples(10)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collater,\n",
    ")\n",
    "\n",
    "inferencer = InferencePipeline(model, device, processor)\n",
    "\n",
    "# set this according to huggingface usage tips: https://huggingface.co/docs/transformers/en/model_doc/llava\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "processor_kwargs = dict(padding=True)\n",
    "\n",
    "# greedy decoding\n",
    "generate_kwargs = {\"num_beams\": 1, \"do_sample\": False}\n",
    "\n",
    "results = inferencer.run_inference(\n",
    "    dataloader,\n",
    "    task=\"vqav2\",\n",
    "    processor_kwargs=processor_kwargs,\n",
    "    generate_kwargs=generate_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ecd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
