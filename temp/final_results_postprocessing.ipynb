{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46510845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    Blip2Processor,\n",
    "    Blip2ForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    Blip2ForImageTextRetrieval,\n",
    ")\n",
    "from operator import attrgetter\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "\n",
    "from transformers import LlavaForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def get_leaf_modules(model: nn.Module) -> OrderedDict[str, nn.Module]:\n",
    "    \"\"\"\n",
    "    Returns an ordered dictionary containing only the leaf modules of a PyTorch model.\n",
    "    Leaf modules are those that do not have any children.\n",
    "    \"\"\"\n",
    "    leaf_modules = OrderedDict()\n",
    "    for name, module in model.named_modules():\n",
    "        if not list(module.children()):  # Check if the module has no children\n",
    "            leaf_modules[name] = module\n",
    "    return leaf_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4220344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpw(\n",
    "    leaves,\n",
    "    quantized_mods,\n",
    "    total_params,\n",
    "    vision_bits=None,\n",
    "    qformer_bits=None,\n",
    "    llm_bits=None,\n",
    "    fp_size=16,\n",
    "):\n",
    "    total_bits = 0\n",
    "    vision_params = 0\n",
    "    qformer_params = 0\n",
    "    llm_params = 0\n",
    "\n",
    "    for key, module in leaves.items():\n",
    "        fp_mod_flag = True\n",
    "\n",
    "        # check if parameters in module should be quantized\n",
    "        for q_mod in quantized_mods:\n",
    "            # add quantized linear bit sizes\n",
    "            if q_mod in key and isinstance(module, nn.Linear):\n",
    "                num_el = module.weight.numel()\n",
    "\n",
    "                if \"vision\" in q_mod:\n",
    "                    total_bits += vision_bits * num_el\n",
    "                    vision_params += num_el\n",
    "                elif \"qformer\" in q_mod:\n",
    "                    total_bits += qformer_bits * num_el\n",
    "                    qformer_params += num_el\n",
    "                elif \"language\" in q_mod:\n",
    "                    total_bits += llm_bits * num_el\n",
    "                    llm_params += num_el\n",
    "                else:\n",
    "                    raise Exception()\n",
    "\n",
    "                fp_mod_flag = False\n",
    "\n",
    "        # full_precision module\n",
    "        if fp_mod_flag:\n",
    "            # print(key)\n",
    "            for param in module.parameters():\n",
    "                total_bits += fp_size * param.numel()\n",
    "\n",
    "    print(f\"vision q params: {vision_params}\")\n",
    "    print(f\"qformer q params: {qformer_params}\")\n",
    "    print(f\"llm_params: {llm_params}\")\n",
    "\n",
    "    return total_bits / total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Salesforce/blip2-itm-vit-g-coco\"\n",
    "# model = Blip2ForImageTextRetrieval.from_pretrained(model_name)\n",
    "\n",
    "# leaves = get_leaf_modules(model)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(total_params)\n",
    "# quantized_mods = [\n",
    "#     \"vision_model.encoder.layers\",\n",
    "#     \"qformer.encoder.layer\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "# model.to('cpu')\n",
    "\n",
    "# leaves = get_leaf_modules(model)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# quantized_mods = [\n",
    "#     \"vision_model.encoder.layers\",\n",
    "#     \"qformer.encoder.layer\",\n",
    "#     \"language_model.model.decoder.layers\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# Load the model\n",
    "# model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16)\n",
    "# # offload model to cpu for now\n",
    "# model.to('cpu')\n",
    "\n",
    "\n",
    "# quantized_mods = [\n",
    "#     \"vision_tower.vision_model.encoder.layers\",\n",
    "#     \"language_model.model.layers\",\n",
    "# ]\n",
    "\n",
    "# leaves = get_leaf_modules(model)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(total_params)\n",
    "\n",
    "# compute_bpw(leaves, quantized_mods, total_params,\n",
    "#                                   vision_bits=4,\n",
    "#                                   qformer_bits=4,\n",
    "#                                   llm_bits=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd4835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpw_llava(vision_bits, llm_bits, fp_bits=16):\n",
    "    total_params = 7063427072\n",
    "\n",
    "    vision_q_params = 301989888\n",
    "    llm_q_params = 6476005376\n",
    "\n",
    "    non_q_params = total_params - vision_q_params - llm_q_params\n",
    "\n",
    "    bpw = (vision_bits * vision_q_params + llm_bits * llm_q_params + fp_bits * non_q_params) / total_params\n",
    "\n",
    "    return bpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bce57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpw_blip_full(vision_bits, qformer_bits, llm_bits, fp_bits=16):\n",
    "    total_params = 3744761856\n",
    "\n",
    "    vision_q_params = 984023040\n",
    "    qformer_q_params = 104988672\n",
    "    llm_q_params = 2516582400\n",
    "\n",
    "    non_q_params = total_params - vision_q_params - qformer_q_params - llm_q_params\n",
    "\n",
    "    bpw = (\n",
    "        vision_bits * vision_q_params\n",
    "        + qformer_bits * qformer_q_params\n",
    "        + llm_bits * llm_q_params\n",
    "        + fp_bits * non_q_params\n",
    "    ) / total_params\n",
    "\n",
    "    return bpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaff34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpw_blip_retrieval(vision_bits, qformer_bits, fp_bits=16):\n",
    "    total_params = 1172623618\n",
    "\n",
    "    vision_q_params = 984023040\n",
    "    qformer_q_params = 161611776\n",
    "\n",
    "    non_q_params = total_params - vision_q_params - qformer_q_params\n",
    "\n",
    "    bpw = (vision_bits * vision_q_params + qformer_bits * qformer_q_params + fp_bits * non_q_params) / total_params\n",
    "\n",
    "    return bpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89debfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./blip2/awq/image_captioning/awq_image_captioning.csv\"\n",
    "df_awq_coco = pd.read_csv(path)\n",
    "df_awq_coco = df_awq_coco.drop([\"model_size\"], axis=1)\n",
    "df_awq_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3359474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute bpw\n",
    "# model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(model_name)\n",
    "# model.to('cpu')\n",
    "\n",
    "# leaves = get_leaf_modules(model)\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# quantized_mods = [\n",
    "#     \"vision_model.encoder.layers\",\n",
    "#     \"qformer.encoder.layer\",\n",
    "#     \"language_model.model.decoder.layers\"\n",
    "# ]\n",
    "\n",
    "# df_awq_coco['bpw'] = [compute_bpw(leaves, quantized_mods, total_params,\n",
    "#                                   vision_bits=x['vit_bits'],\n",
    "#                                   qformer_bits=x['qformer_bits'],\n",
    "#                                   llm_bits=x['llm_bits']) for x in df_awq_coco.to_dict(orient='records')]\n",
    "\n",
    "df_awq_coco[\"bpw\"] = [\n",
    "    compute_bpw_blip_full(\n",
    "        vision_bits=x[\"vit_bits\"],\n",
    "        qformer_bits=x[\"qformer_bits\"],\n",
    "        llm_bits=x[\"llm_bits\"],\n",
    "    )\n",
    "    for x in df_awq_coco.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "df_awq_coco[\"quant_method\"] = \"awq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ba8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e85627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_coco.to_csv(os.path.join(\"./final_results/all_results\", \"blip2_awq_coco.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce72e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./blip2/awq/image_text_retrieval/awq_image_text_retrieval.csv\"\n",
    "df_awq_flickr = pd.read_csv(path)\n",
    "df_awq_flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_flickr[\"bpw\"] = [\n",
    "    compute_bpw_blip_retrieval(\n",
    "        vision_bits=x[\"vit_bits\"],\n",
    "        qformer_bits=x[\"qformer_bits\"],\n",
    "    )\n",
    "    for x in df_awq_flickr.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "df_awq_flickr[\"quant_method\"] = \"awq\"\n",
    "df_awq_flickr = df_awq_flickr.drop([\"model_size\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f123399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5edc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_flickr.to_csv(os.path.join(\"./final_results/all_results\", \"blip2_awq_flickr.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GQA\n",
    "df_gptq_gqa = pd.read_csv(\"./final_results/llava/llava_gptq_gqa_results.csv\")\n",
    "df_gptq_gqa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd43af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gptq_gqa[\"bpw\"] = [\n",
    "    compute_bpw_llava(\n",
    "        vision_bits=x[\"vision_bits\"],\n",
    "        llm_bits=x[\"language_bits\"],\n",
    "    )\n",
    "    for x in df_gptq_gqa.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "df_gptq_gqa[\"quant_method\"] = \"gptq\"\n",
    "\n",
    "df_gptq_gqa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90530e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gptq_gqa.to_csv(os.path.join(\"./final_results/all_results\", \"llava_gptq_gqa.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gptq_vqav2 = pd.read_csv(\"./final_results/llava/llava_gptq_vqav2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gptq_vqav2[\"bpw\"] = [\n",
    "    compute_bpw_llava(\n",
    "        vision_bits=x[\"vision_bits\"],\n",
    "        llm_bits=x[\"language_bits\"],\n",
    "    )\n",
    "    for x in df_gptq_vqav2.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "df_gptq_vqav2[\"quant_method\"] = \"gptq\"\n",
    "df_gptq_vqav2 = df_gptq_vqav2.rename({\"agg_metrics\": \"acc\"}, axis=1)\n",
    "\n",
    "df_gptq_vqav2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gptq_vqav2.to_csv(\"./final_results/all_results/llava_gptq_vqav2.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7eb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_gqa = pd.read_csv(\"./final_results/llava/llava_awq_gqa.csv\")\n",
    "df_awq_gqa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97754e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_gqa[\"bpw\"] = [\n",
    "    compute_bpw_llava(\n",
    "        vision_bits=x[\"vision_bits\"],\n",
    "        llm_bits=x[\"language_bits\"],\n",
    "    )\n",
    "    for x in df_awq_gqa.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "df_awq_gqa[\"quant_method\"] = \"awq\"\n",
    "\n",
    "df_awq_gqa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a980d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_gqa.to_csv(\"./final_results/all_results/llava_awq_gqa.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cee347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_vqav2 = pd.read_csv(\"./final_results/llava/llava_awq_vqav2.csv\")\n",
    "df_awq_vqav2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698cb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_vqav2[\"bpw\"] = [\n",
    "    compute_bpw_llava(\n",
    "        vision_bits=x[\"vision_bits\"],\n",
    "        llm_bits=x[\"language_bits\"],\n",
    "    )\n",
    "    for x in df_awq_vqav2.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "df_awq_vqav2[\"quant_method\"] = \"awq\"\n",
    "df_awq_vqav2 = df_awq_vqav2.rename({\"agg_metrics\": \"acc\"}, axis=1)\n",
    "\n",
    "df_awq_vqav2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20bc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_awq_vqav2.to_csv(\"./final_results/all_results/llava_awq_vqav2.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform flickr\n",
    "df_uniform_flickr = pd.read_csv(\"./final_results/blip2/uniform/blip2_flickr_results.csv\")\n",
    "df_uniform_flickr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250936a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_uniform_flickr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/blip2-itm-vit-g-coco\"\n",
    "model = Blip2ForImageTextRetrieval.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee784b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpw_uniform(leaves, quantized_mods, total_params, row_dict, fp_size=16):\n",
    "    total_bits = 0\n",
    "\n",
    "    for key, module in leaves.items():\n",
    "        fp_mod_flag = True\n",
    "\n",
    "        # check if parameters in module should be quantized\n",
    "        for q_mod in quantized_mods:\n",
    "            # add quantized linear bit sizes\n",
    "            if q_mod in key and isinstance(module, nn.Linear):\n",
    "                num_el = module.weight.numel()\n",
    "\n",
    "                # parse out layer index and module name\n",
    "                layer_idx = int(re.findall(r\"layer[s]*.(\\d*)\", key)[-1])\n",
    "                mod_name = key.split(\".\")[-1]\n",
    "\n",
    "                if mod_name == \"projection\":\n",
    "                    mod_name = \"proj\"\n",
    "\n",
    "                # quantized vision module and layer idx included and mod_name included\n",
    "                if \"vision\" in q_mod:\n",
    "                    # sanity check for nan values\n",
    "                    if (\n",
    "                        row_dict[\"visual_encoder_block_indices\"] == row_dict[\"visual_encoder_block_indices\"]\n",
    "                        and layer_idx in eval(row_dict[\"visual_encoder_block_indices\"])\n",
    "                        and mod_name in eval(row_dict[\"visual_encoder_block_modules\"])\n",
    "                    ):\n",
    "                        # print(layer_idx)\n",
    "                        # print(mod_name)\n",
    "\n",
    "                        total_bits += int(row_dict[\"visual_encoder_block_weight_bits\"]) * num_el\n",
    "                        fp_mod_flag = False\n",
    "\n",
    "                    # total_bits += vision_bits*num_el\n",
    "\n",
    "                elif \"qformer\" in q_mod:  # and \\\n",
    "                    # sanity check for nan values\n",
    "                    if row_dict[\"qformer_layer_indices\"] == row_dict[\"qformer_layer_indices\"] and layer_idx in eval(\n",
    "                        row_dict[\"qformer_layer_indices\"]\n",
    "                    ):\n",
    "                        qformer_weight_bits = int(row_dict[\"qformer_weight_bits\"])\n",
    "\n",
    "                        # NOTE: same quantized mods for self/cross-attn\n",
    "                        if \"attention\" in key:\n",
    "                            if row_dict[\"qformer_self_attention_modules\"] == row_dict[\n",
    "                                \"qformer_self_attention_modules\"\n",
    "                            ] and mod_name in eval(row_dict[\"qformer_self_attention_modules\"]):\n",
    "                                total_bits += qformer_weight_bits * num_el\n",
    "                                fp_mod_flag = False\n",
    "                        # img_ff\n",
    "                        elif \"query\" in key:\n",
    "                            if row_dict[\"qformer_img_ff_modules\"] == row_dict[\"qformer_img_ff_modules\"] and any(\n",
    "                                x in key for x in eval(row_dict[\"qformer_img_ff_modules\"])\n",
    "                            ):\n",
    "                                total_bits += qformer_weight_bits * num_el\n",
    "                                fp_mod_flag = False\n",
    "\n",
    "                        # text_ff\n",
    "                        else:\n",
    "                            if row_dict[\"qformer_text_ff_modules\"] == row_dict[\"qformer_text_ff_modules\"] and any(\n",
    "                                x in key for x in eval(row_dict[\"qformer_text_ff_modules\"])\n",
    "                            ):\n",
    "                                total_bits += qformer_weight_bits * num_el\n",
    "                                fp_mod_flag = False\n",
    "\n",
    "        # full_precision module\n",
    "        if fp_mod_flag:\n",
    "            # print(key)\n",
    "            for param in module.parameters():\n",
    "                total_bits += fp_size * param.numel()\n",
    "\n",
    "    return total_bits / total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniform_flickr[\"visual_encoder_block_modules\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict = df_uniform_flickr.to_dict(orient=\"records\")[202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"qformer_layer_indices\"] == row_dict[\"qformer_layer_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc38a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"qformer_self_attention_modules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18291857",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"qformer_cross_attention_modules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"qformer_img_ff_modules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c29f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"qformer_text_ff_modules\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c699c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"visual_encoder_block_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"qformer_weight_bits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dict[\"visual_encoder_block_weight_bits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11b9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = get_leaf_modules(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "quantized_mods = [\n",
    "    \"vision_model.encoder.layers\",\n",
    "    \"qformer.encoder.layer\",\n",
    "]\n",
    "\n",
    "\n",
    "df_uniform_flickr[\"bpw\"] = [\n",
    "    compute_bpw_uniform(leaves, quantized_mods, total_params, row_dict)\n",
    "    for row_dict in df_uniform_flickr.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "\n",
    "df_uniform_flickr[\"quant_method\"] = \"uniform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniform_flickr.bpw.agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniform_flickr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd60bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export = df_uniform_flickr[\n",
    "    [\n",
    "        \"txt_r1\",\n",
    "        \"txt_r5\",\n",
    "        \"txt_r10\",\n",
    "        \"txt_r_mean\",\n",
    "        \"img_r1\",\n",
    "        \"img_r5\",\n",
    "        \"img_r10\",\n",
    "        \"img_r_mean\",\n",
    "        \"r_mean\",\n",
    "        \"vit_attn\",\n",
    "        \"vit_ff\",\n",
    "        \"vit_front_blocks\",\n",
    "        \"vit_middle_blocks\",\n",
    "        \"vit_end_blocks\",\n",
    "        \"vit_weight_bits\",\n",
    "        \"qformer_front_blocks\",\n",
    "        \"qformer_middle_blocks\",\n",
    "        \"qformer_end_blocks\",\n",
    "        \"qformer_self_attn\",\n",
    "        \"qformer_cross_attn\",\n",
    "        \"qformer_text_ff\",\n",
    "        \"qformer_img_ff\",\n",
    "        \"qformer_weight_bits\",\n",
    "        \"Quantized Portion\",\n",
    "        \"weight_bits\",\n",
    "        \"bpw\",\n",
    "        \"quant_method\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "df_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export.to_csv(os.path.join(\"./final_results/all_results\", \"blip2_uniform_flickr.csv\"), index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
