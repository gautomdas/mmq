{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dcfc88-25bd-4570-9292-8dc8bc670929",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9861ebc-01eb-4040-a2ab-2fe24b76ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"quantization_results.csv\")\n",
    "\n",
    "# Drop the 'config' column\n",
    "df = df.drop(\"config\", axis=1)\n",
    "\n",
    "# One-hot encode the 'type' column\n",
    "onehot = OneHotEncoder(sparse=False)\n",
    "type_encoded = onehot.fit_transform(df[[\"type\"]])\n",
    "type_columns = onehot.get_feature_names([\"type\"])\n",
    "df_encoded = pd.concat([df.drop(\"type\", axis=1), pd.DataFrame(type_encoded, columns=type_columns)], axis=1)\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = df_encoded.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca772944-ce44-44a8-b2e6-5249177bd6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_encoded.drop([\"METEOR\", \"CIDEr\"], axis=1).columns\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = df_encoded[features].corrwith(df_encoded[\"METEOR\"]).to_frame(name=\"METEOR\")\n",
    "correlations[\"CIDEr\"] = df_encoded[features].corrwith(df_encoded[\"CIDEr\"])\n",
    "\n",
    "# Sort by absolute correlation with METEOR\n",
    "correlations = correlations.reindex(correlations[\"METEOR\"].abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(correlations, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, center=0)\n",
    "plt.title(\"Feature Correlations with METEOR and CIDEr Scores\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333eb7a6-2d66-4605-8464-58366364eae6",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295993ad-6c49-4ec3-8ff0-940b0262448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Random Forest\n",
    "X = df_encoded.drop([\"METEOR\", \"CIDEr\"], axis=1)\n",
    "y_meteor = df_encoded[\"METEOR\"]\n",
    "y_cider = df_encoded[\"CIDEr\"]\n",
    "\n",
    "rf_meteor = RandomForestRegressor(n_estimators=1, random_state=42)\n",
    "rf_cider = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_meteor.fit(X, y_meteor)\n",
    "rf_cider.fit(X, y_cider)\n",
    "\n",
    "feature_importance_meteor = pd.Series(rf_meteor.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "feature_importance_cider = pd.Series(rf_cider.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance_meteor.plot(kind=\"bar\")\n",
    "plt.title(\"Feature Importance for METEOR Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance_cider.plot(kind=\"bar\")\n",
    "plt.title(\"Feature Importance for CIDEr Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c0f35-c31a-428d-b3ba-26191ef9b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "importance_df = pd.DataFrame({\"METEOR\": feature_importance_meteor, \"CIDEr\": feature_importance_cider})\n",
    "\n",
    "# Sort the DataFrame based on the sum of importances\n",
    "importance_df[\"Total\"] = importance_df[\"METEOR\"] + importance_df[\"CIDEr\"]\n",
    "importance_df_sorted = importance_df.sort_values(\"Total\", ascending=False).drop(\"Total\", axis=1)\n",
    "\n",
    "# Plot the sorted data\n",
    "ax = importance_df_sorted.plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"Feature Importance Comparison (Sorted)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.legend(title=\"Metric\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeabf7-d161-4d5e-ad8c-69da0f9f1b00",
   "metadata": {},
   "source": [
    "### Tree Fit Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736d652-7945-4283-8c59-e6fa082f9926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to calculate and print model performance metrics\n",
    "def print_model_performance(model, X, y, model_name):\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"r2\")\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Calculate MSE and R-squared\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    print(f\"Performance metrics for {model_name}:\")\n",
    "    print(f\"Mean cross-validation R-squared: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Calculate and print performance metrics for METEOR model\n",
    "print_model_performance(rf_meteor, X, y_meteor, \"METEOR Random Forest\")\n",
    "\n",
    "# Calculate and print performance metrics for CIDEr model\n",
    "print_model_performance(rf_cider, X, y_cider, \"CIDEr Random Forest\")\n",
    "\n",
    "\n",
    "# Optional: Plot actual vs predicted values\n",
    "def plot_actual_vs_predicted(model, X, y, title):\n",
    "    y_pred = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], \"r--\", lw=2)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_actual_vs_predicted(rf_meteor, X, y_meteor, \"Actual vs Predicted METEOR Scores\")\n",
    "plot_actual_vs_predicted(rf_cider, X, y_cider, \"Actual vs Predicted CIDEr Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf9fb6-46fa-45da-afe4-5f81e0efcf3a",
   "metadata": {},
   "source": [
    "## Drop Bit Size and Recalculate Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6c995-9222-4736-92c5-58b711d73200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Random Forest\n",
    "df_encoded_no_bit_size = df_encoded.drop(\"bit_size\", axis=1)\n",
    "X = df_encoded_no_bit_size.drop([\"METEOR\", \"CIDEr\"], axis=1)\n",
    "y_meteor = df_encoded_no_bit_size[\"METEOR\"]\n",
    "y_cider = df_encoded_no_bit_size[\"CIDEr\"]\n",
    "\n",
    "rf_meteor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_cider = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_meteor.fit(X, y_meteor)\n",
    "rf_cider.fit(X, y_cider)\n",
    "\n",
    "feature_importance_meteor = pd.Series(rf_meteor.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "feature_importance_cider = pd.Series(rf_cider.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance_meteor.plot(kind=\"bar\")\n",
    "plt.title(\"Feature Importance for METEOR Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance_cider.plot(kind=\"bar\")\n",
    "plt.title(\"Feature Importance for CIDEr Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284fb9a-9ecb-4a46-ab2f-26ce3a38b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "importance_df = pd.DataFrame({\"METEOR\": feature_importance_meteor, \"CIDEr\": feature_importance_cider})\n",
    "\n",
    "# Sort the DataFrame based on the sum of importances\n",
    "importance_df[\"Total\"] = importance_df[\"METEOR\"] + importance_df[\"CIDEr\"]\n",
    "importance_df_sorted = importance_df.sort_values(\"Total\", ascending=False).drop(\"Total\", axis=1)\n",
    "\n",
    "# Plot the sorted data\n",
    "ax = importance_df_sorted.plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"Feature Importance Comparison (Sorted)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.legend(title=\"Metric\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503545ee-4324-42f8-8cab-64f189440ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to calculate and print model performance metrics\n",
    "def print_model_performance(model, X, y, model_name):\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"r2\")\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Calculate MSE and R-squared\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    print(f\"Performance metrics for {model_name}:\")\n",
    "    print(f\"Mean cross-validation R-squared: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# Calculate and print performance metrics for METEOR model\n",
    "print_model_performance(rf_meteor, X, y_meteor, \"METEOR Random Forest\")\n",
    "\n",
    "# Calculate and print performance metrics for CIDEr model\n",
    "print_model_performance(rf_cider, X, y_cider, \"CIDEr Random Forest\")\n",
    "\n",
    "\n",
    "# Optional: Plot actual vs predicted values\n",
    "def plot_actual_vs_predicted(model, X, y, title):\n",
    "    y_pred = model.predict(X)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y, y_pred, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], \"r--\", lw=2)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_actual_vs_predicted(rf_meteor, X, y_meteor, \"Actual vs Predicted METEOR Scores\")\n",
    "plot_actual_vs_predicted(rf_cider, X, y_cider, \"Actual vs Predicted CIDEr Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78373ba-c4b2-4699-8d30-ae1538fb5903",
   "metadata": {},
   "source": [
    "## Tree Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff41815-b957-4ee2-8c73-ba95e36cdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For METEOR\n",
    "plt.figure(figsize=(40, 20), dpi=1000)  # Increased figure size and DPI for high resolution\n",
    "plot_tree(rf_meteor.estimators_[0], feature_names=X.columns, filled=True, rounded=True)\n",
    "plt.title(\"Example Decision Tree for METEOR Score\")\n",
    "plt.savefig(\"meteor_decision_tree.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "plt.close()\n",
    "\n",
    "# For CIDEr\n",
    "plt.figure(figsize=(40, 20), dpi=1000)  # Increased figure size and DPI for high resolution\n",
    "plot_tree(rf_cider.estimators_[0], feature_names=X.columns, filled=True, rounded=True)\n",
    "plt.title(\"Example Decision Tree for CIDEr Score\")\n",
    "plt.savefig(\"cider_decision_tree.png\", dpi=300, bbox_inches=\"tight\")  # Save as PNG\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30229a6-1162-42fa-8507-620b9e14011f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
